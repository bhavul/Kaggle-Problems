{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv', 'train.csv', 'gender_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import tensorflow as tf\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# Read Data\n",
    "\n",
    "Here's what we'll do \n",
    "- Read data into csv files (train and test)\n",
    "- Print out a small summary of the data\n",
    "- Combine them into one dataset if we require later on\n",
    "- Find out how many examples of each class exist in the training data (check if skewed or not)\n",
    "- Find out how many features have null values\n",
    "- Fix null values for numerical features\n",
    "- Fix null values with some values for categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ae5a0f1e-38e7-4767-af5f-c9e92fb9c5c4",
    "_uuid": "c271e9a42063b776cfa8def1fa3e6ca6aa6b9120"
   },
   "source": [
    " ## Read data into csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "e0ce7063-5c8a-4ec9-8992-948db5ba8d6a",
    "_uuid": "93acd5d867a147f567cdfe541ec2a23e61ba5daa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape :  (891, 12)\n",
      "test_df shape :  (418, 11)\n"
     ]
    }
   ],
   "source": [
    "# Read data into csv files\n",
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "print(\"train_df shape : \",train_df.shape)\n",
    "print(\"test_df shape : \",test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "82bd9ccf-90b7-4422-8088-e1e1888eec9c",
    "_uuid": "1a7038bea44483e00d4dcfa224836cb2b05b2813"
   },
   "source": [
    "## Print out summary of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "889f4b2c-ffb0-4b05-b4db-e85ee5c165ea",
    "_uuid": "ab9955211cbc36244805ff836d89cb855dd6f965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A look at training data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print small summary\n",
    "print(\"A look at training data:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "eb5d8eb9-9078-492a-be00-0f38a17587ba",
    "_uuid": "aa79c8fe2bf76d847a225a309376e5d319f72743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A look at testing data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"A look at testing data:\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "966d2a48-7482-49c8-b91e-ee95fe13b0bb",
    "_uuid": "8566ea5c22cf69f11cdebb3ac6e64294edbc33d8"
   },
   "source": [
    "> ***Obvious observation - 'Survived' column is missing in test_df***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1d199dc-275b-4c04-ba25-cfbcb2809113",
    "_uuid": "a802e4a312c37870113f7a3fd4807626cb2d3856"
   },
   "source": [
    "## Find out how many examples of each class in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "f3230aaa-c108-40d2-b676-9ae61a6d8e1f",
    "_uuid": "0f6fca9063bce97ea6bc50641d95b7a0df63f694"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived\n",
       "0    549\n",
       "1    342\n",
       "Name: PassengerId, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('Survived')['PassengerId'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6e844766-8978-496a-9150-a22cd9abbc4a",
    "_uuid": "f28ac6e46794dd26115bc33e80c0c213ea736170"
   },
   "source": [
    "**Observations** : \n",
    "1. 549+342 = 891. So no data in the training data is missing its class\n",
    "2. It's not such a skewed dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f22483a-a295-46dc-92a9-b4bd394fa92b",
    "_uuid": "29b3dbe3c9634a6deafbcf75053ce36090d7f9c4"
   },
   "source": [
    "## How many features have null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "d05bcb46-97d6-4a92-927e-da962ceff85a",
    "_uuid": "4c5230cacfd1e079cd3715c316a7e137c2623c9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    False\n",
       "Survived       False\n",
       "Pclass         False\n",
       "Name           False\n",
       "Sex            False\n",
       "Age             True\n",
       "SibSp          False\n",
       "Parch          False\n",
       "Ticket         False\n",
       "Fare           False\n",
       "Cabin           True\n",
       "Embarked        True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What any does is return whether any element in a particular axis is true or not. So, it works for us in this case. For each column, it checks if any column has a NaN value or not.\n",
    "train_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "14d190bb-fb0e-42e7-9d5c-b3084acc0d02",
    "_uuid": "dc4c59e61b2a027cb04a4d146efc12c8ceafcb3c"
   },
   "source": [
    "**Age**, **Cabin** and **Embarked** are the only ones having NaN values. We gotta fix them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "22303fa2-44a9-485b-8686-d2bf3e2ef220",
    "_uuid": "6fceb74db0bbfa3cefbe5e48c7779ccdcf18df55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many NaN values of Age in train_df?\n",
    "train_df['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "490e4b65-5097-4945-9750-7a2c235c48d9",
    "_uuid": "e284bdfdd31ae0375b4a8e9007bb10e9aca3ccd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Cabin\n",
    "train_df['Cabin'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "70fafbde-6498-491c-87a2-14338d9281fc",
    "_uuid": "d13739d3a0195c4161fc35abfeaec97781b9dcae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Embarked\n",
    "train_df['Embarked'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "be973170-db43-4bde-b1ef-24435c379204",
    "_uuid": "6ba65f2651f04590e63786ed9dee43c4de01e7a7"
   },
   "source": [
    "## Fixing null / NaN values for each column one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "503fd234-a659-466f-91c1-70df5f831390",
    "_uuid": "555646f8afe1ea5b4818cd43380133531a0ff75d"
   },
   "source": [
    "### For embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "677dcd34-2364-4f53-ba78-e6969c3457e4",
    "_uuid": "457cc7e18ed946947aecc3bfed80a2bf7b14018c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embarked\n",
       "C    168\n",
       "Q     77\n",
       "S    644\n",
       "Name: PassengerId, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('Embarked')['PassengerId'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6ce4f64c-0da5-4dff-a06d-cf2ddea5d1b2",
    "_uuid": "663824d65d95834bf097b4324b8a6b03470cea3c"
   },
   "source": [
    "We observed earlier that only 2 entries have NaN for Embarked. And here, we see there are only 3 possible values of Embarked - C, Q and S. Out of which, S has the most number. So, let's just assign the missing ones to S. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "44eeb547-f721-4da8-8e42-82271744e861",
    "_uuid": "ddc0fe4242d146761c82b11e96097d64deb039d9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['Embarked'] = train_df['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1ab5994d-039d-482a-9a3c-dc73e68c5737",
    "_uuid": "a4027c9ec404e9af75e37fd545ee67818f832da8"
   },
   "source": [
    "Now, let's check again...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "8d70ea2d-cdb5-4bc9-b12f-66bfd98a8447",
    "_uuid": "63872c657fbee0643ed9d63b20d63d3b1b465a39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embarked\n",
       "C    168\n",
       "Q     77\n",
       "S    646\n",
       "Name: PassengerId, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('Embarked')['PassengerId'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0d8f9d47-738c-4d56-9af2-0eceafea721a",
    "_uuid": "575ecc7dc22997aad63e70c2f0d1514af3fbc5ba"
   },
   "source": [
    "Perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea8a0bef-415f-45a5-ab7d-b6bbaec3a3c3",
    "_uuid": "c279a1509f78b400cfc00bebe958a2d11285988d"
   },
   "source": [
    "### For Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "f234733f-5289-4116-ade3-708f634dac0d",
    "_uuid": "1cea91351872a48c0a5ffd475561453a6f4fc693"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age\n",
       "0.42      1\n",
       "0.67      1\n",
       "0.75      2\n",
       "0.83      2\n",
       "0.92      1\n",
       "1.00      7\n",
       "2.00     10\n",
       "3.00      6\n",
       "4.00     10\n",
       "5.00      4\n",
       "6.00      3\n",
       "7.00      3\n",
       "8.00      4\n",
       "9.00      8\n",
       "10.00     2\n",
       "11.00     4\n",
       "12.00     1\n",
       "13.00     2\n",
       "14.00     6\n",
       "14.50     1\n",
       "15.00     5\n",
       "16.00    17\n",
       "17.00    13\n",
       "18.00    26\n",
       "19.00    25\n",
       "20.00    15\n",
       "20.50     1\n",
       "21.00    24\n",
       "22.00    27\n",
       "23.00    15\n",
       "         ..\n",
       "44.00     9\n",
       "45.00    12\n",
       "45.50     2\n",
       "46.00     3\n",
       "47.00     9\n",
       "48.00     9\n",
       "49.00     6\n",
       "50.00    10\n",
       "51.00     7\n",
       "52.00     6\n",
       "53.00     1\n",
       "54.00     8\n",
       "55.00     2\n",
       "55.50     1\n",
       "56.00     4\n",
       "57.00     2\n",
       "58.00     5\n",
       "59.00     2\n",
       "60.00     4\n",
       "61.00     3\n",
       "62.00     4\n",
       "63.00     2\n",
       "64.00     2\n",
       "65.00     3\n",
       "66.00     1\n",
       "70.00     2\n",
       "70.50     1\n",
       "71.00     2\n",
       "74.00     1\n",
       "80.00     1\n",
       "Name: PassengerId, Length: 88, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('Age')['PassengerId'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f919afb-b2af-40ea-b5b3-c501a2db4909",
    "_uuid": "e4204d5f0dd18533662995363f25f2781db5945e"
   },
   "source": [
    "So, the first thing to note is, thie Age can be in decimals! So, it's more of a continuous variable than discrete one.\n",
    "I think it would make sense to fix the missing ones by filling them with the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "6d9c7349-726a-4d23-b3e2-08fd9298dd37",
    "_uuid": "7e891707df51253a5b369fe08c5c309c10b7e964"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.69911764705882"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "2bda0a23-8ab0-463e-af4a-17f5270c4c5c",
    "_uuid": "e5d7be7cc1facc520fff1d0ec7234a36f99173ee",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['Age'] = train_df['Age'].fillna(train_df['Age'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "76b0606d-bd85-4aaa-98d1-ada3437a3e17",
    "_uuid": "e7032f2754dc4ab7f689fa44408dd08e9f4e229a"
   },
   "source": [
    "Now, let's check how many missing values remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "2dc0d6ab-ea8f-4994-a76d-322f5501dcb7",
    "_uuid": "41151b525ff7fdd0d7d1bd334f80f11ad3123191"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4e0f46b4-00e8-479b-adf2-0e00f166fcd1",
    "_uuid": "d48d43305ee948afb201f2f8faeff4a9f704f2a4"
   },
   "source": [
    "Perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d04a3b1a-519d-4662-ba2e-dace1075bb36",
    "_uuid": "8dfac3aceaa1a36a775b0e29701344930c1fd191"
   },
   "source": [
    "### For Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "7d9dce10-b813-468a-83bb-b835d3355af4",
    "_uuid": "caef1b4f21c9b545459d1e8440820948d21ed47f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cabin\n",
       "A10      1\n",
       "A14      1\n",
       "A16      1\n",
       "A19      1\n",
       "A20      1\n",
       "A23      1\n",
       "A24      1\n",
       "A26      1\n",
       "A31      1\n",
       "A32      1\n",
       "A34      1\n",
       "A36      1\n",
       "A5       1\n",
       "A6       1\n",
       "A7       1\n",
       "B101     1\n",
       "B102     1\n",
       "B18      2\n",
       "B19      1\n",
       "B20      2\n",
       "B22      2\n",
       "B28      2\n",
       "B3       1\n",
       "B30      1\n",
       "B35      2\n",
       "B37      1\n",
       "B38      1\n",
       "B39      1\n",
       "B4       1\n",
       "B41      1\n",
       "        ..\n",
       "E12      1\n",
       "E121     2\n",
       "E17      1\n",
       "E24      2\n",
       "E25      2\n",
       "E31      1\n",
       "E33      2\n",
       "E34      1\n",
       "E36      1\n",
       "E38      1\n",
       "E40      1\n",
       "E44      2\n",
       "E46      1\n",
       "E49      1\n",
       "E50      1\n",
       "E58      1\n",
       "E63      1\n",
       "E67      2\n",
       "E68      1\n",
       "E77      1\n",
       "E8       2\n",
       "F E69    1\n",
       "F G63    1\n",
       "F G73    2\n",
       "F2       3\n",
       "F33      3\n",
       "F38      1\n",
       "F4       2\n",
       "G6       4\n",
       "T        1\n",
       "Name: PassengerId, Length: 147, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('Cabin')['PassengerId'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f50f41c9-1085-4cb6-8a0e-74765ef698b2",
    "_uuid": "cdf837ff32346220b59c0aa39bfa0e8095995716"
   },
   "source": [
    "Okay, So : \n",
    "- This can be alphanumeric\n",
    "- 147 different vaulues exist for Cabin\n",
    "- None of them seem to be far far greater in number than others\n",
    "- A lot of values are actually missing - 687!\n",
    "\n",
    "So, let's do one thing - Add a new 'Cabin' value as 'UNKNOWN' and fill the data with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "06e5e7ae-dd88-46d3-94a3-4795d8be0921",
    "_uuid": "422660b880279158ab3a454f19c5531ac9dc8344",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['Cabin'] = train_df['Cabin'].fillna('UNKNOWN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d4c4c03-c860-437e-bf5d-1d542961fd6e",
    "_uuid": "11a8856b34c9166e33dc846cef325f6810aa508d"
   },
   "source": [
    "Check how many NaN now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "c0372aae-117e-4e0b-85ac-62d174e35ff9",
    "_uuid": "fa493c886c93ea945c4c8a88a410872e6ece7ab6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Cabin'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "248a6290-ca5b-4d93-88ff-42c47a9d9b4c",
    "_uuid": "4f87fdfa05847920741c5b9fc40ec1bc1ee1ce2e"
   },
   "source": [
    "Perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7db9fa91-2589-4977-963f-527875a5f6f7",
    "_uuid": "99f0d9422641f4579594db39392ba2fdd00c5632"
   },
   "source": [
    "### All NaN values fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "75991fe7-e95b-480e-9998-21f20bcac50f",
    "_uuid": "b80554750fd5d4e7bcf970e54b37f6a6cff57f58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    False\n",
       "Survived       False\n",
       "Pclass         False\n",
       "Name           False\n",
       "Sex            False\n",
       "Age            False\n",
       "SibSp          False\n",
       "Parch          False\n",
       "Ticket         False\n",
       "Fare           False\n",
       "Cabin          False\n",
       "Embarked       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What any does is return whether any element in a particular axis is true or not. So, it works for us in this case. For each column, it checks if any column has a NaN value or not.\n",
    "train_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d003e687-cee2-4571-9017-977515df1d07",
    "_uuid": "12fb48243010b8c0ccb02184660caaa9bf657dd7"
   },
   "source": [
    "## Helper Methods we learnt from above\n",
    "\n",
    "We'll use these for testing dataset, and maybe in future as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "2037ae87-51da-442f-a5d7-6ac52f170008",
    "_uuid": "1869bcc7c906c8d7ef73e0d1cb372e7eacbd0d53",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_of_NaN_rows(df):\n",
    "    return df.isnull().sum()\n",
    "\n",
    "def fill_NaN_values_for_numerical_column(df, colname):\n",
    "    df[colname] = df[colname].fillna(df[colname].mean())\n",
    "    return df\n",
    "\n",
    "def fill_NaN_values_for_categorical_column(df, colname, value):\n",
    "    df[colname] = df[colname].fillna(value)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "06b9ae58-b56f-4897-90a3-c56d4e1913c9",
    "_uuid": "6646f8e8ed41822e9962d0c59c7424ee486b3d85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_NaN_rows_of_test_set :  PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Let's test them on test data (which still might have missing rows!)\n",
    "num_of_NaN_rows_of_test_set = get_num_of_NaN_rows(test_df)\n",
    "print(\"num_of_NaN_rows_of_test_set : \",num_of_NaN_rows_of_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e975c9a-96c6-4e90-b7a0-55f2707d9319",
    "_uuid": "56df5eca27b763db884876730770aa89d33eaa99"
   },
   "source": [
    "One chapter done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d170a225-e8a8-4408-a517-768e27a23c78",
    "_uuid": "4e9ce88ff4cd005649539691b2b9f4b58a6821df"
   },
   "source": [
    "# Preprocessing Data\n",
    "\n",
    "- Convert Categorical values to numerical ones\n",
    "- Divide train_df into train_df_X and train_df_y\n",
    "- One hot values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7a4dc49b-7188-4856-a604-9f6346b5e630",
    "_uuid": "05859c25be3cbfcc2a3e1ebeeae10ea6ce89624b"
   },
   "source": [
    "### Convert Categorical values to numerical ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "de9801a6-1d9a-41ba-a0f5-b65e97ca915d",
    "_uuid": "de7b719dd5e364bac3596d5a015045a479d6824c"
   },
   "source": [
    "**1. Find which columns are categorical**\n",
    "\n",
    "Ref : https://stackoverflow.com/questions/29803093/check-which-columns-in-dataframe-are-categorical/29803290#29803290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "a588f113-a311-48cf-8fc7-52e792c050d4",
    "_uuid": "57bdc5e7c89f4ff47a3ace399264e6ba517aa428",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_cols = train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "9dd8ee78-4dc5-4272-9517-933cf1f1ead2",
    "_uuid": "5d17ba74addd657890fddc9d4c6392a76ab6c3d4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_cols = train_df._get_numeric_data().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "56633f88-446c-400b-97d1-f4495c139eb9",
    "_uuid": "0aae0e87028fa387ab4925c825025f514e36b1ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cabin', 'Embarked', 'Name', 'Sex', 'Ticket'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols = set(all_cols) - set(numeric_cols)\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "12d0198e-e350-4529-bda3-ee8f0fed74b3",
    "_uuid": "a8ba798afb2320a4635a0d3e7f5a43e6c72095ea",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's make a helper method from this now.\n",
    "def find_categorical_columns(df):\n",
    "    all_cols = df.columns\n",
    "    numeric_cols = df._get_numeric_data().columns\n",
    "    return set(all_cols) - set(numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b6f1e030-d2b2-44a2-9947-68ec6f1a8023",
    "_uuid": "109322e8ea2dda40357b72b536fcb8e1c9eaa0d9"
   },
   "source": [
    "**2. Convert to numerical ones using get_dummies of Pandas**\n",
    "\n",
    "Ref : http://fastml.com/converting-categorical-data-into-numbers-with-pandas-and-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "e6ce3139-b340-456d-aa71-ec277a0a9dde",
    "_uuid": "3074a2d8bbccf80caf4e0ce49f6cd540234950c8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare    Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500  UNKNOWN        S  \n",
       "1      0          PC 17599  71.2833      C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250  UNKNOWN        S  \n",
       "3      0            113803  53.1000     C123        S  \n",
       "4      0            373450   8.0500  UNKNOWN        S  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's backup our train_df and test_df till now\n",
    "train_df_backup_filledna_still_having_categorical_data = train_df\n",
    "train_df_backup_filledna_still_having_categorical_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "18c6806b-9ee4-41d9-93e9-f72c52c6a734",
    "_uuid": "f130afd68fb49254fde2c56caee2fd8fdd5d70e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 1732)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, let's convert it.\n",
    "train_df_dummies = pd.get_dummies(train_df, columns=categorical_cols)\n",
    "train_df_dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "874fc8c7-d181-4f9f-84ff-dbac2eee85ac",
    "_uuid": "90e2bccb4ea509f590824b454c1ddb4cb5162712"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# However, backup's shape is still \n",
    "train_df_backup_filledna_still_having_categorical_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "31b0a8aa-3f19-44a5-9ce6-077d77d6ee65",
    "_uuid": "eac180edc84145662bd3ab8f17c5d83eed2ecaa7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Ticket_110152</th>\n",
       "      <th>...</th>\n",
       "      <th>Name_Yrois, Miss. Henriette (\"Mrs Harbeck\")</th>\n",
       "      <th>Name_Zabour, Miss. Hileni</th>\n",
       "      <th>Name_Zabour, Miss. Thamine</th>\n",
       "      <th>Name_Zimmerman, Mr. Leo</th>\n",
       "      <th>Name_de Messemaeker, Mrs. Guillaume Joseph (Emma)</th>\n",
       "      <th>Name_de Mulder, Mr. Theodore</th>\n",
       "      <th>Name_de Pelsmaeker, Mr. Alfons</th>\n",
       "      <th>Name_del Carlo, Mr. Sebastiano</th>\n",
       "      <th>Name_van Billiard, Mr. Austin Blyler</th>\n",
       "      <th>Name_van Melkebeke, Mr. Philemon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1732 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass   Age  SibSp  Parch     Fare  Sex_female  \\\n",
       "0            1         0       3  22.0      1      0   7.2500           0   \n",
       "1            2         1       1  38.0      1      0  71.2833           1   \n",
       "2            3         1       3  26.0      0      0   7.9250           1   \n",
       "3            4         1       1  35.0      1      0  53.1000           1   \n",
       "4            5         0       3  35.0      0      0   8.0500           0   \n",
       "\n",
       "   Sex_male  Ticket_110152                ...                 \\\n",
       "0         1              0                ...                  \n",
       "1         0              0                ...                  \n",
       "2         0              0                ...                  \n",
       "3         0              0                ...                  \n",
       "4         1              0                ...                  \n",
       "\n",
       "   Name_Yrois, Miss. Henriette (\"Mrs Harbeck\")  Name_Zabour, Miss. Hileni  \\\n",
       "0                                            0                          0   \n",
       "1                                            0                          0   \n",
       "2                                            0                          0   \n",
       "3                                            0                          0   \n",
       "4                                            0                          0   \n",
       "\n",
       "   Name_Zabour, Miss. Thamine  Name_Zimmerman, Mr. Leo  \\\n",
       "0                           0                        0   \n",
       "1                           0                        0   \n",
       "2                           0                        0   \n",
       "3                           0                        0   \n",
       "4                           0                        0   \n",
       "\n",
       "   Name_de Messemaeker, Mrs. Guillaume Joseph (Emma)  \\\n",
       "0                                                  0   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "   Name_de Mulder, Mr. Theodore  Name_de Pelsmaeker, Mr. Alfons  \\\n",
       "0                             0                               0   \n",
       "1                             0                               0   \n",
       "2                             0                               0   \n",
       "3                             0                               0   \n",
       "4                             0                               0   \n",
       "\n",
       "   Name_del Carlo, Mr. Sebastiano  Name_van Billiard, Mr. Austin Blyler  \\\n",
       "0                               0                                     0   \n",
       "1                               0                                     0   \n",
       "2                               0                                     0   \n",
       "3                               0                                     0   \n",
       "4                               0                                     0   \n",
       "\n",
       "   Name_van Melkebeke, Mr. Philemon  \n",
       "0                                 0  \n",
       "1                                 0  \n",
       "2                                 0  \n",
       "3                                 0  \n",
       "4                                 0  \n",
       "\n",
       "[5 rows x 1732 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out data once\n",
    "train_df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "6e792fc1-68a5-473b-a1ae-f730cf6a03a7",
    "_uuid": "0a4c6966ecccacc2c522510133bc58d2c3a03d65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "12912989-070d-4530-8266-ba2665ba4b3d",
    "_uuid": "fb300f6e35216545132444654716e9bfd9e4032f"
   },
   "source": [
    "### Another way to convert Categorical columns data into numerical is assigning them integers\n",
    "Ref : https://stackoverflow.com/questions/42215354/pandas-get-mapping-of-categories-to-integer-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "0bfcfeb9-409c-426c-a4d4-85e750ca7ae5",
    "_uuid": "1ef6827e56fa3df922e1486963fcdd68bd8d14ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd way to convert is having integers represent different values of each categorical column\n",
    "train_df_numerical = train_df.copy()\n",
    "for col in categorical_cols:\n",
    "    train_df_numerical[col] = train_df_numerical[col].astype('category')\n",
    "    train_df_numerical[col] = train_df_numerical[col].cat.codes\n",
    "train_df_numerical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "3245a0d2-d089-4aed-b371-17001230c88d",
    "_uuid": "b0b99b54af0728512f0566aaf61a5d5b936f5a1a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>523</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>596</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>353</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>669</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>472</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  Name  Sex   Age  SibSp  Parch  Ticket  \\\n",
       "0            1         0       3   108    1  22.0      1      0     523   \n",
       "1            2         1       1   190    0  38.0      1      0     596   \n",
       "2            3         1       3   353    0  26.0      0      0     669   \n",
       "3            4         1       1   272    0  35.0      1      0      49   \n",
       "4            5         0       3    15    1  35.0      0      0     472   \n",
       "\n",
       "      Fare  Cabin  Embarked  \n",
       "0   7.2500    147         2  \n",
       "1  71.2833     81         0  \n",
       "2   7.9250    147         2  \n",
       "3  53.1000     55         2  \n",
       "4   8.0500    147         2  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "e383941d-f0a6-4762-a822-7b44594ce64a",
    "_uuid": "bfa0c2942508189d7e652cb1feb59928c18f4f48",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's make helper function here also\n",
    "def convert_categorical_column_to_integer_values(df):\n",
    "    df_numerical = df.copy()\n",
    "    for col in find_categorical_columns(df):\n",
    "        df_numerical[col] = df_numerical[col].astype('category')\n",
    "        df_numerical[col] = df_numerical[col].cat.codes\n",
    "    return df_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "99a52e82-b995-4330-8c82-78a1fefb0411",
    "_uuid": "a4ff661f428e97f1eb758e5f2ea7766862ffeb03"
   },
   "source": [
    "*Perfect*.\n",
    "\n",
    "Now, we have all of these available for our use : \n",
    "\n",
    "* **train_df**                    : original training dataset   (891,12)\n",
    "* **train_df_dummies**  : training dataset with dummies (891, 1732)\n",
    "* **train_df_numerical** : training dataset with integers for categorical attributes (891,12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f0d77ea-6a9a-46e5-a10f-ea2badd00121",
    "_uuid": "92e693929151e0fe4655805d0a4965fbce207a40"
   },
   "source": [
    "# Running a model in Tensorflow\n",
    "\n",
    "This will again involve a set of steps\n",
    "- Get data converted to numpy arrays so tensorflow can read them\n",
    "- Write tensorflow model\n",
    "- Run a session of tensorflow model and check accuracy on training data set\n",
    "\n",
    "Try the above for both train_df_dummies and train_df_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "50d50bcb-5c32-4585-a429-3f14d164c49f",
    "_uuid": "ac48a581aa491924e8001676169755346c0eae71"
   },
   "outputs": [],
   "source": [
    "# import tensorflow stuff...\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "0b38e3db-1b4e-456c-8450-48ef492ce12f",
    "_uuid": "6475152eeddb252d8857a6214e59b470eb727256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df_numerical_X shape :  (891, 11)\n",
      "train_df_numerical_Y shape :  (891,)\n",
      "train_df_dummies_X shape :  (891, 1731)\n",
      "train_df_dummies_Y shape :  (891,)\n"
     ]
    }
   ],
   "source": [
    "# Dividing data between X and Y\n",
    "# Ref : https://stackoverflow.com/questions/29763620/how-to-select-all-columns-except-one-column-in-pandas\n",
    "\n",
    "train_df_dummies_Y = train_df_dummies['Survived']\n",
    "# Don't worry. drop does not change the existing dataframe unless inplace=True is passed.\n",
    "train_df_dummies_X = train_df_dummies.drop('Survived', axis=1)\n",
    "\n",
    "train_df_numerical_X = train_df_numerical.drop('Survived', axis=1)\n",
    "train_df_numerical_Y = train_df_numerical['Survived']\n",
    "\n",
    "print(\"train_df_numerical_X shape : \",train_df_numerical_X.shape)\n",
    "print(\"train_df_numerical_Y shape : \",train_df_numerical_Y.shape)\n",
    "print(\"train_df_dummies_X shape : \",train_df_dummies_X.shape)\n",
    "print(\"train_df_dummies_Y shape : \",train_df_dummies_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6162d69e-fa84-4f64-95ac-64c7c6acfc5d",
    "_uuid": "ff0063289704a1d675371acf68409992acc4d7f3"
   },
   "source": [
    "### Converting to numpy arrays so tensorflow variables can pick it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "77ee5b0b-829f-42b8-a21d-162ce1dc496c",
    "_uuid": "f0aeadac660453bcdcd3a105ad8530c2e5defc99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX_num.shape =  (891, 11)\n",
      "trainY_num.shape =  (891,)\n",
      "trainX_dummies.shape =  (891, 1731)\n",
      "trainY_dummies.shape =  (891,)\n"
     ]
    }
   ],
   "source": [
    "trainX_num = train_df_numerical_X.as_matrix()\n",
    "trainY_num = train_df_numerical_Y.as_matrix()\n",
    "\n",
    "trainX_dummies = train_df_dummies_X.as_matrix()\n",
    "trainY_dummies = train_df_dummies_Y.as_matrix()\n",
    "\n",
    "print(\"trainX_num.shape = \",trainX_num.shape)\n",
    "print(\"trainY_num.shape = \",trainY_num.shape)\n",
    "print(\"trainX_dummies.shape = \",trainX_dummies.shape)\n",
    "print(\"trainY_dummies.shape = \",trainY_dummies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "ebbdeaf2-5d0a-4e06-ba0f-d57ccf17e73b",
    "_uuid": "71455c6a3d9bb3efac4105f164fcb31dc9e8e556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX_num.shape =  (891, 11)\n",
      "trainY_num.shape =  (891, 1)\n",
      "trainX_dummies.shape =  (891, 1731)\n",
      "trainY_dummies.shape =  (891, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the rank 1 arrays formed to proper 2 dimensions\n",
    "trainY_num = trainY_num[:,np.newaxis]\n",
    "trainY_dummies = trainY_dummies[:,np.newaxis]\n",
    "\n",
    "print(\"trainX_num.shape = \",trainX_num.shape)\n",
    "print(\"trainY_num.shape = \",trainY_num.shape)\n",
    "print(\"trainX_dummies.shape = \",trainX_dummies.shape)\n",
    "print(\"trainY_dummies.shape = \",trainY_dummies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9a6eb8a6-4c9e-44fa-9997-90f5606e2619",
    "_uuid": "bb97c7a4cd92eb2b9bcc6a9863fca1e97e80b17d"
   },
   "source": [
    "### Tensorflow Model\n",
    "\n",
    "Now, let's build our model. \n",
    "We could use existing DNN classifier. But instead, we're gonna build this one with calculations ourselves.\n",
    "2 layers. Hence, W1, b1, W2, b2 as parameters representing weights and biases to layer 1 and layer 2 respectively. \n",
    "\n",
    "We'll use RELU as our activation function for first layer. Why? Because it performs better in general.\n",
    "And sigmoid for the 2nd layer. Since output is going to be a binary classification, it makes sense to use sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "945171b7-3737-4cc7-9d80-5d9f054cf146",
    "_uuid": "510a17c7bac58a2e8c6fe8db0632b941e3680ffe",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Tensorflow model\n",
    "def model(learning_rate, X_arg, Y_arg, num_of_epochs):\n",
    "    # 1. Placeholders to hold data\n",
    "    X = tf.placeholder(tf.float32, [11,None])\n",
    "    Y = tf.placeholder(tf.float32, [1, None])\n",
    "\n",
    "    # 2. Model. 2 layers NN. So, W1, b1, W2, b2.\n",
    "    # This is basically coding forward propagation formulaes\n",
    "    W1 = tf.Variable(tf.random_normal((20,11)))\n",
    "    b1 = tf.Variable(tf.zeros((20,1)))\n",
    "    Z1 = tf.matmul(W1,X) + b1             # This is also called logits in tensorflow terms\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal((1, 20)))\n",
    "    b2 = tf.Variable(tf.zeros((1,1)))\n",
    "    Z2 = tf.matmul(W2,A1) + b2\n",
    "    A2 = tf.nn.sigmoid(Z2)\n",
    "\n",
    "    # 3. Calculate cost\n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z2, labels=Y)\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "\n",
    "    # 4. Optimizer (Gradient Descent / AdamOptimizer ) - Using this line, tensorflow automatically does backpropagation\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost_mean)\n",
    "    \n",
    "    # 5. initialize variabls\n",
    "    session = tf.Session()\n",
    "    tf.set_random_seed(1)\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    \n",
    "    # 6. Actual loop where learning happens\n",
    "    for i in range(num_of_epochs):\n",
    "        _, cost_mean_val = session.run([optimizer, cost_mean], feed_dict={X:X_arg, Y:Y_arg})\n",
    "        if i % 100 == 0:\n",
    "            print(\"i : \",i,\", cost : \",cost_mean_val)\n",
    "            \n",
    "    return session.run([W1,b1,W2,b2,A2,Y],feed_dict={X:X_arg, Y:Y_arg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "def67d5f-fcfd-45bd-b4bb-0e2575b6d787",
    "_uuid": "108c40bde0fe872da823f000f9568ac959687e77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i :  0 , cost :  3030.03\n",
      "i :  100 , cost :  91.9023\n",
      "i :  200 , cost :  40.3933\n",
      "i :  300 , cost :  7.13846\n",
      "i :  400 , cost :  12.6343\n",
      "i :  500 , cost :  5.83331\n",
      "i :  600 , cost :  2.08085\n",
      "i :  700 , cost :  1.58742\n",
      "i :  800 , cost :  5.6428\n",
      "i :  900 , cost :  3.63964\n",
      "i :  1000 , cost :  2.19955\n",
      "i :  1100 , cost :  2.23141\n",
      "i :  1200 , cost :  1.67917\n",
      "i :  1300 , cost :  1.24362\n",
      "i :  1400 , cost :  1.01248\n",
      "i :  1500 , cost :  2.31802\n",
      "i :  1600 , cost :  1.34666\n",
      "i :  1700 , cost :  1.44293\n",
      "i :  1800 , cost :  1.17003\n",
      "i :  1900 , cost :  0.926903\n",
      "i :  2000 , cost :  6.90624\n",
      "i :  2100 , cost :  1.24673\n",
      "i :  2200 , cost :  3.33068\n",
      "i :  2300 , cost :  2.26824\n",
      "i :  2400 , cost :  1.08037\n",
      "i :  2500 , cost :  0.876024\n",
      "i :  2600 , cost :  22.3073\n",
      "i :  2700 , cost :  1.11995\n",
      "i :  2800 , cost :  2.93875\n",
      "i :  2900 , cost :  1.62486\n"
     ]
    }
   ],
   "source": [
    "W1_tr,b1_tr,W2_tr,b2_tr,A2,Y = model(0.01, trainX_num.T, trainY_num.T, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "e6bd600c-8faf-4982-a48a-7b7f97280cd3",
    "_uuid": "5342004533241f23a260a21d39ec757bcceda874"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 891)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validating that our formulaes were correct by checking shapes of ouput prediction\n",
    "A2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "ec9bc3f8-ea91-4cae-a5f5-decd5745a2cb",
    "_uuid": "5ab614f0bb50ca3ca8373cc839b475d7bcd6eb25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 891)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "46fae9fa-f9ff-453b-824e-fa8e468e035c",
    "_uuid": "946213023625b02d635fe3d953efc0e9bdbdcb6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  1.        ,  0.92490357,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the predictions variable\n",
    "A2[:,0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a36cb857-1c47-42d9-811d-62377f88e8f9",
    "_uuid": "5f299e26988322bb2100f7dcfd56678d2f0ba989"
   },
   "source": [
    "**As we see, our predictions array isn't 0s or 1s. So, we must convert it to 0s / 1s. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "095c8328-db5c-403b-a6f2-0ca5a82eeaff",
    "_uuid": "3925356e326d4c1f0290f063f389420c9a4138b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A2_bool = A2 > 0.5\n",
    "Y_prediction_training = A2_bool.astype(int)\n",
    "Y_int = Y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "304034f8-912c-48e6-a609-e4d97ccd7cb3",
    "_uuid": "66e3d20d1ef485524f453fdae28b623797fbe9c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "d5d55c08-39ce-47f8-8119-e0c52e09f505",
    "_uuid": "ccf93485ec714a28e72256091e115c12d4557dc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_prediction_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "dbd494aa-be4c-4765-bf33-8a9280da2f50",
    "_uuid": "8c78b9abc22f6805d03195ee44962f2012ad9793"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80246913580246915"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (Y_prediction_training == Y_int).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd4624eb-5c9d-4fbe-b1c5-bc1da03f3394",
    "_uuid": "46c0e766df485a688181fa27e2e30d41c3247a68"
   },
   "source": [
    "### Awesome\n",
    "\n",
    "81.48% accuracy isn't bad on training dataset. That too, with just 3000 epochs!\n",
    "\n",
    "People got near 85% with 40000 epochs. So, it's fine. This is good enough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "088f830e-6750-4458-935f-e551a8d7903a",
    "_uuid": "458dd029f8623870e0a0682c2d924f9f1da8fe7f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, let's list our helper functions we could make from logic used above.\n",
    "def convert_sigmoid_output_to_boolean_array(array, threshold):\n",
    "    array = array > threshold\n",
    "    return array\n",
    "\n",
    "def convert_boolean_array_to_binary_array(array):\n",
    "    array_binary = array.astype(int)\n",
    "    return array_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c44db26c-5bdf-430c-af0d-080a7971d90c",
    "_uuid": "fb614c70877b5f3ce45d61841480d89772250f45"
   },
   "source": [
    "**Let's try now with dummies wala data.**\n",
    "\n",
    "This is the time. Let's generalize the model we wrote above to take more arguments and not be specific to shapes of our X or Y.\n",
    "Also, let's now print the training accuracy in the model itself with the cost at each 100th epoch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "7f4c3853-c9ca-4a5e-b9b8-4a06fdf8a8c3",
    "_uuid": "dfdadd2396db77a905b9e10595000ebbae8e9fa0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Tensorflow model\n",
    "def model_generic(learning_rate, X_arg, Y_arg, num_of_epochs, hidden_units, threshold):\n",
    "    # 1. Placeholders to hold data\n",
    "    X = tf.placeholder(tf.float32, [X_arg.shape[0],None])\n",
    "    Y = tf.placeholder(tf.float32, [1, None])\n",
    "\n",
    "    # 2. Model. 2 layers NN. So, W1, b1, W2, b2.\n",
    "    # This is basically coding forward propagation formulaes\n",
    "    W1 = tf.Variable(tf.random_normal((hidden_units,X_arg.shape[0])))\n",
    "    b1 = tf.Variable(tf.zeros((hidden_units,1)))\n",
    "    Z1 = tf.matmul(W1,X) + b1\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal((1, hidden_units)))\n",
    "    b2 = tf.Variable(tf.zeros((1,1)))\n",
    "    Z2 = tf.matmul(W2,A1) + b2\n",
    "    A2 = tf.nn.sigmoid(Z2)\n",
    "\n",
    "    # 3. Calculate cost\n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z2, labels=Y)\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "\n",
    "    # 4. Optimizer (Gradient Descent / AdamOptimizer ) - Using this line, tensorflow automatically does backpropagation\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost_mean)\n",
    "    \n",
    "    # 5. Accuracy methods\n",
    "    predicted_class = tf.greater(A2,threshold)\n",
    "    prediction_arr = tf.equal(predicted_class, tf.equal(Y,1.0))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction_arr, tf.float32))\n",
    "    \n",
    "    # 5. initialize variabls\n",
    "    session = tf.Session()\n",
    "    tf.set_random_seed(1)\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    \n",
    "    # 6. Actual loop where learning happens\n",
    "    for i in range(num_of_epochs):\n",
    "        _, cost_mean_val, accuracy_val = session.run([optimizer, cost_mean, accuracy], feed_dict={X:X_arg, Y:Y_arg})\n",
    "        if i % 100 == 0:\n",
    "            print(\"i:\",i,\", cost : \",cost_mean_val,\", training accuracy : \",accuracy_val)\n",
    "            \n",
    "    return session.run([W1,b1,W2,b2,A2,Y,accuracy],feed_dict={X:X_arg, Y:Y_arg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "afa7a982-45a4-4be2-8d32-61aff199831a",
    "_uuid": "db2029eb89368b236cbf6b55b89e6244cd383bb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 , cost :  2395.99 , training accuracy :  0.616162\n",
      "i: 100 , cost :  67.3294 , training accuracy :  0.61055\n",
      "i: 200 , cost :  20.3746 , training accuracy :  0.676768\n",
      "i: 300 , cost :  12.0113 , training accuracy :  0.735129\n",
      "i: 400 , cost :  8.66318 , training accuracy :  0.762065\n",
      "i: 500 , cost :  6.96889 , training accuracy :  0.780022\n",
      "i: 600 , cost :  6.11968 , training accuracy :  0.790123\n",
      "i: 700 , cost :  5.93834 , training accuracy :  0.805836\n",
      "i: 800 , cost :  7.09067 , training accuracy :  0.790123\n",
      "i: 900 , cost :  5.11551 , training accuracy :  0.801347\n",
      "i: 1000 , cost :  4.36806 , training accuracy :  0.819304\n",
      "i: 1100 , cost :  6.33762 , training accuracy :  0.799102\n",
      "i: 1200 , cost :  4.45729 , training accuracy :  0.813693\n",
      "i: 1300 , cost :  4.10381 , training accuracy :  0.82716\n",
      "i: 1400 , cost :  3.39415 , training accuracy :  0.824916\n",
      "i: 1500 , cost :  5.11004 , training accuracy :  0.803591\n",
      "i: 1600 , cost :  5.91911 , training accuracy :  0.794613\n",
      "i: 1700 , cost :  9.30856 , training accuracy :  0.682379\n",
      "i: 1800 , cost :  2.872 , training accuracy :  0.851852\n",
      "i: 1900 , cost :  2.76268 , training accuracy :  0.842873\n",
      "i: 2000 , cost :  2.56319 , training accuracy :  0.845118\n",
      "i: 2100 , cost :  3.13857 , training accuracy :  0.81257\n",
      "i: 2200 , cost :  2.67441 , training accuracy :  0.838384\n",
      "i: 2300 , cost :  2.93557 , training accuracy :  0.813693\n",
      "i: 2400 , cost :  3.59246 , training accuracy :  0.819304\n",
      "i: 2500 , cost :  3.13911 , training accuracy :  0.833894\n",
      "i: 2600 , cost :  4.16979 , training accuracy :  0.804714\n",
      "i: 2700 , cost :  1.98198 , training accuracy :  0.874299\n",
      "i: 2800 , cost :  4.87771 , training accuracy :  0.801347\n",
      "i: 2900 , cost :  4.12083 , training accuracy :  0.821549\n"
     ]
    }
   ],
   "source": [
    "W1_dum,b1_dum,W2_dum,b2_dum,A2_dummies,Y_dummies,training_accuracy_val = model_generic(0.005, trainX_num.T, trainY_num.T, 3000, 100,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "59620bf487f9433e207e994d046c169134d93169"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83501685"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    training_accuracy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "07c54abc-a6e5-46c1-8e2d-995f38fdd333",
    "_uuid": "bd38ff125de91f524217ebe0ca7bb48bec3936e3"
   },
   "source": [
    "So, for when we use dummies data, accuracy goes up and down, and after 3000 epochs is somewhere near 85.52%. This is good only! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e9fc019d-fac7-4f7b-97d7-0ddcb8054209",
    "_uuid": "044349742f1dd1b05439e3facbaed36f28189c66"
   },
   "source": [
    "# Prediction on Test Data\n",
    "\n",
    "Let's use numerical wala data only now.\n",
    "- Converting test data in the same form\n",
    "- Pass it through the network to get the value of A2\n",
    "- Concatenate this with the data and write that into csv\n",
    "- Submit the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "ecaab17b-0ec5-417b-bfc4-7b06d612fbba",
    "_uuid": "ee02bda7bfc74c67424bc809fac22882569d2aa0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>3</td>\n",
       "      <td>Svensson, Mr. Johan Cervin</td>\n",
       "      <td>male</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7538</td>\n",
       "      <td>9.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>3</td>\n",
       "      <td>Connolly, Miss. Kate</td>\n",
       "      <td>female</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330972</td>\n",
       "      <td>7.6292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>2</td>\n",
       "      <td>Caldwell, Mr. Albert Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>248738</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>3</td>\n",
       "      <td>Abrahim, Mrs. Joseph (Sophie Halaut Easu)</td>\n",
       "      <td>female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2657</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>3</td>\n",
       "      <td>Davies, Mr. John Samuel</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>A/4 48871</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>902</td>\n",
       "      <td>3</td>\n",
       "      <td>Ilieff, Mr. Ylio</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349220</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>903</td>\n",
       "      <td>1</td>\n",
       "      <td>Jones, Mr. Charles Cresson</td>\n",
       "      <td>male</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>694</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>904</td>\n",
       "      <td>1</td>\n",
       "      <td>Snyder, Mrs. John Pillsbury (Nelle Stevenson)</td>\n",
       "      <td>female</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21228</td>\n",
       "      <td>82.2667</td>\n",
       "      <td>B45</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>905</td>\n",
       "      <td>2</td>\n",
       "      <td>Howard, Mr. Benjamin</td>\n",
       "      <td>male</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24065</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>906</td>\n",
       "      <td>1</td>\n",
       "      <td>Chaffee, Mrs. Herbert Fuller (Carrie Constance...</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>W.E.P. 5734</td>\n",
       "      <td>61.1750</td>\n",
       "      <td>E31</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>907</td>\n",
       "      <td>2</td>\n",
       "      <td>del Carlo, Mrs. Sebastiano (Argenia Genovesi)</td>\n",
       "      <td>female</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>SC/PARIS 2167</td>\n",
       "      <td>27.7208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>908</td>\n",
       "      <td>2</td>\n",
       "      <td>Keane, Mr. Daniel</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>233734</td>\n",
       "      <td>12.3500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>909</td>\n",
       "      <td>3</td>\n",
       "      <td>Assaf, Mr. Gerios</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2692</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>910</td>\n",
       "      <td>3</td>\n",
       "      <td>Ilmakangas, Miss. Ida Livija</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101270</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>911</td>\n",
       "      <td>3</td>\n",
       "      <td>Assaf Khalil, Mrs. Mariana (Miriam\")\"</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2696</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>912</td>\n",
       "      <td>1</td>\n",
       "      <td>Rothschild, Mr. Martin</td>\n",
       "      <td>male</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17603</td>\n",
       "      <td>59.4000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>913</td>\n",
       "      <td>3</td>\n",
       "      <td>Olsen, Master. Artur Karl</td>\n",
       "      <td>male</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>C 17368</td>\n",
       "      <td>3.1708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "      <td>Flegenheim, Mrs. Alfred (Antoinette)</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17598</td>\n",
       "      <td>31.6833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>915</td>\n",
       "      <td>1</td>\n",
       "      <td>Williams, Mr. Richard Norris II</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PC 17597</td>\n",
       "      <td>61.3792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>916</td>\n",
       "      <td>1</td>\n",
       "      <td>Ryerson, Mrs. Arthur Larned (Emily Maria Borie)</td>\n",
       "      <td>female</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>PC 17608</td>\n",
       "      <td>262.3750</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>917</td>\n",
       "      <td>3</td>\n",
       "      <td>Robins, Mr. Alexander A</td>\n",
       "      <td>male</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5. 3337</td>\n",
       "      <td>14.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>918</td>\n",
       "      <td>1</td>\n",
       "      <td>Ostby, Miss. Helene Ragnhild</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>113509</td>\n",
       "      <td>61.9792</td>\n",
       "      <td>B36</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>919</td>\n",
       "      <td>3</td>\n",
       "      <td>Daher, Mr. Shedid</td>\n",
       "      <td>male</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2698</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>920</td>\n",
       "      <td>1</td>\n",
       "      <td>Brady, Mr. John Bertram</td>\n",
       "      <td>male</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113054</td>\n",
       "      <td>30.5000</td>\n",
       "      <td>A21</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>921</td>\n",
       "      <td>3</td>\n",
       "      <td>Samaan, Mr. Elias</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2662</td>\n",
       "      <td>21.6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1280</td>\n",
       "      <td>3</td>\n",
       "      <td>Canavan, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>364858</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1281</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Paul Folke</td>\n",
       "      <td>male</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1282</td>\n",
       "      <td>1</td>\n",
       "      <td>Payne, Mr. Vivian Ponsonby</td>\n",
       "      <td>male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12749</td>\n",
       "      <td>93.5000</td>\n",
       "      <td>B24</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1283</td>\n",
       "      <td>1</td>\n",
       "      <td>Lines, Mrs. Ernest H (Elizabeth Lindsey James)</td>\n",
       "      <td>female</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PC 17592</td>\n",
       "      <td>39.4000</td>\n",
       "      <td>D28</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1284</td>\n",
       "      <td>3</td>\n",
       "      <td>Abbott, Master. Eugene Joseph</td>\n",
       "      <td>male</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>C.A. 2673</td>\n",
       "      <td>20.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1285</td>\n",
       "      <td>2</td>\n",
       "      <td>Gilbert, Mr. William</td>\n",
       "      <td>male</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C.A. 30769</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1286</td>\n",
       "      <td>3</td>\n",
       "      <td>Kink-Heilmann, Mr. Anton</td>\n",
       "      <td>male</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>315153</td>\n",
       "      <td>22.0250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1287</td>\n",
       "      <td>1</td>\n",
       "      <td>Smith, Mrs. Lucien Philip (Mary Eloise Hughes)</td>\n",
       "      <td>female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13695</td>\n",
       "      <td>60.0000</td>\n",
       "      <td>C31</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1288</td>\n",
       "      <td>3</td>\n",
       "      <td>Colbert, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>371109</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1289</td>\n",
       "      <td>1</td>\n",
       "      <td>Frolicher-Stehli, Mrs. Maxmillian (Margaretha ...</td>\n",
       "      <td>female</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13567</td>\n",
       "      <td>79.2000</td>\n",
       "      <td>B41</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1290</td>\n",
       "      <td>3</td>\n",
       "      <td>Larsson-Rondberg, Mr. Edvard A</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>347065</td>\n",
       "      <td>7.7750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1291</td>\n",
       "      <td>3</td>\n",
       "      <td>Conlon, Mr. Thomas Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21332</td>\n",
       "      <td>7.7333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "      <td>Bonnell, Miss. Caroline</td>\n",
       "      <td>female</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36928</td>\n",
       "      <td>164.8667</td>\n",
       "      <td>C7</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1293</td>\n",
       "      <td>2</td>\n",
       "      <td>Gale, Mr. Harry</td>\n",
       "      <td>male</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28664</td>\n",
       "      <td>21.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1294</td>\n",
       "      <td>1</td>\n",
       "      <td>Gibson, Miss. Dorothy Winifred</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112378</td>\n",
       "      <td>59.4000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1295</td>\n",
       "      <td>1</td>\n",
       "      <td>Carrau, Mr. Jose Pedro</td>\n",
       "      <td>male</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113059</td>\n",
       "      <td>47.1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1296</td>\n",
       "      <td>1</td>\n",
       "      <td>Frauenthal, Mr. Isaac Gerald</td>\n",
       "      <td>male</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17765</td>\n",
       "      <td>27.7208</td>\n",
       "      <td>D40</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1297</td>\n",
       "      <td>2</td>\n",
       "      <td>Nourney, Mr. Alfred (Baron von Drachstedt\")\"</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SC/PARIS 2166</td>\n",
       "      <td>13.8625</td>\n",
       "      <td>D38</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1298</td>\n",
       "      <td>2</td>\n",
       "      <td>Ware, Mr. William Jeffery</td>\n",
       "      <td>male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28666</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1299</td>\n",
       "      <td>1</td>\n",
       "      <td>Widener, Mr. George Dunton</td>\n",
       "      <td>male</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>113503</td>\n",
       "      <td>211.5000</td>\n",
       "      <td>C80</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1300</td>\n",
       "      <td>3</td>\n",
       "      <td>Riordan, Miss. Johanna Hannah\"\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>334915</td>\n",
       "      <td>7.7208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1301</td>\n",
       "      <td>3</td>\n",
       "      <td>Peacock, Miss. Treasteall</td>\n",
       "      <td>female</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SOTON/O.Q. 3101315</td>\n",
       "      <td>13.7750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1302</td>\n",
       "      <td>3</td>\n",
       "      <td>Naughton, Miss. Hannah</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>365237</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1303</td>\n",
       "      <td>1</td>\n",
       "      <td>Minahan, Mrs. William Edward (Lillian E Thorpe)</td>\n",
       "      <td>female</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19928</td>\n",
       "      <td>90.0000</td>\n",
       "      <td>C78</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1304</td>\n",
       "      <td>3</td>\n",
       "      <td>Henriksson, Miss. Jenny Lovisa</td>\n",
       "      <td>female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>347086</td>\n",
       "      <td>7.7750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>3</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>3</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>male</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>3</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                               Name  \\\n",
       "0            892       3                                   Kelly, Mr. James   \n",
       "1            893       3                   Wilkes, Mrs. James (Ellen Needs)   \n",
       "2            894       2                          Myles, Mr. Thomas Francis   \n",
       "3            895       3                                   Wirz, Mr. Albert   \n",
       "4            896       3       Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
       "5            897       3                         Svensson, Mr. Johan Cervin   \n",
       "6            898       3                               Connolly, Miss. Kate   \n",
       "7            899       2                       Caldwell, Mr. Albert Francis   \n",
       "8            900       3          Abrahim, Mrs. Joseph (Sophie Halaut Easu)   \n",
       "9            901       3                            Davies, Mr. John Samuel   \n",
       "10           902       3                                   Ilieff, Mr. Ylio   \n",
       "11           903       1                         Jones, Mr. Charles Cresson   \n",
       "12           904       1      Snyder, Mrs. John Pillsbury (Nelle Stevenson)   \n",
       "13           905       2                               Howard, Mr. Benjamin   \n",
       "14           906       1  Chaffee, Mrs. Herbert Fuller (Carrie Constance...   \n",
       "15           907       2      del Carlo, Mrs. Sebastiano (Argenia Genovesi)   \n",
       "16           908       2                                  Keane, Mr. Daniel   \n",
       "17           909       3                                  Assaf, Mr. Gerios   \n",
       "18           910       3                       Ilmakangas, Miss. Ida Livija   \n",
       "19           911       3              Assaf Khalil, Mrs. Mariana (Miriam\")\"   \n",
       "20           912       1                             Rothschild, Mr. Martin   \n",
       "21           913       3                          Olsen, Master. Artur Karl   \n",
       "22           914       1               Flegenheim, Mrs. Alfred (Antoinette)   \n",
       "23           915       1                    Williams, Mr. Richard Norris II   \n",
       "24           916       1    Ryerson, Mrs. Arthur Larned (Emily Maria Borie)   \n",
       "25           917       3                            Robins, Mr. Alexander A   \n",
       "26           918       1                       Ostby, Miss. Helene Ragnhild   \n",
       "27           919       3                                  Daher, Mr. Shedid   \n",
       "28           920       1                            Brady, Mr. John Bertram   \n",
       "29           921       3                                  Samaan, Mr. Elias   \n",
       "..           ...     ...                                                ...   \n",
       "388         1280       3                               Canavan, Mr. Patrick   \n",
       "389         1281       3                        Palsson, Master. Paul Folke   \n",
       "390         1282       1                         Payne, Mr. Vivian Ponsonby   \n",
       "391         1283       1     Lines, Mrs. Ernest H (Elizabeth Lindsey James)   \n",
       "392         1284       3                      Abbott, Master. Eugene Joseph   \n",
       "393         1285       2                               Gilbert, Mr. William   \n",
       "394         1286       3                           Kink-Heilmann, Mr. Anton   \n",
       "395         1287       1     Smith, Mrs. Lucien Philip (Mary Eloise Hughes)   \n",
       "396         1288       3                               Colbert, Mr. Patrick   \n",
       "397         1289       1  Frolicher-Stehli, Mrs. Maxmillian (Margaretha ...   \n",
       "398         1290       3                     Larsson-Rondberg, Mr. Edvard A   \n",
       "399         1291       3                           Conlon, Mr. Thomas Henry   \n",
       "400         1292       1                            Bonnell, Miss. Caroline   \n",
       "401         1293       2                                    Gale, Mr. Harry   \n",
       "402         1294       1                     Gibson, Miss. Dorothy Winifred   \n",
       "403         1295       1                             Carrau, Mr. Jose Pedro   \n",
       "404         1296       1                       Frauenthal, Mr. Isaac Gerald   \n",
       "405         1297       2       Nourney, Mr. Alfred (Baron von Drachstedt\")\"   \n",
       "406         1298       2                          Ware, Mr. William Jeffery   \n",
       "407         1299       1                         Widener, Mr. George Dunton   \n",
       "408         1300       3                    Riordan, Miss. Johanna Hannah\"\"   \n",
       "409         1301       3                          Peacock, Miss. Treasteall   \n",
       "410         1302       3                             Naughton, Miss. Hannah   \n",
       "411         1303       1    Minahan, Mrs. William Edward (Lillian E Thorpe)   \n",
       "412         1304       3                     Henriksson, Miss. Jenny Lovisa   \n",
       "413         1305       3                                 Spector, Mr. Woolf   \n",
       "414         1306       1                       Oliva y Ocana, Dona. Fermina   \n",
       "415         1307       3                       Saether, Mr. Simon Sivertsen   \n",
       "416         1308       3                                Ware, Mr. Frederick   \n",
       "417         1309       3                           Peter, Master. Michael J   \n",
       "\n",
       "        Sex   Age  SibSp  Parch              Ticket      Fare  \\\n",
       "0      male  34.5      0      0              330911    7.8292   \n",
       "1    female  47.0      1      0              363272    7.0000   \n",
       "2      male  62.0      0      0              240276    9.6875   \n",
       "3      male  27.0      0      0              315154    8.6625   \n",
       "4    female  22.0      1      1             3101298   12.2875   \n",
       "5      male  14.0      0      0                7538    9.2250   \n",
       "6    female  30.0      0      0              330972    7.6292   \n",
       "7      male  26.0      1      1              248738   29.0000   \n",
       "8    female  18.0      0      0                2657    7.2292   \n",
       "9      male  21.0      2      0           A/4 48871   24.1500   \n",
       "10     male   NaN      0      0              349220    7.8958   \n",
       "11     male  46.0      0      0                 694   26.0000   \n",
       "12   female  23.0      1      0               21228   82.2667   \n",
       "13     male  63.0      1      0               24065   26.0000   \n",
       "14   female  47.0      1      0         W.E.P. 5734   61.1750   \n",
       "15   female  24.0      1      0       SC/PARIS 2167   27.7208   \n",
       "16     male  35.0      0      0              233734   12.3500   \n",
       "17     male  21.0      0      0                2692    7.2250   \n",
       "18   female  27.0      1      0    STON/O2. 3101270    7.9250   \n",
       "19   female  45.0      0      0                2696    7.2250   \n",
       "20     male  55.0      1      0            PC 17603   59.4000   \n",
       "21     male   9.0      0      1             C 17368    3.1708   \n",
       "22   female   NaN      0      0            PC 17598   31.6833   \n",
       "23     male  21.0      0      1            PC 17597   61.3792   \n",
       "24   female  48.0      1      3            PC 17608  262.3750   \n",
       "25     male  50.0      1      0           A/5. 3337   14.5000   \n",
       "26   female  22.0      0      1              113509   61.9792   \n",
       "27     male  22.5      0      0                2698    7.2250   \n",
       "28     male  41.0      0      0              113054   30.5000   \n",
       "29     male   NaN      2      0                2662   21.6792   \n",
       "..      ...   ...    ...    ...                 ...       ...   \n",
       "388    male  21.0      0      0              364858    7.7500   \n",
       "389    male   6.0      3      1              349909   21.0750   \n",
       "390    male  23.0      0      0               12749   93.5000   \n",
       "391  female  51.0      0      1            PC 17592   39.4000   \n",
       "392    male  13.0      0      2           C.A. 2673   20.2500   \n",
       "393    male  47.0      0      0          C.A. 30769   10.5000   \n",
       "394    male  29.0      3      1              315153   22.0250   \n",
       "395  female  18.0      1      0               13695   60.0000   \n",
       "396    male  24.0      0      0              371109    7.2500   \n",
       "397  female  48.0      1      1               13567   79.2000   \n",
       "398    male  22.0      0      0              347065    7.7750   \n",
       "399    male  31.0      0      0               21332    7.7333   \n",
       "400  female  30.0      0      0               36928  164.8667   \n",
       "401    male  38.0      1      0               28664   21.0000   \n",
       "402  female  22.0      0      1              112378   59.4000   \n",
       "403    male  17.0      0      0              113059   47.1000   \n",
       "404    male  43.0      1      0               17765   27.7208   \n",
       "405    male  20.0      0      0       SC/PARIS 2166   13.8625   \n",
       "406    male  23.0      1      0               28666   10.5000   \n",
       "407    male  50.0      1      1              113503  211.5000   \n",
       "408  female   NaN      0      0              334915    7.7208   \n",
       "409  female   3.0      1      1  SOTON/O.Q. 3101315   13.7750   \n",
       "410  female   NaN      0      0              365237    7.7500   \n",
       "411  female  37.0      1      0               19928   90.0000   \n",
       "412  female  28.0      0      0              347086    7.7750   \n",
       "413    male   NaN      0      0           A.5. 3236    8.0500   \n",
       "414  female  39.0      0      0            PC 17758  108.9000   \n",
       "415    male  38.5      0      0  SOTON/O.Q. 3101262    7.2500   \n",
       "416    male   NaN      0      0              359309    8.0500   \n",
       "417    male   NaN      1      1                2668   22.3583   \n",
       "\n",
       "               Cabin Embarked  \n",
       "0                NaN        Q  \n",
       "1                NaN        S  \n",
       "2                NaN        Q  \n",
       "3                NaN        S  \n",
       "4                NaN        S  \n",
       "5                NaN        S  \n",
       "6                NaN        Q  \n",
       "7                NaN        S  \n",
       "8                NaN        C  \n",
       "9                NaN        S  \n",
       "10               NaN        S  \n",
       "11               NaN        S  \n",
       "12               B45        S  \n",
       "13               NaN        S  \n",
       "14               E31        S  \n",
       "15               NaN        C  \n",
       "16               NaN        Q  \n",
       "17               NaN        C  \n",
       "18               NaN        S  \n",
       "19               NaN        C  \n",
       "20               NaN        C  \n",
       "21               NaN        S  \n",
       "22               NaN        S  \n",
       "23               NaN        C  \n",
       "24   B57 B59 B63 B66        C  \n",
       "25               NaN        S  \n",
       "26               B36        C  \n",
       "27               NaN        C  \n",
       "28               A21        S  \n",
       "29               NaN        C  \n",
       "..               ...      ...  \n",
       "388              NaN        Q  \n",
       "389              NaN        S  \n",
       "390              B24        S  \n",
       "391              D28        S  \n",
       "392              NaN        S  \n",
       "393              NaN        S  \n",
       "394              NaN        S  \n",
       "395              C31        S  \n",
       "396              NaN        Q  \n",
       "397              B41        C  \n",
       "398              NaN        S  \n",
       "399              NaN        Q  \n",
       "400               C7        S  \n",
       "401              NaN        S  \n",
       "402              NaN        C  \n",
       "403              NaN        S  \n",
       "404              D40        C  \n",
       "405              D38        C  \n",
       "406              NaN        S  \n",
       "407              C80        C  \n",
       "408              NaN        Q  \n",
       "409              NaN        S  \n",
       "410              NaN        Q  \n",
       "411              C78        Q  \n",
       "412              NaN        S  \n",
       "413              NaN        S  \n",
       "414             C105        C  \n",
       "415              NaN        S  \n",
       "416              NaN        S  \n",
       "417              NaN        C  \n",
       "\n",
       "[418 rows x 11 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "2cd38d20-6b31-43ec-a813-59780206442a",
    "_uuid": "5566bdcdc694176854ec0754e9ce3aae4230df92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    False\n",
       "Pclass         False\n",
       "Name           False\n",
       "Sex            False\n",
       "Age             True\n",
       "SibSp          False\n",
       "Parch          False\n",
       "Ticket         False\n",
       "Fare            True\n",
       "Cabin           True\n",
       "Embarked       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "3574ca83-ecc3-4902-b813-4e50dd99cb25",
    "_uuid": "de5924a5250e35b7cb8e3339c0e7cfe6cbc957c6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df['Age'] = test_df['Age'].fillna(test_df['Age'].mean())\n",
    "test_df['Fare'] = test_df['Fare'].fillna(test_df['Fare'].mean())\n",
    "test_df['Cabin'] = test_df['Cabin'].fillna('UNKNOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "2653ba68-e862-422a-8dc5-6cb8491a2e39",
    "_uuid": "58f8b29466bd7462ef68231abb9c8a4f2402020f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    False\n",
       "Pclass         False\n",
       "Name           False\n",
       "Sex            False\n",
       "Age            False\n",
       "SibSp          False\n",
       "Parch          False\n",
       "Ticket         False\n",
       "Fare           False\n",
       "Cabin          False\n",
       "Embarked       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "6a8420c2-e80a-40e2-b853-7fd8b2f3937a",
    "_uuid": "c10aa1355d89b8f838f92a035919643c78ed1f5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, 11)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting to numerical data\n",
    "test_df_numerical = test_df.copy()\n",
    "for col in categorical_cols:\n",
    "    test_df_numerical[col] = test_df_numerical[col].astype('category')\n",
    "    test_df_numerical[col] = test_df_numerical[col].cat.codes\n",
    "test_df_numerical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "fb614986-eb50-4978-9a0c-48093bbc52fd",
    "_uuid": "58c1b2bd793bb937da91c062c6ff73fb9f1722e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>206</td>\n",
       "      <td>1</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>221</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>269</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>408</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass  Name  Sex   Age  SibSp  Parch  Ticket     Fare  Cabin  \\\n",
       "0          892       3   206    1  34.5      0      0     152   7.8292     76   \n",
       "1          893       3   403    0  47.0      1      0     221   7.0000     76   \n",
       "2          894       2   269    1  62.0      0      0      73   9.6875     76   \n",
       "3          895       3   408    1  27.0      0      0     147   8.6625     76   \n",
       "4          896       3   178    0  22.0      1      1     138  12.2875     76   \n",
       "\n",
       "   Embarked  \n",
       "0         1  \n",
       "1         2  \n",
       "2         1  \n",
       "3         2  \n",
       "4         2  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "34485ef8-4b81-4063-8dd7-d6aae80b0255",
    "_uuid": "1ec0994a48dcd7509dbaa26d1ae98596e8938794",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# Ref : https://stackoverflow.com/questions/32109319/how-to-implement-the-relu-function-in-numpy\n",
    "# Ref : https://stackoverflow.com/questions/3985619/how-to-calculate-a-logistic-sigmoid-function-in-python\n",
    "def predict(W1,b1,W2,b2,X):\n",
    "    \n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.maximum(Z1, 0, Z1)\n",
    "    \n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = 1 / (1 + np.exp(-Z2))\n",
    "    return A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_cell_guid": "cac735aa-b3d8-427c-91b0-6790d1aaaa09",
    "_uuid": "b2da5f548439c867c9d86c3fa25ddd8e4636a5c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, 11)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's predict\n",
    "X_test = test_df_numerical.as_matrix()\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_cell_guid": "9c1501b6-a205-4554-af93-fd6236fb63f0",
    "_uuid": "25dce3714aae8053480baa90d0e5255e039591d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 11)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_cell_guid": "444e923c-53c0-4056-beb3-9c85eadbdc3f",
    "_uuid": "1ffcee8fb44ce9139824a923be8c5122c84cf072"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_cell_guid": "52bfb483-4ba2-44fb-baff-a159abeb1931",
    "_uuid": "5eb489f7f6335574d6cd6229fef95df5c9fdfb6c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_prediction = predict(W1_tr,b1_tr,W2_tr,b2_tr,X_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_cell_guid": "73f86075-3aa0-49e3-afa1-044450d26cda",
    "_uuid": "9e606456b38d8fbab46e590af9e14fc2ba03ef78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 418)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prediction_int = final_prediction > 0.5\n",
    "final_prediction_int = final_prediction_int.astype(int)\n",
    "final_prediction_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_cell_guid": "370872ce-9c70-4a9d-b0ec-0fbc21b8b890",
    "_uuid": "199641f87a01b28d16c5069792188da78a9f0951"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived\n",
       "0           0\n",
       "1           1\n",
       "2           0\n",
       "3           0\n",
       "4           1\n",
       "5           0\n",
       "6           1\n",
       "7           0\n",
       "8           0\n",
       "9           0\n",
       "10          0\n",
       "11          1\n",
       "12          1\n",
       "13          0\n",
       "14          1\n",
       "15          1\n",
       "16          0\n",
       "17          0\n",
       "18          1\n",
       "19          1\n",
       "20          1\n",
       "21          0\n",
       "22          1\n",
       "23          1\n",
       "24          1\n",
       "25          0\n",
       "26          1\n",
       "27          0\n",
       "28          1\n",
       "29          0\n",
       "..        ...\n",
       "388         0\n",
       "389         0\n",
       "390         1\n",
       "391         1\n",
       "392         0\n",
       "393         0\n",
       "394         0\n",
       "395         1\n",
       "396         0\n",
       "397         1\n",
       "398         0\n",
       "399         0\n",
       "400         1\n",
       "401         0\n",
       "402         1\n",
       "403         0\n",
       "404         1\n",
       "405         1\n",
       "406         0\n",
       "407         1\n",
       "408         1\n",
       "409         1\n",
       "410         1\n",
       "411         1\n",
       "412         1\n",
       "413         0\n",
       "414         1\n",
       "415         0\n",
       "416         0\n",
       "417         0\n",
       "\n",
       "[418 rows x 1 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_survived_df = pd.DataFrame(data=final_prediction_int.T, columns=['Survived'])\n",
    "final_survived_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_cell_guid": "f11d65ad-3581-4822-8346-e7084641f146",
    "_uuid": "616342cde8d2b6ae60c2301c50b0cd5da3af06e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       892\n",
       "1       893\n",
       "2       894\n",
       "3       895\n",
       "4       896\n",
       "5       897\n",
       "6       898\n",
       "7       899\n",
       "8       900\n",
       "9       901\n",
       "10      902\n",
       "11      903\n",
       "12      904\n",
       "13      905\n",
       "14      906\n",
       "15      907\n",
       "16      908\n",
       "17      909\n",
       "18      910\n",
       "19      911\n",
       "20      912\n",
       "21      913\n",
       "22      914\n",
       "23      915\n",
       "24      916\n",
       "25      917\n",
       "26      918\n",
       "27      919\n",
       "28      920\n",
       "29      921\n",
       "       ... \n",
       "388    1280\n",
       "389    1281\n",
       "390    1282\n",
       "391    1283\n",
       "392    1284\n",
       "393    1285\n",
       "394    1286\n",
       "395    1287\n",
       "396    1288\n",
       "397    1289\n",
       "398    1290\n",
       "399    1291\n",
       "400    1292\n",
       "401    1293\n",
       "402    1294\n",
       "403    1295\n",
       "404    1296\n",
       "405    1297\n",
       "406    1298\n",
       "407    1299\n",
       "408    1300\n",
       "409    1301\n",
       "410    1302\n",
       "411    1303\n",
       "412    1304\n",
       "413    1305\n",
       "414    1306\n",
       "415    1307\n",
       "416    1308\n",
       "417    1309\n",
       "Name: PassengerId, Length: 418, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_cell_guid": "f5789fba-4ffd-4975-8408-229da26e5927",
    "_uuid": "736363528b2c36ce7355a5f6a9f0851ee75c4922"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>903</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>912</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>915</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1289</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1299</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1303</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1304</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         1\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         1\n",
       "5            897         0\n",
       "6            898         1\n",
       "7            899         0\n",
       "8            900         0\n",
       "9            901         0\n",
       "10           902         0\n",
       "11           903         1\n",
       "12           904         1\n",
       "13           905         0\n",
       "14           906         1\n",
       "15           907         1\n",
       "16           908         0\n",
       "17           909         0\n",
       "18           910         1\n",
       "19           911         1\n",
       "20           912         1\n",
       "21           913         0\n",
       "22           914         1\n",
       "23           915         1\n",
       "24           916         1\n",
       "25           917         0\n",
       "26           918         1\n",
       "27           919         0\n",
       "28           920         1\n",
       "29           921         0\n",
       "..           ...       ...\n",
       "388         1280         0\n",
       "389         1281         0\n",
       "390         1282         1\n",
       "391         1283         1\n",
       "392         1284         0\n",
       "393         1285         0\n",
       "394         1286         0\n",
       "395         1287         1\n",
       "396         1288         0\n",
       "397         1289         1\n",
       "398         1290         0\n",
       "399         1291         0\n",
       "400         1292         1\n",
       "401         1293         0\n",
       "402         1294         1\n",
       "403         1295         0\n",
       "404         1296         1\n",
       "405         1297         1\n",
       "406         1298         0\n",
       "407         1299         1\n",
       "408         1300         1\n",
       "409         1301         1\n",
       "410         1302         1\n",
       "411         1303         1\n",
       "412         1304         1\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.concat([test_df['PassengerId'], final_survived_df], axis=1)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_cell_guid": "6bf29a84-c5fe-4edc-a6fd-068024f3b3c5",
    "_uuid": "81012d55e9c5d8e2fa9822b8e0e7b4c591713714",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exporting to a csv file\n",
    "final_df.to_csv(\"output-prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e94697d0f0af65c63f68afe3c74bdecbc9d18ee"
   },
   "source": [
    "# One function to sum the whole notebook\n",
    "\n",
    "Now that we've reached here, we would want to execute the same notebook for different values of hyperparameters - to see how well our ouput csv file does on the leaderboard, and if we can improve our position. \n",
    "For this, I've tried to utilize the helper functions we kept writing above and made one method which does everything from loading data, to fixing null values, to evaluating the model and then predicting and outputing a csv file. \n",
    "Ultimately, you could just call this method with a range of hyperparameters, and let it do its magic. I'm gonna do the same on a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_cell_guid": "5275e6b5-4fe7-47dc-8973-3f29c01b412b",
    "_uuid": "52c73ea34b13f3d491fc63be9d7e7c7f63e32c49"
   },
   "outputs": [],
   "source": [
    "# helper exercise which does the whole thing for any training dataframe given \n",
    "def execute_steps_for_titanic(columns_to_use, output_file_name, learning_rate=0.01, num_of_epochs=3000, hidden_units=50, threshold_for_output=0.5, ):\n",
    "    # read data\n",
    "    training_df_orig = pd.read_csv(\"../input/train.csv\")\n",
    "    testing_df_orig = pd.read_csv(\"../input/test.csv\")\n",
    "    # get X and Y separated\n",
    "    train_df_Y = training_df_orig['Survived']\n",
    "    train_df_X = training_df_orig[columns_to_use]\n",
    "    test_df_X = testing_df_orig[columns_to_use]\n",
    "    # fix missing data\n",
    "    categorical_columns = find_categorical_columns(train_df_X)\n",
    "    replace_values_dict = {'Embarked':'S', 'Cabin':'UNKNOWN'}\n",
    "    for col in columns_to_use:\n",
    "        num_of_NaN_rows = get_num_of_NaN_rows(train_df_X)[col]\n",
    "        num_of_NaN_rows_test = get_num_of_NaN_rows(test_df_X)[col]\n",
    "        if(num_of_NaN_rows > 0):\n",
    "            print(\"Filling NaN values for column:\",col)\n",
    "            if col not in categorical_columns:\n",
    "                train_df_X[col] = train_df_X[col].fillna(train_df_X[col].mean())\n",
    "            else:\n",
    "                train_df_X[col] = train_df_X[col].fillna(replace_values_dict[col])\n",
    "        if(num_of_NaN_rows_test > 0):\n",
    "            print(\"Filling NaN values for column:\",col,\" in test data\")\n",
    "            if col not in categorical_columns:\n",
    "                test_df_X[col] = test_df_X[col].fillna(test_df_X[col].mean())\n",
    "            else:\n",
    "                test_df_X[col] = test_df_X[col].fillna(replace_values_dict[col])\n",
    "    print(\"Fixed NaN values in training and testing data.\")\n",
    "    # convert categorical to numerical data\n",
    "    train_df_X_num = convert_categorical_column_to_integer_values(train_df_X)\n",
    "    test_df_X_num = convert_categorical_column_to_integer_values(test_df_X)\n",
    "    # Get numpy arrays for this data\n",
    "    train_X = train_df_X_num.as_matrix()\n",
    "    test_X = test_df_X_num.as_matrix()\n",
    "    train_Y = train_df_Y.as_matrix()\n",
    "    # fix rank-1 array created\n",
    "    train_Y = train_Y[:,np.newaxis]\n",
    "    # call model and get values \n",
    "    W1,b1,W2,b2,A2,Y,final_tr_accuracy = model_generic(learning_rate, train_X.T, train_Y.T, num_of_epochs, hidden_units, threshold_for_output)\n",
    "    print(\"Final training accuracy : \",final_tr_accuracy)\n",
    "    # get prediction and save it to output file\n",
    "    prediction = predict(W1,b1,W2,b2,test_X.T)\n",
    "    # if prediction value > threshold, then set as True, else as False\n",
    "    prediction = prediction > threshold_for_output\n",
    "    # Convert the True/False array to a 0 , 1 array\n",
    "    prediction = prediction.astype(int)\n",
    "    # Convert back to dataframe and give the column name as 'Survived'\n",
    "    prediction_df = pd.DataFrame(data=prediction.T, columns=['Survived'])\n",
    "    # Make a final data frame of the required output and output to csv\n",
    "    final_df = pd.concat([testing_df_orig['PassengerId'], prediction_df], axis=1)\n",
    "    final_file_name = output_file_name+\"_tr_acc_\"+\"{0:.2f}\".format(final_tr_accuracy)+\"_prediction.csv\"\n",
    "    final_df.to_csv(final_file_name, index=False)\n",
    "    print(\"Done.\")\n",
    "    return final_file_name, final_tr_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_uuid": "73e67be0d06b602b945e7caba8ec3f7ddad22651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhavul.g/.virtualenvs/ailearn/lib/python3.5/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/bhavul.g/.virtualenvs/ailearn/lib/python3.5/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhavul.g/.virtualenvs/ailearn/lib/python3.5/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/bhavul.g/.virtualenvs/ailearn/lib/python3.5/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  622.927 , training accuracy :  0.615039\n",
      "i: 100 , cost :  32.1764 , training accuracy :  0.638608\n",
      "i: 200 , cost :  5.37419 , training accuracy :  0.676768\n",
      "i: 300 , cost :  3.16623 , training accuracy :  0.721661\n",
      "i: 400 , cost :  2.37026 , training accuracy :  0.732884\n",
      "i: 500 , cost :  2.64006 , training accuracy :  0.705948\n",
      "i: 600 , cost :  2.30937 , training accuracy :  0.707071\n",
      "i: 700 , cost :  1.52118 , training accuracy :  0.780022\n",
      "i: 800 , cost :  1.27412 , training accuracy :  0.782267\n",
      "i: 900 , cost :  1.78044 , training accuracy :  0.739618\n",
      "i: 1000 , cost :  1.10221 , training accuracy :  0.79798\n",
      "i: 1100 , cost :  1.08912 , training accuracy :  0.796857\n",
      "i: 1200 , cost :  1.99274 , training accuracy :  0.719416\n",
      "i: 1300 , cost :  1.3371 , training accuracy :  0.762065\n",
      "i: 1400 , cost :  1.36572 , training accuracy :  0.749719\n",
      "i: 1500 , cost :  0.930596 , training accuracy :  0.799102\n",
      "i: 1600 , cost :  1.5046 , training accuracy :  0.645342\n",
      "i: 1700 , cost :  0.898017 , training accuracy :  0.800224\n",
      "i: 1800 , cost :  1.29985 , training accuracy :  0.758698\n",
      "i: 1900 , cost :  0.798484 , training accuracy :  0.803591\n",
      "i: 2000 , cost :  0.972553 , training accuracy :  0.708193\n",
      "i: 2100 , cost :  0.777418 , training accuracy :  0.789001\n",
      "i: 2200 , cost :  0.80996 , training accuracy :  0.809203\n",
      "i: 2300 , cost :  0.736565 , training accuracy :  0.805836\n",
      "i: 2400 , cost :  0.791441 , training accuracy :  0.786756\n",
      "i: 2500 , cost :  1.14528 , training accuracy :  0.737374\n",
      "i: 2600 , cost :  1.62474 , training accuracy :  0.748597\n",
      "i: 2700 , cost :  0.867392 , training accuracy :  0.773288\n",
      "i: 2800 , cost :  0.700251 , training accuracy :  0.806958\n",
      "i: 2900 , cost :  0.809972 , training accuracy :  0.729517\n",
      "i: 3000 , cost :  1.82195 , training accuracy :  0.732884\n",
      "i: 3100 , cost :  1.33814 , training accuracy :  0.636364\n",
      "i: 3200 , cost :  0.772434 , training accuracy :  0.799102\n",
      "i: 3300 , cost :  0.643406 , training accuracy :  0.810326\n",
      "i: 3400 , cost :  0.913987 , training accuracy :  0.731762\n",
      "i: 3500 , cost :  0.783569 , training accuracy :  0.781145\n",
      "i: 3600 , cost :  0.743473 , training accuracy :  0.804714\n",
      "i: 3700 , cost :  0.693213 , training accuracy :  0.799102\n",
      "i: 3800 , cost :  0.664913 , training accuracy :  0.800224\n",
      "i: 3900 , cost :  1.39304 , training accuracy :  0.76431\n",
      "i: 4000 , cost :  0.681111 , training accuracy :  0.822671\n",
      "i: 4100 , cost :  0.729168 , training accuracy :  0.811448\n",
      "i: 4200 , cost :  0.569124 , training accuracy :  0.817059\n",
      "i: 4300 , cost :  0.609912 , training accuracy :  0.819304\n",
      "i: 4400 , cost :  3.17547 , training accuracy :  0.430976\n",
      "i: 4500 , cost :  0.627744 , training accuracy :  0.814815\n",
      "i: 4600 , cost :  0.756601 , training accuracy :  0.804714\n",
      "i: 4700 , cost :  0.544772 , training accuracy :  0.819304\n",
      "i: 4800 , cost :  0.572795 , training accuracy :  0.815937\n",
      "i: 4900 , cost :  2.78949 , training accuracy :  0.710438\n",
      "Final training accuracy :  0.772166\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Let's try this once?\n",
    "# All this while, we kept including Name and PassengerId as 2 important columns however in real life, they actually don't really matter in deciding whether a person would live or not. \n",
    "# So, now let's check without them.\n",
    "columns_to_use = ['Pclass','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked']\n",
    "execute_steps_for_titanic(columns_to_use, \"bhavul\", learning_rate=0.005, num_of_epochs=5000, hidden_units=30, threshold_for_output=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_uuid": "2adf33a71ed00cee8e8721925579bb589e803da3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperdash import monitor_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  118.938 , training accuracy :  0.616162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhavul.g/.virtualenvs/ailearn/lib/python3.5/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/bhavul.g/.virtualenvs/ailearn/lib/python3.5/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/bhavul.g/.virtualenvs/ailearn/lib/python3.5/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/bhavul.g/.virtualenvs/ailearn/lib/python3.5/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 100 , cost :  86.9805 , training accuracy :  0.615039\n",
      "i: 200 , cost :  59.4641 , training accuracy :  0.631874\n",
      "i: 300 , cost :  38.533 , training accuracy :  0.662177\n",
      "i: 400 , cost :  21.2574 , training accuracy :  0.659933\n",
      "i: 500 , cost :  13.3527 , training accuracy :  0.620651\n",
      "i: 600 , cost :  9.92742 , training accuracy :  0.620651\n",
      "i: 700 , cost :  7.47914 , training accuracy :  0.620651\n",
      "i: 800 , cost :  5.63277 , training accuracy :  0.619529\n",
      "i: 900 , cost :  4.2046 , training accuracy :  0.615039\n",
      "Final training accuracy :  0.612795\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 1000 ][hidden: 3 ][file: bhavul_tr_acc_0.61_prediction.csv ] ACCURACY :  0.612795\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  324.82 , training accuracy :  0.616162\n",
      "i: 100 , cost :  164.685 , training accuracy :  0.622896\n",
      "i: 200 , cost :  98.445 , training accuracy :  0.551066\n",
      "i: 300 , cost :  73.1216 , training accuracy :  0.506173\n",
      "i: 400 , cost :  48.7633 , training accuracy :  0.519641\n",
      "i: 500 , cost :  28.4704 , training accuracy :  0.553311\n",
      "i: 600 , cost :  17.7577 , training accuracy :  0.593715\n",
      "i: 700 , cost :  12.9252 , training accuracy :  0.62963\n",
      "i: 800 , cost :  9.23197 , training accuracy :  0.643098\n",
      "i: 900 , cost :  6.71271 , training accuracy :  0.654321\n",
      "Final training accuracy :  0.673401\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 1000 ][hidden: 10 ][file: bhavul_tr_acc_0.67_prediction.csv ] ACCURACY :  0.673401\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1070.77 , training accuracy :  0.383838\n",
      "i: 100 , cost :  447.242 , training accuracy :  0.372615\n",
      "i: 200 , cost :  97.8315 , training accuracy :  0.50505\n",
      "i: 300 , cost :  59.8845 , training accuracy :  0.612795\n",
      "i: 400 , cost :  39.5679 , training accuracy :  0.664422\n",
      "i: 500 , cost :  27.1224 , training accuracy :  0.671156\n",
      "i: 600 , cost :  21.4983 , training accuracy :  0.67789\n",
      "i: 700 , cost :  17.3304 , training accuracy :  0.687991\n",
      "i: 800 , cost :  12.042 , training accuracy :  0.680135\n",
      "i: 900 , cost :  7.76123 , training accuracy :  0.682379\n",
      "Final training accuracy :  0.703704\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 1000 ][hidden: 15 ][file: bhavul_tr_acc_0.70_prediction.csv ] ACCURACY :  0.703704\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  247.083 , training accuracy :  0.315376\n",
      "i: 100 , cost :  54.2761 , training accuracy :  0.424242\n",
      "i: 200 , cost :  9.09696 , training accuracy :  0.590348\n",
      "i: 300 , cost :  4.78458 , training accuracy :  0.721661\n",
      "i: 400 , cost :  3.6361 , training accuracy :  0.729517\n",
      "i: 500 , cost :  3.09376 , training accuracy :  0.737374\n",
      "i: 600 , cost :  2.65725 , training accuracy :  0.751964\n",
      "i: 700 , cost :  2.26961 , training accuracy :  0.758698\n",
      "i: 800 , cost :  2.02269 , training accuracy :  0.753086\n",
      "i: 900 , cost :  1.7908 , training accuracy :  0.750842\n",
      "Final training accuracy :  0.751964\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 1000 ][hidden: 50 ][file: bhavul_tr_acc_0.75_prediction.csv ] ACCURACY :  0.751964\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1025.55 , training accuracy :  0.607183\n",
      "i: 100 , cost :  288.765 , training accuracy :  0.571268\n",
      "i: 200 , cost :  56.9522 , training accuracy :  0.554433\n",
      "i: 300 , cost :  18.2768 , training accuracy :  0.59596\n",
      "i: 400 , cost :  9.6422 , training accuracy :  0.670034\n",
      "i: 500 , cost :  6.43311 , training accuracy :  0.704826\n",
      "i: 600 , cost :  4.9551 , training accuracy :  0.721661\n",
      "i: 700 , cost :  4.23077 , training accuracy :  0.744108\n",
      "i: 800 , cost :  3.68972 , training accuracy :  0.75982\n",
      "i: 900 , cost :  2.86471 , training accuracy :  0.768799\n",
      "Final training accuracy :  0.792368\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 1000 ][hidden: 100 ][file: bhavul_tr_acc_0.79_prediction.csv ] ACCURACY :  0.792368\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  45.3931 , training accuracy :  0.647587\n",
      "i: 100 , cost :  24.9534 , training accuracy :  0.653199\n",
      "i: 200 , cost :  13.8191 , training accuracy :  0.64422\n",
      "i: 300 , cost :  9.40233 , training accuracy :  0.630752\n",
      "i: 400 , cost :  6.84087 , training accuracy :  0.620651\n",
      "i: 500 , cost :  4.94328 , training accuracy :  0.618406\n",
      "i: 600 , cost :  3.64946 , training accuracy :  0.617284\n",
      "i: 700 , cost :  2.73665 , training accuracy :  0.618406\n",
      "i: 800 , cost :  2.19626 , training accuracy :  0.619529\n",
      "i: 900 , cost :  1.83215 , training accuracy :  0.616162\n",
      "i: 1000 , cost :  1.56616 , training accuracy :  0.613917\n",
      "i: 1100 , cost :  1.41901 , training accuracy :  0.612795\n",
      "i: 1200 , cost :  1.29151 , training accuracy :  0.613917\n",
      "i: 1300 , cost :  1.16959 , training accuracy :  0.612795\n",
      "i: 1400 , cost :  1.07048 , training accuracy :  0.612795\n",
      "i: 1500 , cost :  1.0006 , training accuracy :  0.61055\n",
      "i: 1600 , cost :  0.941815 , training accuracy :  0.609428\n",
      "i: 1700 , cost :  0.893007 , training accuracy :  0.608305\n",
      "i: 1800 , cost :  0.845259 , training accuracy :  0.608305\n",
      "i: 1900 , cost :  0.804483 , training accuracy :  0.609428\n",
      "i: 2000 , cost :  0.768449 , training accuracy :  0.61055\n",
      "i: 2100 , cost :  0.738135 , training accuracy :  0.611672\n",
      "i: 2200 , cost :  0.713246 , training accuracy :  0.611672\n",
      "i: 2300 , cost :  0.687854 , training accuracy :  0.613917\n",
      "i: 2400 , cost :  0.674079 , training accuracy :  0.615039\n",
      "i: 2500 , cost :  0.6684 , training accuracy :  0.613917\n",
      "i: 2600 , cost :  0.665531 , training accuracy :  0.613917\n",
      "i: 2700 , cost :  0.664313 , training accuracy :  0.615039\n",
      "i: 2800 , cost :  0.663978 , training accuracy :  0.615039\n",
      "i: 2900 , cost :  0.663718 , training accuracy :  0.615039\n",
      "i: 3000 , cost :  0.663489 , training accuracy :  0.615039\n",
      "i: 3100 , cost :  0.663279 , training accuracy :  0.615039\n",
      "i: 3200 , cost :  0.663087 , training accuracy :  0.615039\n",
      "i: 3300 , cost :  0.662909 , training accuracy :  0.615039\n",
      "i: 3400 , cost :  0.662742 , training accuracy :  0.615039\n",
      "i: 3500 , cost :  0.662587 , training accuracy :  0.615039\n",
      "i: 3600 , cost :  0.662441 , training accuracy :  0.615039\n",
      "i: 3700 , cost :  0.662305 , training accuracy :  0.615039\n",
      "i: 3800 , cost :  0.662176 , training accuracy :  0.615039\n",
      "i: 3900 , cost :  0.662055 , training accuracy :  0.615039\n",
      "i: 4000 , cost :  0.661943 , training accuracy :  0.615039\n",
      "i: 4100 , cost :  0.661839 , training accuracy :  0.616162\n",
      "i: 4200 , cost :  0.661742 , training accuracy :  0.616162\n",
      "i: 4300 , cost :  0.661648 , training accuracy :  0.615039\n",
      "i: 4400 , cost :  0.661561 , training accuracy :  0.615039\n",
      "i: 4500 , cost :  0.661477 , training accuracy :  0.615039\n",
      "i: 4600 , cost :  0.661396 , training accuracy :  0.615039\n",
      "i: 4700 , cost :  0.661317 , training accuracy :  0.615039\n",
      "i: 4800 , cost :  0.66124 , training accuracy :  0.615039\n",
      "i: 4900 , cost :  0.661171 , training accuracy :  0.615039\n",
      "Final training accuracy :  0.615039\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 5000 ][hidden: 3 ][file: bhavul_tr_acc_0.62_prediction.csv ] ACCURACY :  0.615039\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  116.123 , training accuracy :  0.613917\n",
      "i: 100 , cost :  47.9522 , training accuracy :  0.479237\n",
      "i: 200 , cost :  22.9158 , training accuracy :  0.52413\n",
      "i: 300 , cost :  9.66018 , training accuracy :  0.56229\n",
      "i: 400 , cost :  4.59566 , training accuracy :  0.640853\n",
      "i: 500 , cost :  2.67596 , training accuracy :  0.64422\n",
      "i: 600 , cost :  1.82522 , training accuracy :  0.661055\n",
      "i: 700 , cost :  1.45185 , training accuracy :  0.694725\n",
      "i: 800 , cost :  1.20021 , training accuracy :  0.710438\n",
      "i: 900 , cost :  1.04581 , training accuracy :  0.73064\n",
      "i: 1000 , cost :  0.918847 , training accuracy :  0.732884\n",
      "i: 1100 , cost :  0.837404 , training accuracy :  0.748597\n",
      "i: 1200 , cost :  0.800638 , training accuracy :  0.757576\n",
      "i: 1300 , cost :  0.776071 , training accuracy :  0.76431\n",
      "i: 1400 , cost :  0.757779 , training accuracy :  0.76431\n",
      "i: 1500 , cost :  0.742791 , training accuracy :  0.768799\n",
      "i: 1600 , cost :  0.729637 , training accuracy :  0.771044\n",
      "i: 1700 , cost :  0.71767 , training accuracy :  0.774411\n",
      "i: 1800 , cost :  0.706352 , training accuracy :  0.774411\n",
      "i: 1900 , cost :  0.695543 , training accuracy :  0.774411\n",
      "i: 2000 , cost :  0.683612 , training accuracy :  0.777778\n",
      "i: 2100 , cost :  0.672045 , training accuracy :  0.777778\n",
      "i: 2200 , cost :  0.663342 , training accuracy :  0.7789\n",
      "i: 2300 , cost :  0.650108 , training accuracy :  0.781145\n",
      "i: 2400 , cost :  0.640902 , training accuracy :  0.785634\n",
      "i: 2500 , cost :  0.632192 , training accuracy :  0.786756\n",
      "i: 2600 , cost :  0.624123 , training accuracy :  0.784512\n",
      "i: 2700 , cost :  0.616977 , training accuracy :  0.789001\n",
      "i: 2800 , cost :  0.607958 , training accuracy :  0.783389\n",
      "i: 2900 , cost :  0.600399 , training accuracy :  0.786756\n",
      "i: 3000 , cost :  0.593485 , training accuracy :  0.784512\n",
      "i: 3100 , cost :  0.59578 , training accuracy :  0.791246\n",
      "i: 3200 , cost :  0.576079 , training accuracy :  0.789001\n",
      "i: 3300 , cost :  0.567902 , training accuracy :  0.790123\n",
      "i: 3400 , cost :  0.560818 , training accuracy :  0.790123\n",
      "i: 3500 , cost :  0.553434 , training accuracy :  0.792368\n",
      "i: 3600 , cost :  0.546652 , training accuracy :  0.792368\n",
      "i: 3700 , cost :  0.540608 , training accuracy :  0.789001\n",
      "i: 3800 , cost :  0.532899 , training accuracy :  0.794613\n",
      "i: 3900 , cost :  0.527118 , training accuracy :  0.786756\n",
      "i: 4000 , cost :  0.519667 , training accuracy :  0.789001\n",
      "i: 4100 , cost :  0.512778 , training accuracy :  0.790123\n",
      "i: 4200 , cost :  0.510644 , training accuracy :  0.789001\n",
      "i: 4300 , cost :  0.503504 , training accuracy :  0.790123\n",
      "i: 4400 , cost :  0.499736 , training accuracy :  0.795735\n",
      "i: 4500 , cost :  0.505229 , training accuracy :  0.785634\n",
      "i: 4600 , cost :  0.482943 , training accuracy :  0.795735\n",
      "i: 4700 , cost :  0.47506 , training accuracy :  0.800224\n",
      "i: 4800 , cost :  0.471477 , training accuracy :  0.800224\n",
      "i: 4900 , cost :  0.46958 , training accuracy :  0.805836\n",
      "Final training accuracy :  0.810326\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 5000 ][hidden: 10 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.810326\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  169.133 , training accuracy :  0.35578\n",
      "i: 100 , cost :  53.4933 , training accuracy :  0.564534\n",
      "i: 200 , cost :  21.3494 , training accuracy :  0.600449\n",
      "i: 300 , cost :  9.25123 , training accuracy :  0.649832\n",
      "i: 400 , cost :  5.92874 , training accuracy :  0.662177\n",
      "i: 500 , cost :  3.51129 , training accuracy :  0.690236\n",
      "i: 600 , cost :  2.86186 , training accuracy :  0.685746\n",
      "i: 700 , cost :  2.39023 , training accuracy :  0.705948\n",
      "i: 800 , cost :  2.01805 , training accuracy :  0.717172\n",
      "i: 900 , cost :  1.73132 , training accuracy :  0.720539\n",
      "i: 1000 , cost :  1.45832 , training accuracy :  0.722783\n",
      "i: 1100 , cost :  1.26395 , training accuracy :  0.73064\n",
      "i: 1200 , cost :  1.11752 , training accuracy :  0.736251\n",
      "i: 1300 , cost :  0.994897 , training accuracy :  0.746352\n",
      "i: 1400 , cost :  0.911838 , training accuracy :  0.747475\n",
      "i: 1500 , cost :  0.843111 , training accuracy :  0.755331\n",
      "i: 1600 , cost :  0.785027 , training accuracy :  0.762065\n",
      "i: 1700 , cost :  0.731939 , training accuracy :  0.768799\n",
      "i: 1800 , cost :  0.685825 , training accuracy :  0.771044\n",
      "i: 1900 , cost :  0.647157 , training accuracy :  0.780022\n",
      "i: 2000 , cost :  0.623066 , training accuracy :  0.787879\n",
      "i: 2100 , cost :  0.606152 , training accuracy :  0.79349\n",
      "i: 2200 , cost :  0.590839 , training accuracy :  0.794613\n",
      "i: 2300 , cost :  0.576715 , training accuracy :  0.79349\n",
      "i: 2400 , cost :  0.563368 , training accuracy :  0.801347\n",
      "i: 2500 , cost :  0.55283 , training accuracy :  0.801347\n",
      "i: 2600 , cost :  0.543158 , training accuracy :  0.805836\n",
      "i: 2700 , cost :  0.533571 , training accuracy :  0.804714\n",
      "i: 2800 , cost :  0.524533 , training accuracy :  0.799102\n",
      "i: 2900 , cost :  0.513592 , training accuracy :  0.809203\n",
      "i: 3000 , cost :  0.503469 , training accuracy :  0.813693\n",
      "i: 3100 , cost :  0.495008 , training accuracy :  0.813693\n",
      "i: 3200 , cost :  0.488412 , training accuracy :  0.805836\n",
      "i: 3300 , cost :  0.483262 , training accuracy :  0.800224\n",
      "i: 3400 , cost :  0.479625 , training accuracy :  0.79798\n",
      "i: 3500 , cost :  0.472418 , training accuracy :  0.818182\n",
      "i: 3600 , cost :  0.46883 , training accuracy :  0.79798\n",
      "i: 3700 , cost :  0.465577 , training accuracy :  0.796857\n",
      "i: 3800 , cost :  0.460255 , training accuracy :  0.819304\n",
      "i: 3900 , cost :  0.463285 , training accuracy :  0.814815\n",
      "i: 4000 , cost :  0.452053 , training accuracy :  0.811448\n",
      "i: 4100 , cost :  0.448813 , training accuracy :  0.806958\n",
      "i: 4200 , cost :  0.445808 , training accuracy :  0.804714\n",
      "i: 4300 , cost :  0.44412 , training accuracy :  0.815937\n",
      "i: 4400 , cost :  0.443712 , training accuracy :  0.823793\n",
      "i: 4500 , cost :  0.441855 , training accuracy :  0.803591\n",
      "i: 4600 , cost :  0.438962 , training accuracy :  0.802469\n",
      "i: 4700 , cost :  0.43881 , training accuracy :  0.806958\n",
      "i: 4800 , cost :  0.435547 , training accuracy :  0.806958\n",
      "i: 4900 , cost :  0.438629 , training accuracy :  0.804714\n",
      "Final training accuracy :  0.81257\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 5000 ][hidden: 15 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.81257\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1624.87 , training accuracy :  0.383838\n",
      "i: 100 , cost :  285.61 , training accuracy :  0.430976\n",
      "i: 200 , cost :  89.15 , training accuracy :  0.647587\n",
      "i: 300 , cost :  60.7188 , training accuracy :  0.664422\n",
      "i: 400 , cost :  39.3516 , training accuracy :  0.675645\n",
      "i: 500 , cost :  24.3346 , training accuracy :  0.666667\n",
      "i: 600 , cost :  17.3398 , training accuracy :  0.674523\n",
      "i: 700 , cost :  13.5314 , training accuracy :  0.673401\n",
      "i: 800 , cost :  11.6826 , training accuracy :  0.69248\n",
      "i: 900 , cost :  10.4422 , training accuracy :  0.698092\n",
      "i: 1000 , cost :  9.34855 , training accuracy :  0.708193\n",
      "i: 1100 , cost :  8.25518 , training accuracy :  0.716049\n",
      "i: 1200 , cost :  7.27703 , training accuracy :  0.727273\n",
      "i: 1300 , cost :  6.37195 , training accuracy :  0.734007\n",
      "i: 1400 , cost :  5.45037 , training accuracy :  0.741863\n",
      "i: 1500 , cost :  4.6737 , training accuracy :  0.740741\n",
      "i: 1600 , cost :  4.10421 , training accuracy :  0.746352\n",
      "i: 1700 , cost :  3.7148 , training accuracy :  0.749719\n",
      "i: 1800 , cost :  3.39907 , training accuracy :  0.749719\n",
      "i: 1900 , cost :  3.13294 , training accuracy :  0.746352\n",
      "i: 2000 , cost :  2.90188 , training accuracy :  0.748597\n",
      "i: 2100 , cost :  2.70375 , training accuracy :  0.755331\n",
      "i: 2200 , cost :  2.48742 , training accuracy :  0.758698\n",
      "i: 2300 , cost :  2.30132 , training accuracy :  0.768799\n",
      "i: 2400 , cost :  2.13485 , training accuracy :  0.769921\n",
      "i: 2500 , cost :  2.04596 , training accuracy :  0.765432\n",
      "i: 2600 , cost :  1.8938 , training accuracy :  0.772166\n",
      "i: 2700 , cost :  1.8667 , training accuracy :  0.768799\n",
      "i: 2800 , cost :  1.73911 , training accuracy :  0.783389\n",
      "i: 2900 , cost :  1.67318 , training accuracy :  0.786756\n",
      "i: 3000 , cost :  1.62879 , training accuracy :  0.785634\n",
      "i: 3100 , cost :  1.60177 , training accuracy :  0.775533\n",
      "i: 3200 , cost :  1.52832 , training accuracy :  0.782267\n",
      "i: 3300 , cost :  1.46086 , training accuracy :  0.791246\n",
      "i: 3400 , cost :  1.39433 , training accuracy :  0.781145\n",
      "i: 3500 , cost :  1.3375 , training accuracy :  0.775533\n",
      "i: 3600 , cost :  1.27026 , training accuracy :  0.789001\n",
      "i: 3700 , cost :  1.24555 , training accuracy :  0.787879\n",
      "i: 3800 , cost :  1.24595 , training accuracy :  0.774411\n",
      "i: 3900 , cost :  1.23024 , training accuracy :  0.773288\n",
      "i: 4000 , cost :  1.1726 , training accuracy :  0.784512\n",
      "i: 4100 , cost :  1.10711 , training accuracy :  0.791246\n",
      "i: 4200 , cost :  1.05175 , training accuracy :  0.799102\n",
      "i: 4300 , cost :  1.06106 , training accuracy :  0.783389\n",
      "i: 4400 , cost :  1.0615 , training accuracy :  0.773288\n",
      "i: 4500 , cost :  1.0349 , training accuracy :  0.776655\n",
      "i: 4600 , cost :  0.973614 , training accuracy :  0.792368\n",
      "i: 4700 , cost :  0.940988 , training accuracy :  0.795735\n",
      "i: 4800 , cost :  0.929982 , training accuracy :  0.794613\n",
      "i: 4900 , cost :  0.872686 , training accuracy :  0.805836\n",
      "Final training accuracy :  0.790123\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 5000 ][hidden: 50 ][file: bhavul_tr_acc_0.79_prediction.csv ] ACCURACY :  0.790123\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  3140.38 , training accuracy :  0.383838\n",
      "i: 100 , cost :  714.471 , training accuracy :  0.380471\n",
      "i: 200 , cost :  110.728 , training accuracy :  0.645342\n",
      "i: 300 , cost :  47.6055 , training accuracy :  0.671156\n",
      "i: 400 , cost :  27.0077 , training accuracy :  0.674523\n",
      "i: 500 , cost :  17.8606 , training accuracy :  0.665544\n",
      "i: 600 , cost :  12.242 , training accuracy :  0.656566\n",
      "i: 700 , cost :  8.21846 , training accuracy :  0.683502\n",
      "i: 800 , cost :  6.60867 , training accuracy :  0.689113\n",
      "i: 900 , cost :  5.35658 , training accuracy :  0.690236\n",
      "i: 1000 , cost :  4.47554 , training accuracy :  0.721661\n",
      "i: 1100 , cost :  3.81026 , training accuracy :  0.740741\n",
      "i: 1200 , cost :  3.25396 , training accuracy :  0.75982\n",
      "i: 1300 , cost :  2.78885 , training accuracy :  0.769921\n",
      "i: 1400 , cost :  2.42726 , training accuracy :  0.775533\n",
      "i: 1500 , cost :  2.11832 , training accuracy :  0.780022\n",
      "i: 1600 , cost :  1.89174 , training accuracy :  0.785634\n",
      "i: 1700 , cost :  1.72069 , training accuracy :  0.791246\n",
      "i: 1800 , cost :  1.57531 , training accuracy :  0.796857\n",
      "i: 1900 , cost :  1.458 , training accuracy :  0.799102\n",
      "i: 2000 , cost :  1.37928 , training accuracy :  0.794613\n",
      "i: 2100 , cost :  1.30882 , training accuracy :  0.794613\n",
      "i: 2200 , cost :  1.24743 , training accuracy :  0.79798\n",
      "i: 2300 , cost :  1.20017 , training accuracy :  0.79798\n",
      "i: 2400 , cost :  1.14308 , training accuracy :  0.799102\n",
      "i: 2500 , cost :  1.09514 , training accuracy :  0.799102\n",
      "i: 2600 , cost :  1.19828 , training accuracy :  0.790123\n",
      "i: 2700 , cost :  1.01498 , training accuracy :  0.804714\n",
      "i: 2800 , cost :  0.983026 , training accuracy :  0.810326\n",
      "i: 2900 , cost :  0.970768 , training accuracy :  0.820426\n",
      "i: 3000 , cost :  0.920354 , training accuracy :  0.806958\n",
      "i: 3100 , cost :  0.908308 , training accuracy :  0.809203\n",
      "i: 3200 , cost :  2.22119 , training accuracy :  0.747475\n",
      "i: 3300 , cost :  0.87162 , training accuracy :  0.824916\n",
      "i: 3400 , cost :  0.835157 , training accuracy :  0.828283\n",
      "i: 3500 , cost :  0.814054 , training accuracy :  0.823793\n",
      "i: 3600 , cost :  0.796445 , training accuracy :  0.823793\n",
      "i: 3700 , cost :  0.782906 , training accuracy :  0.826038\n",
      "i: 3800 , cost :  0.801682 , training accuracy :  0.809203\n",
      "i: 3900 , cost :  0.8322 , training accuracy :  0.802469\n",
      "i: 4000 , cost :  0.739486 , training accuracy :  0.828283\n",
      "i: 4100 , cost :  0.884931 , training accuracy :  0.796857\n",
      "i: 4200 , cost :  0.786606 , training accuracy :  0.811448\n",
      "i: 4300 , cost :  0.786282 , training accuracy :  0.806958\n",
      "i: 4400 , cost :  0.681135 , training accuracy :  0.826038\n",
      "i: 4500 , cost :  0.787178 , training accuracy :  0.81257\n",
      "i: 4600 , cost :  0.667214 , training accuracy :  0.83165\n",
      "i: 4700 , cost :  0.690937 , training accuracy :  0.821549\n",
      "i: 4800 , cost :  0.630701 , training accuracy :  0.832772\n",
      "i: 4900 , cost :  0.685574 , training accuracy :  0.82716\n",
      "Final training accuracy :  0.832772\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 5000 ][hidden: 100 ][file: bhavul_tr_acc_0.83_prediction.csv ] ACCURACY :  0.832772\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  932.424 , training accuracy :  0.383838\n",
      "i: 100 , cost :  721.828 , training accuracy :  0.383838\n",
      "i: 200 , cost :  518.521 , training accuracy :  0.383838\n",
      "i: 300 , cost :  319.621 , training accuracy :  0.384961\n",
      "i: 400 , cost :  122.741 , training accuracy :  0.391695\n",
      "i: 500 , cost :  10.9808 , training accuracy :  0.616162\n",
      "i: 600 , cost :  7.20016 , training accuracy :  0.617284\n",
      "i: 700 , cost :  4.59505 , training accuracy :  0.641975\n",
      "i: 800 , cost :  2.60541 , training accuracy :  0.661055\n",
      "i: 900 , cost :  1.43211 , training accuracy :  0.705948\n",
      "i: 1000 , cost :  1.21307 , training accuracy :  0.693603\n",
      "i: 1100 , cost :  1.17493 , training accuracy :  0.698092\n",
      "i: 1200 , cost :  1.14276 , training accuracy :  0.707071\n",
      "i: 1300 , cost :  1.11139 , training accuracy :  0.710438\n",
      "i: 1400 , cost :  1.08108 , training accuracy :  0.716049\n",
      "i: 1500 , cost :  1.05169 , training accuracy :  0.728395\n",
      "i: 1600 , cost :  1.02422 , training accuracy :  0.735129\n",
      "i: 1700 , cost :  0.998595 , training accuracy :  0.741863\n",
      "i: 1800 , cost :  0.974098 , training accuracy :  0.742985\n",
      "i: 1900 , cost :  0.951064 , training accuracy :  0.747475\n",
      "i: 2000 , cost :  0.929331 , training accuracy :  0.744108\n",
      "i: 2100 , cost :  0.909131 , training accuracy :  0.744108\n",
      "i: 2200 , cost :  0.890115 , training accuracy :  0.744108\n",
      "i: 2300 , cost :  0.871899 , training accuracy :  0.742985\n",
      "i: 2400 , cost :  0.854363 , training accuracy :  0.740741\n",
      "i: 2500 , cost :  0.837423 , training accuracy :  0.742985\n",
      "i: 2600 , cost :  0.820787 , training accuracy :  0.744108\n",
      "i: 2700 , cost :  0.803995 , training accuracy :  0.748597\n",
      "i: 2800 , cost :  0.787552 , training accuracy :  0.750842\n",
      "i: 2900 , cost :  0.77143 , training accuracy :  0.753086\n",
      "i: 3000 , cost :  0.755629 , training accuracy :  0.754209\n",
      "i: 3100 , cost :  0.740155 , training accuracy :  0.754209\n",
      "i: 3200 , cost :  0.725032 , training accuracy :  0.758698\n",
      "i: 3300 , cost :  0.710298 , training accuracy :  0.757576\n",
      "i: 3400 , cost :  0.696374 , training accuracy :  0.766554\n",
      "i: 3500 , cost :  0.683627 , training accuracy :  0.767677\n",
      "i: 3600 , cost :  0.670697 , training accuracy :  0.762065\n",
      "i: 3700 , cost :  0.657888 , training accuracy :  0.762065\n",
      "i: 3800 , cost :  0.645436 , training accuracy :  0.766554\n",
      "i: 3900 , cost :  0.633354 , training accuracy :  0.768799\n",
      "i: 4000 , cost :  0.621633 , training accuracy :  0.777778\n",
      "i: 4100 , cost :  0.61032 , training accuracy :  0.781145\n",
      "i: 4200 , cost :  0.599476 , training accuracy :  0.782267\n",
      "i: 4300 , cost :  0.589242 , training accuracy :  0.782267\n",
      "i: 4400 , cost :  0.579683 , training accuracy :  0.784512\n",
      "i: 4500 , cost :  0.570839 , training accuracy :  0.783389\n",
      "i: 4600 , cost :  0.562415 , training accuracy :  0.787879\n",
      "i: 4700 , cost :  0.554672 , training accuracy :  0.789001\n",
      "i: 4800 , cost :  0.546627 , training accuracy :  0.790123\n",
      "i: 4900 , cost :  0.539047 , training accuracy :  0.791246\n",
      "i: 5000 , cost :  0.531614 , training accuracy :  0.79349\n",
      "i: 5100 , cost :  0.524374 , training accuracy :  0.795735\n",
      "i: 5200 , cost :  0.517492 , training accuracy :  0.795735\n",
      "i: 5300 , cost :  0.511159 , training accuracy :  0.794613\n",
      "i: 5400 , cost :  0.504174 , training accuracy :  0.795735\n",
      "i: 5500 , cost :  0.497945 , training accuracy :  0.795735\n",
      "i: 5600 , cost :  0.493424 , training accuracy :  0.794613\n",
      "i: 5700 , cost :  0.489021 , training accuracy :  0.794613\n",
      "i: 5800 , cost :  0.484271 , training accuracy :  0.79349\n",
      "i: 5900 , cost :  0.479506 , training accuracy :  0.794613\n",
      "i: 6000 , cost :  0.475206 , training accuracy :  0.794613\n",
      "i: 6100 , cost :  0.471387 , training accuracy :  0.796857\n",
      "i: 6200 , cost :  0.468078 , training accuracy :  0.796857\n",
      "i: 6300 , cost :  0.465403 , training accuracy :  0.796857\n",
      "i: 6400 , cost :  0.462662 , training accuracy :  0.796857\n",
      "i: 6500 , cost :  0.460248 , training accuracy :  0.795735\n",
      "i: 6600 , cost :  0.458016 , training accuracy :  0.796857\n",
      "i: 6700 , cost :  0.456014 , training accuracy :  0.796857\n",
      "i: 6800 , cost :  0.454659 , training accuracy :  0.79798\n",
      "i: 6900 , cost :  0.452984 , training accuracy :  0.800224\n",
      "i: 7000 , cost :  0.451635 , training accuracy :  0.801347\n",
      "i: 7100 , cost :  0.450608 , training accuracy :  0.800224\n",
      "i: 7200 , cost :  0.449698 , training accuracy :  0.799102\n",
      "i: 7300 , cost :  0.449013 , training accuracy :  0.800224\n",
      "i: 7400 , cost :  0.451442 , training accuracy :  0.809203\n",
      "i: 7500 , cost :  0.44774 , training accuracy :  0.802469\n",
      "i: 7600 , cost :  0.447455 , training accuracy :  0.801347\n",
      "i: 7700 , cost :  0.447483 , training accuracy :  0.801347\n",
      "i: 7800 , cost :  0.447704 , training accuracy :  0.800224\n",
      "i: 7900 , cost :  0.446745 , training accuracy :  0.79798\n",
      "i: 8000 , cost :  0.446547 , training accuracy :  0.799102\n",
      "i: 8100 , cost :  0.446518 , training accuracy :  0.800224\n",
      "i: 8200 , cost :  0.446196 , training accuracy :  0.801347\n",
      "i: 8300 , cost :  0.446053 , training accuracy :  0.803591\n",
      "i: 8400 , cost :  0.445916 , training accuracy :  0.800224\n",
      "i: 8500 , cost :  0.44577 , training accuracy :  0.800224\n",
      "i: 8600 , cost :  0.445726 , training accuracy :  0.79798\n",
      "i: 8700 , cost :  0.445594 , training accuracy :  0.79798\n",
      "i: 8800 , cost :  0.445417 , training accuracy :  0.799102\n",
      "i: 8900 , cost :  0.445351 , training accuracy :  0.79798\n",
      "i: 9000 , cost :  0.445833 , training accuracy :  0.79349\n",
      "i: 9100 , cost :  0.445117 , training accuracy :  0.79798\n",
      "i: 9200 , cost :  0.445035 , training accuracy :  0.79798\n",
      "i: 9300 , cost :  0.445163 , training accuracy :  0.801347\n",
      "i: 9400 , cost :  0.445387 , training accuracy :  0.795735\n",
      "i: 9500 , cost :  0.446045 , training accuracy :  0.802469\n",
      "i: 9600 , cost :  0.444683 , training accuracy :  0.800224\n",
      "i: 9700 , cost :  0.444618 , training accuracy :  0.801347\n",
      "i: 9800 , cost :  0.446592 , training accuracy :  0.792368\n",
      "i: 9900 , cost :  0.444458 , training accuracy :  0.800224\n",
      "Final training accuracy :  0.796857\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 10000 ][hidden: 3 ][file: bhavul_tr_acc_0.80_prediction.csv ] ACCURACY :  0.796857\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  265.582 , training accuracy :  0.615039\n",
      "i: 100 , cost :  108.867 , training accuracy :  0.655443\n",
      "i: 200 , cost :  38.879 , training accuracy :  0.634119\n",
      "i: 300 , cost :  19.075 , training accuracy :  0.628507\n",
      "i: 400 , cost :  8.04573 , training accuracy :  0.645342\n",
      "i: 500 , cost :  5.77913 , training accuracy :  0.652076\n",
      "i: 600 , cost :  4.27023 , training accuracy :  0.659933\n",
      "i: 700 , cost :  3.01916 , training accuracy :  0.664422\n",
      "i: 800 , cost :  2.2366 , training accuracy :  0.67789\n",
      "i: 900 , cost :  1.76084 , training accuracy :  0.698092\n",
      "i: 1000 , cost :  1.49593 , training accuracy :  0.717172\n",
      "i: 1100 , cost :  1.3355 , training accuracy :  0.741863\n",
      "i: 1200 , cost :  1.19165 , training accuracy :  0.744108\n",
      "i: 1300 , cost :  1.06405 , training accuracy :  0.756453\n",
      "i: 1400 , cost :  0.978659 , training accuracy :  0.750842\n",
      "i: 1500 , cost :  0.906828 , training accuracy :  0.757576\n",
      "i: 1600 , cost :  0.842379 , training accuracy :  0.769921\n",
      "i: 1700 , cost :  0.784657 , training accuracy :  0.772166\n",
      "i: 1800 , cost :  0.733155 , training accuracy :  0.773288\n",
      "i: 1900 , cost :  0.689386 , training accuracy :  0.775533\n",
      "i: 2000 , cost :  0.648321 , training accuracy :  0.781145\n",
      "i: 2100 , cost :  0.611654 , training accuracy :  0.781145\n",
      "i: 2200 , cost :  0.581706 , training accuracy :  0.776655\n",
      "i: 2300 , cost :  0.560051 , training accuracy :  0.777778\n",
      "i: 2400 , cost :  0.545055 , training accuracy :  0.7789\n",
      "i: 2500 , cost :  0.533718 , training accuracy :  0.782267\n",
      "i: 2600 , cost :  0.525159 , training accuracy :  0.781145\n",
      "i: 2700 , cost :  0.518368 , training accuracy :  0.780022\n",
      "i: 2800 , cost :  0.512679 , training accuracy :  0.782267\n",
      "i: 2900 , cost :  0.507737 , training accuracy :  0.782267\n",
      "i: 3000 , cost :  0.503275 , training accuracy :  0.781145\n",
      "i: 3100 , cost :  0.498995 , training accuracy :  0.782267\n",
      "i: 3200 , cost :  0.494774 , training accuracy :  0.783389\n",
      "i: 3300 , cost :  0.490597 , training accuracy :  0.783389\n",
      "i: 3400 , cost :  0.486451 , training accuracy :  0.783389\n",
      "i: 3500 , cost :  0.482349 , training accuracy :  0.783389\n",
      "i: 3600 , cost :  0.478309 , training accuracy :  0.785634\n",
      "i: 3700 , cost :  0.474354 , training accuracy :  0.785634\n",
      "i: 3800 , cost :  0.470519 , training accuracy :  0.786756\n",
      "i: 3900 , cost :  0.466857 , training accuracy :  0.784512\n",
      "i: 4000 , cost :  0.463408 , training accuracy :  0.786756\n",
      "i: 4100 , cost :  0.460199 , training accuracy :  0.784512\n",
      "i: 4200 , cost :  0.457577 , training accuracy :  0.791246\n",
      "i: 4300 , cost :  0.454624 , training accuracy :  0.790123\n",
      "i: 4400 , cost :  0.452227 , training accuracy :  0.79349\n",
      "i: 4500 , cost :  0.450097 , training accuracy :  0.79349\n",
      "i: 4600 , cost :  0.454165 , training accuracy :  0.795735\n",
      "i: 4700 , cost :  0.446537 , training accuracy :  0.796857\n",
      "i: 4800 , cost :  0.445038 , training accuracy :  0.79798\n",
      "i: 4900 , cost :  0.443886 , training accuracy :  0.799102\n",
      "i: 5000 , cost :  0.447932 , training accuracy :  0.791246\n",
      "i: 5100 , cost :  0.441889 , training accuracy :  0.79798\n",
      "i: 5200 , cost :  0.44212 , training accuracy :  0.800224\n",
      "i: 5300 , cost :  0.44011 , training accuracy :  0.79798\n",
      "i: 5400 , cost :  0.439225 , training accuracy :  0.799102\n",
      "i: 5500 , cost :  0.438071 , training accuracy :  0.803591\n",
      "i: 5600 , cost :  0.437634 , training accuracy :  0.800224\n",
      "i: 5700 , cost :  0.439493 , training accuracy :  0.800224\n",
      "i: 5800 , cost :  0.438066 , training accuracy :  0.800224\n",
      "i: 5900 , cost :  0.437212 , training accuracy :  0.800224\n",
      "i: 6000 , cost :  0.435874 , training accuracy :  0.802469\n",
      "i: 6100 , cost :  0.434976 , training accuracy :  0.802469\n",
      "i: 6200 , cost :  0.434616 , training accuracy :  0.801347\n",
      "i: 6300 , cost :  0.434297 , training accuracy :  0.801347\n",
      "i: 6400 , cost :  0.434036 , training accuracy :  0.801347\n",
      "i: 6500 , cost :  0.433812 , training accuracy :  0.800224\n",
      "i: 6600 , cost :  0.433546 , training accuracy :  0.801347\n",
      "i: 6700 , cost :  0.433312 , training accuracy :  0.801347\n",
      "i: 6800 , cost :  0.434208 , training accuracy :  0.805836\n",
      "i: 6900 , cost :  0.432928 , training accuracy :  0.799102\n",
      "i: 7000 , cost :  0.432785 , training accuracy :  0.801347\n",
      "i: 7100 , cost :  0.432609 , training accuracy :  0.79798\n",
      "i: 7200 , cost :  0.438154 , training accuracy :  0.792368\n",
      "i: 7300 , cost :  0.43233 , training accuracy :  0.79798\n",
      "i: 7400 , cost :  0.432214 , training accuracy :  0.799102\n",
      "i: 7500 , cost :  0.440107 , training accuracy :  0.789001\n",
      "i: 7600 , cost :  0.431972 , training accuracy :  0.799102\n",
      "i: 7700 , cost :  0.431839 , training accuracy :  0.800224\n",
      "i: 7800 , cost :  0.432082 , training accuracy :  0.79798\n",
      "i: 7900 , cost :  0.434943 , training accuracy :  0.811448\n",
      "i: 8000 , cost :  0.432111 , training accuracy :  0.795735\n",
      "i: 8100 , cost :  0.43114 , training accuracy :  0.799102\n",
      "i: 8200 , cost :  0.431023 , training accuracy :  0.79798\n",
      "i: 8300 , cost :  0.431114 , training accuracy :  0.801347\n",
      "i: 8400 , cost :  0.430737 , training accuracy :  0.799102\n",
      "i: 8500 , cost :  0.43057 , training accuracy :  0.800224\n",
      "i: 8600 , cost :  0.430346 , training accuracy :  0.799102\n",
      "i: 8700 , cost :  0.430163 , training accuracy :  0.800224\n",
      "i: 8800 , cost :  0.43185 , training accuracy :  0.806958\n",
      "i: 8900 , cost :  0.429967 , training accuracy :  0.79798\n",
      "i: 9000 , cost :  0.429736 , training accuracy :  0.799102\n",
      "i: 9100 , cost :  0.42962 , training accuracy :  0.800224\n",
      "i: 9200 , cost :  0.429741 , training accuracy :  0.796857\n",
      "i: 9300 , cost :  0.429376 , training accuracy :  0.800224\n",
      "i: 9400 , cost :  0.429261 , training accuracy :  0.79798\n",
      "i: 9500 , cost :  0.429164 , training accuracy :  0.801347\n",
      "i: 9600 , cost :  0.429223 , training accuracy :  0.799102\n",
      "i: 9700 , cost :  0.438228 , training accuracy :  0.810326\n",
      "i: 9800 , cost :  0.428775 , training accuracy :  0.800224\n",
      "i: 9900 , cost :  0.428667 , training accuracy :  0.79798\n",
      "Final training accuracy :  0.802469\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 10000 ][hidden: 10 ][file: bhavul_tr_acc_0.80_prediction.csv ] ACCURACY :  0.802469\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  370.286 , training accuracy :  0.62963\n",
      "i: 100 , cost :  176.851 , training accuracy :  0.654321\n",
      "i: 200 , cost :  25.6931 , training accuracy :  0.549944\n",
      "i: 300 , cost :  12.7181 , training accuracy :  0.593715\n",
      "i: 400 , cost :  7.91864 , training accuracy :  0.619529\n",
      "i: 500 , cost :  5.33422 , training accuracy :  0.648709\n",
      "i: 600 , cost :  4.40985 , training accuracy :  0.673401\n",
      "i: 700 , cost :  3.72237 , training accuracy :  0.679012\n",
      "i: 800 , cost :  3.08403 , training accuracy :  0.691358\n",
      "i: 900 , cost :  2.62601 , training accuracy :  0.701459\n",
      "i: 1000 , cost :  2.35677 , training accuracy :  0.713805\n",
      "i: 1100 , cost :  2.14629 , training accuracy :  0.720539\n",
      "i: 1200 , cost :  1.98812 , training accuracy :  0.723906\n",
      "i: 1300 , cost :  1.85278 , training accuracy :  0.73064\n",
      "i: 1400 , cost :  1.73884 , training accuracy :  0.731762\n",
      "i: 1500 , cost :  1.6413 , training accuracy :  0.741863\n",
      "i: 1600 , cost :  1.5566 , training accuracy :  0.744108\n",
      "i: 1700 , cost :  1.46797 , training accuracy :  0.751964\n",
      "i: 1800 , cost :  1.36312 , training accuracy :  0.753086\n",
      "i: 1900 , cost :  1.28942 , training accuracy :  0.750842\n",
      "i: 2000 , cost :  1.22744 , training accuracy :  0.755331\n",
      "i: 2100 , cost :  1.14545 , training accuracy :  0.753086\n",
      "i: 2200 , cost :  1.06474 , training accuracy :  0.758698\n",
      "i: 2300 , cost :  0.995816 , training accuracy :  0.766554\n",
      "i: 2400 , cost :  0.945616 , training accuracy :  0.767677\n",
      "i: 2500 , cost :  0.900634 , training accuracy :  0.767677\n",
      "i: 2600 , cost :  0.851428 , training accuracy :  0.780022\n",
      "i: 2700 , cost :  0.801285 , training accuracy :  0.783389\n",
      "i: 2800 , cost :  0.759715 , training accuracy :  0.785634\n",
      "i: 2900 , cost :  0.721378 , training accuracy :  0.790123\n",
      "i: 3000 , cost :  0.687214 , training accuracy :  0.795735\n",
      "i: 3100 , cost :  0.651519 , training accuracy :  0.79349\n",
      "i: 3200 , cost :  0.621247 , training accuracy :  0.790123\n",
      "i: 3300 , cost :  0.611188 , training accuracy :  0.800224\n",
      "i: 3400 , cost :  0.589196 , training accuracy :  0.790123\n",
      "i: 3500 , cost :  0.576437 , training accuracy :  0.792368\n",
      "i: 3600 , cost :  0.568233 , training accuracy :  0.804714\n",
      "i: 3700 , cost :  0.556986 , training accuracy :  0.796857\n",
      "i: 3800 , cost :  0.5471 , training accuracy :  0.796857\n",
      "i: 3900 , cost :  0.538452 , training accuracy :  0.800224\n",
      "i: 4000 , cost :  0.533076 , training accuracy :  0.803591\n",
      "i: 4100 , cost :  0.529333 , training accuracy :  0.790123\n",
      "i: 4200 , cost :  0.518406 , training accuracy :  0.804714\n",
      "i: 4300 , cost :  0.513752 , training accuracy :  0.787879\n",
      "i: 4400 , cost :  0.511429 , training accuracy :  0.805836\n",
      "i: 4500 , cost :  0.506008 , training accuracy :  0.805836\n",
      "i: 4600 , cost :  0.497846 , training accuracy :  0.81257\n",
      "i: 4700 , cost :  0.490344 , training accuracy :  0.79798\n",
      "i: 4800 , cost :  0.487554 , training accuracy :  0.804714\n",
      "i: 4900 , cost :  0.478887 , training accuracy :  0.809203\n",
      "i: 5000 , cost :  0.474299 , training accuracy :  0.810326\n",
      "i: 5100 , cost :  0.470064 , training accuracy :  0.81257\n",
      "i: 5200 , cost :  0.466729 , training accuracy :  0.81257\n",
      "i: 5300 , cost :  0.461149 , training accuracy :  0.804714\n",
      "i: 5400 , cost :  0.45879 , training accuracy :  0.818182\n",
      "i: 5500 , cost :  0.452286 , training accuracy :  0.805836\n",
      "i: 5600 , cost :  0.451293 , training accuracy :  0.809203\n",
      "i: 5700 , cost :  0.449456 , training accuracy :  0.814815\n",
      "i: 5800 , cost :  0.444129 , training accuracy :  0.815937\n",
      "i: 5900 , cost :  0.440867 , training accuracy :  0.811448\n",
      "i: 6000 , cost :  0.437077 , training accuracy :  0.810326\n",
      "i: 6100 , cost :  0.435873 , training accuracy :  0.810326\n",
      "i: 6200 , cost :  0.432937 , training accuracy :  0.814815\n",
      "i: 6300 , cost :  0.432309 , training accuracy :  0.815937\n",
      "i: 6400 , cost :  0.433597 , training accuracy :  0.811448\n",
      "i: 6500 , cost :  0.429837 , training accuracy :  0.814815\n",
      "i: 6600 , cost :  0.426621 , training accuracy :  0.81257\n",
      "i: 6700 , cost :  0.429391 , training accuracy :  0.817059\n",
      "i: 6800 , cost :  0.43086 , training accuracy :  0.814815\n",
      "i: 6900 , cost :  0.426633 , training accuracy :  0.815937\n",
      "i: 7000 , cost :  0.424878 , training accuracy :  0.817059\n",
      "i: 7100 , cost :  0.442377 , training accuracy :  0.811448\n",
      "i: 7200 , cost :  0.425912 , training accuracy :  0.819304\n",
      "i: 7300 , cost :  0.42375 , training accuracy :  0.818182\n",
      "i: 7400 , cost :  0.431055 , training accuracy :  0.814815\n",
      "i: 7500 , cost :  0.419702 , training accuracy :  0.817059\n",
      "i: 7600 , cost :  0.419979 , training accuracy :  0.819304\n",
      "i: 7700 , cost :  0.417984 , training accuracy :  0.811448\n",
      "i: 7800 , cost :  0.417494 , training accuracy :  0.813693\n",
      "i: 7900 , cost :  0.418209 , training accuracy :  0.818182\n",
      "i: 8000 , cost :  0.4182 , training accuracy :  0.817059\n",
      "i: 8100 , cost :  0.417413 , training accuracy :  0.819304\n",
      "i: 8200 , cost :  0.415494 , training accuracy :  0.814815\n",
      "i: 8300 , cost :  0.414886 , training accuracy :  0.81257\n",
      "i: 8400 , cost :  0.415915 , training accuracy :  0.822671\n",
      "i: 8500 , cost :  0.417634 , training accuracy :  0.823793\n",
      "i: 8600 , cost :  0.413734 , training accuracy :  0.819304\n",
      "i: 8700 , cost :  0.415931 , training accuracy :  0.820426\n",
      "i: 8800 , cost :  0.41336 , training accuracy :  0.820426\n",
      "i: 8900 , cost :  0.414115 , training accuracy :  0.818182\n",
      "i: 9000 , cost :  0.412802 , training accuracy :  0.820426\n",
      "i: 9100 , cost :  0.412281 , training accuracy :  0.823793\n",
      "i: 9200 , cost :  0.413659 , training accuracy :  0.819304\n",
      "i: 9300 , cost :  0.411428 , training accuracy :  0.817059\n",
      "i: 9400 , cost :  0.415295 , training accuracy :  0.821549\n",
      "i: 9500 , cost :  0.420392 , training accuracy :  0.820426\n",
      "i: 9600 , cost :  0.411843 , training accuracy :  0.828283\n",
      "i: 9700 , cost :  0.410432 , training accuracy :  0.818182\n",
      "i: 9800 , cost :  0.410602 , training accuracy :  0.823793\n",
      "i: 9900 , cost :  0.413107 , training accuracy :  0.824916\n",
      "Final training accuracy :  0.821549\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 10000 ][hidden: 15 ][file: bhavul_tr_acc_0.82_prediction.csv ] ACCURACY :  0.821549\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1461.03 , training accuracy :  0.383838\n",
      "i: 100 , cost :  563.842 , training accuracy :  0.329966\n",
      "i: 200 , cost :  85.5453 , training accuracy :  0.580247\n",
      "i: 300 , cost :  45.9311 , training accuracy :  0.607183\n",
      "i: 400 , cost :  24.5294 , training accuracy :  0.606061\n",
      "i: 500 , cost :  10.1716 , training accuracy :  0.609428\n",
      "i: 600 , cost :  5.47029 , training accuracy :  0.707071\n",
      "i: 700 , cost :  4.15767 , training accuracy :  0.717172\n",
      "i: 800 , cost :  3.33446 , training accuracy :  0.736251\n",
      "i: 900 , cost :  2.86148 , training accuracy :  0.747475\n",
      "i: 1000 , cost :  2.53734 , training accuracy :  0.760943\n",
      "i: 1100 , cost :  2.26741 , training accuracy :  0.765432\n",
      "i: 1200 , cost :  2.06543 , training accuracy :  0.75982\n",
      "i: 1300 , cost :  1.91979 , training accuracy :  0.768799\n",
      "i: 1400 , cost :  1.80567 , training accuracy :  0.774411\n",
      "i: 1500 , cost :  1.71613 , training accuracy :  0.772166\n",
      "i: 1600 , cost :  1.63668 , training accuracy :  0.775533\n",
      "i: 1700 , cost :  1.5625 , training accuracy :  0.784512\n",
      "i: 1800 , cost :  1.49719 , training accuracy :  0.784512\n",
      "i: 1900 , cost :  1.43763 , training accuracy :  0.783389\n",
      "i: 2000 , cost :  1.38406 , training accuracy :  0.781145\n",
      "i: 2100 , cost :  1.33316 , training accuracy :  0.782267\n",
      "i: 2200 , cost :  1.28488 , training accuracy :  0.783389\n",
      "i: 2300 , cost :  1.23929 , training accuracy :  0.787879\n",
      "i: 2400 , cost :  1.19683 , training accuracy :  0.787879\n",
      "i: 2500 , cost :  1.15619 , training accuracy :  0.792368\n",
      "i: 2600 , cost :  1.12028 , training accuracy :  0.79349\n",
      "i: 2700 , cost :  1.07946 , training accuracy :  0.794613\n",
      "i: 2800 , cost :  1.04762 , training accuracy :  0.794613\n",
      "i: 2900 , cost :  1.0086 , training accuracy :  0.796857\n",
      "i: 3000 , cost :  0.974445 , training accuracy :  0.795735\n",
      "i: 3100 , cost :  0.928099 , training accuracy :  0.804714\n",
      "i: 3200 , cost :  0.897446 , training accuracy :  0.804714\n",
      "i: 3300 , cost :  0.885065 , training accuracy :  0.799102\n",
      "i: 3400 , cost :  0.841562 , training accuracy :  0.808081\n",
      "i: 3500 , cost :  0.814628 , training accuracy :  0.805836\n",
      "i: 3600 , cost :  0.789136 , training accuracy :  0.802469\n",
      "i: 3700 , cost :  0.756192 , training accuracy :  0.806958\n",
      "i: 3800 , cost :  0.736105 , training accuracy :  0.804714\n",
      "i: 3900 , cost :  0.715889 , training accuracy :  0.808081\n",
      "i: 4000 , cost :  0.697363 , training accuracy :  0.813693\n",
      "i: 4100 , cost :  0.681051 , training accuracy :  0.811448\n",
      "i: 4200 , cost :  0.667261 , training accuracy :  0.814815\n",
      "i: 4300 , cost :  0.6547 , training accuracy :  0.806958\n",
      "i: 4400 , cost :  0.641598 , training accuracy :  0.808081\n",
      "i: 4500 , cost :  0.628883 , training accuracy :  0.808081\n",
      "i: 4600 , cost :  0.615231 , training accuracy :  0.813693\n",
      "i: 4700 , cost :  0.602975 , training accuracy :  0.811448\n",
      "i: 4800 , cost :  0.592227 , training accuracy :  0.81257\n",
      "i: 4900 , cost :  0.579217 , training accuracy :  0.813693\n",
      "i: 5000 , cost :  0.56644 , training accuracy :  0.818182\n",
      "i: 5100 , cost :  0.55439 , training accuracy :  0.814815\n",
      "i: 5200 , cost :  0.541163 , training accuracy :  0.818182\n",
      "i: 5300 , cost :  0.529486 , training accuracy :  0.820426\n",
      "i: 5400 , cost :  0.520029 , training accuracy :  0.819304\n",
      "i: 5500 , cost :  0.510562 , training accuracy :  0.819304\n",
      "i: 5600 , cost :  0.498474 , training accuracy :  0.826038\n",
      "i: 5700 , cost :  0.49007 , training accuracy :  0.824916\n",
      "i: 5800 , cost :  0.487585 , training accuracy :  0.823793\n",
      "i: 5900 , cost :  0.481028 , training accuracy :  0.82716\n",
      "i: 6000 , cost :  0.470429 , training accuracy :  0.83165\n",
      "i: 6100 , cost :  0.463283 , training accuracy :  0.830527\n",
      "i: 6200 , cost :  0.457868 , training accuracy :  0.830527\n",
      "i: 6300 , cost :  0.454421 , training accuracy :  0.833894\n",
      "i: 6400 , cost :  0.449537 , training accuracy :  0.836139\n",
      "i: 6500 , cost :  0.449579 , training accuracy :  0.839506\n",
      "i: 6600 , cost :  0.439746 , training accuracy :  0.839506\n",
      "i: 6700 , cost :  0.444172 , training accuracy :  0.842873\n",
      "i: 6800 , cost :  0.438199 , training accuracy :  0.842873\n",
      "i: 6900 , cost :  0.439083 , training accuracy :  0.843996\n",
      "i: 7000 , cost :  0.43146 , training accuracy :  0.841751\n",
      "i: 7100 , cost :  0.434791 , training accuracy :  0.84624\n",
      "i: 7200 , cost :  0.426136 , training accuracy :  0.848485\n",
      "i: 7300 , cost :  0.423771 , training accuracy :  0.848485\n",
      "i: 7400 , cost :  0.421815 , training accuracy :  0.849607\n",
      "i: 7500 , cost :  0.419624 , training accuracy :  0.85073\n",
      "i: 7600 , cost :  0.419827 , training accuracy :  0.852974\n",
      "i: 7700 , cost :  0.417183 , training accuracy :  0.852974\n",
      "i: 7800 , cost :  0.415242 , training accuracy :  0.854097\n",
      "i: 7900 , cost :  0.412677 , training accuracy :  0.85073\n",
      "i: 8000 , cost :  0.411873 , training accuracy :  0.854097\n",
      "i: 8100 , cost :  0.410871 , training accuracy :  0.852974\n",
      "i: 8200 , cost :  0.410225 , training accuracy :  0.854097\n",
      "i: 8300 , cost :  0.40935 , training accuracy :  0.854097\n",
      "i: 8400 , cost :  0.406807 , training accuracy :  0.855219\n",
      "i: 8500 , cost :  0.407116 , training accuracy :  0.858586\n",
      "i: 8600 , cost :  0.404268 , training accuracy :  0.856341\n",
      "i: 8700 , cost :  0.403913 , training accuracy :  0.858586\n",
      "i: 8800 , cost :  0.402074 , training accuracy :  0.858586\n",
      "i: 8900 , cost :  0.402544 , training accuracy :  0.857464\n",
      "i: 9000 , cost :  0.400123 , training accuracy :  0.858586\n",
      "i: 9100 , cost :  0.400535 , training accuracy :  0.857464\n",
      "i: 9200 , cost :  0.398819 , training accuracy :  0.858586\n",
      "i: 9300 , cost :  0.396307 , training accuracy :  0.859708\n",
      "i: 9400 , cost :  0.395406 , training accuracy :  0.858586\n",
      "i: 9500 , cost :  0.395668 , training accuracy :  0.857464\n",
      "i: 9600 , cost :  0.392831 , training accuracy :  0.857464\n",
      "i: 9700 , cost :  0.390591 , training accuracy :  0.858586\n",
      "i: 9800 , cost :  0.391864 , training accuracy :  0.857464\n",
      "i: 9900 , cost :  0.389274 , training accuracy :  0.855219\n",
      "Final training accuracy :  0.857464\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 10000 ][hidden: 50 ][file: bhavul_tr_acc_0.86_prediction.csv ] ACCURACY :  0.857464\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  584.305 , training accuracy :  0.616162\n",
      "i: 100 , cost :  39.8914 , training accuracy :  0.615039\n",
      "i: 200 , cost :  16.5876 , training accuracy :  0.636364\n",
      "i: 300 , cost :  6.6033 , training accuracy :  0.69697\n",
      "i: 400 , cost :  4.70745 , training accuracy :  0.718294\n",
      "i: 500 , cost :  3.64167 , training accuracy :  0.739618\n",
      "i: 600 , cost :  2.80278 , training accuracy :  0.747475\n",
      "i: 700 , cost :  2.22099 , training accuracy :  0.760943\n",
      "i: 800 , cost :  1.87829 , training accuracy :  0.751964\n",
      "i: 900 , cost :  1.71577 , training accuracy :  0.762065\n",
      "i: 1000 , cost :  1.48701 , training accuracy :  0.775533\n",
      "i: 1100 , cost :  1.28525 , training accuracy :  0.777778\n",
      "i: 1200 , cost :  1.23172 , training accuracy :  0.73064\n",
      "i: 1300 , cost :  1.11951 , training accuracy :  0.742985\n",
      "i: 1400 , cost :  1.12537 , training accuracy :  0.763187\n",
      "i: 1500 , cost :  1.00696 , training accuracy :  0.774411\n",
      "i: 1600 , cost :  0.887241 , training accuracy :  0.79349\n",
      "i: 1700 , cost :  0.930089 , training accuracy :  0.755331\n",
      "i: 1800 , cost :  0.891907 , training accuracy :  0.748597\n",
      "i: 1900 , cost :  0.785778 , training accuracy :  0.787879\n",
      "i: 2000 , cost :  0.739502 , training accuracy :  0.815937\n",
      "i: 2100 , cost :  0.934487 , training accuracy :  0.777778\n",
      "i: 2200 , cost :  0.782177 , training accuracy :  0.804714\n",
      "i: 2300 , cost :  0.671013 , training accuracy :  0.821549\n",
      "i: 2400 , cost :  0.671847 , training accuracy :  0.808081\n",
      "i: 2500 , cost :  0.725475 , training accuracy :  0.7789\n",
      "i: 2600 , cost :  0.730706 , training accuracy :  0.767677\n",
      "i: 2700 , cost :  0.696245 , training accuracy :  0.780022\n",
      "i: 2800 , cost :  0.62693 , training accuracy :  0.810326\n",
      "i: 2900 , cost :  0.595996 , training accuracy :  0.817059\n",
      "i: 3000 , cost :  0.584945 , training accuracy :  0.819304\n",
      "i: 3100 , cost :  0.582577 , training accuracy :  0.818182\n",
      "i: 3200 , cost :  0.609945 , training accuracy :  0.79798\n",
      "i: 3300 , cost :  0.650728 , training accuracy :  0.780022\n",
      "i: 3400 , cost :  0.59931 , training accuracy :  0.802469\n",
      "i: 3500 , cost :  0.528132 , training accuracy :  0.833894\n",
      "i: 3600 , cost :  0.6211 , training accuracy :  0.814815\n",
      "i: 3700 , cost :  0.717712 , training accuracy :  0.789001\n",
      "i: 3800 , cost :  0.57525 , training accuracy :  0.833894\n",
      "i: 3900 , cost :  0.510617 , training accuracy :  0.841751\n",
      "i: 4000 , cost :  0.502991 , training accuracy :  0.833894\n",
      "i: 4100 , cost :  0.514262 , training accuracy :  0.817059\n",
      "i: 4200 , cost :  0.537011 , training accuracy :  0.809203\n",
      "i: 4300 , cost :  0.547627 , training accuracy :  0.799102\n",
      "i: 4400 , cost :  0.587296 , training accuracy :  0.787879\n",
      "i: 4500 , cost :  0.587365 , training accuracy :  0.786756\n",
      "i: 4600 , cost :  0.5278 , training accuracy :  0.813693\n",
      "i: 4700 , cost :  0.469306 , training accuracy :  0.839506\n",
      "i: 4800 , cost :  0.475851 , training accuracy :  0.840629\n",
      "i: 4900 , cost :  0.458899 , training accuracy :  0.842873\n",
      "i: 5000 , cost :  0.588119 , training accuracy :  0.802469\n",
      "i: 5100 , cost :  0.474452 , training accuracy :  0.840629\n",
      "i: 5200 , cost :  0.446989 , training accuracy :  0.847363\n",
      "i: 5300 , cost :  0.484624 , training accuracy :  0.821549\n",
      "i: 5400 , cost :  0.450412 , training accuracy :  0.848485\n",
      "i: 5500 , cost :  0.520242 , training accuracy :  0.840629\n",
      "i: 5600 , cost :  0.450877 , training accuracy :  0.83165\n",
      "i: 5700 , cost :  0.499893 , training accuracy :  0.813693\n",
      "i: 5800 , cost :  0.446633 , training accuracy :  0.836139\n",
      "i: 5900 , cost :  0.430155 , training accuracy :  0.847363\n",
      "i: 6000 , cost :  0.439008 , training accuracy :  0.859708\n",
      "i: 6100 , cost :  0.548506 , training accuracy :  0.818182\n",
      "i: 6200 , cost :  0.443238 , training accuracy :  0.829405\n",
      "i: 6300 , cost :  0.438315 , training accuracy :  0.858586\n",
      "i: 6400 , cost :  0.462836 , training accuracy :  0.823793\n",
      "i: 6500 , cost :  0.800829 , training accuracy :  0.802469\n",
      "i: 6600 , cost :  0.4131 , training accuracy :  0.851852\n",
      "i: 6700 , cost :  0.439745 , training accuracy :  0.851852\n",
      "i: 6800 , cost :  0.426792 , training accuracy :  0.859708\n",
      "i: 6900 , cost :  0.586908 , training accuracy :  0.811448\n",
      "i: 7000 , cost :  0.522313 , training accuracy :  0.801347\n",
      "i: 7100 , cost :  0.393209 , training accuracy :  0.852974\n",
      "i: 7200 , cost :  0.619945 , training accuracy :  0.796857\n",
      "i: 7300 , cost :  0.396033 , training accuracy :  0.857464\n",
      "i: 7400 , cost :  0.399089 , training accuracy :  0.866442\n",
      "i: 7500 , cost :  0.387847 , training accuracy :  0.86532\n",
      "i: 7600 , cost :  0.435562 , training accuracy :  0.828283\n",
      "i: 7700 , cost :  0.46753 , training accuracy :  0.852974\n",
      "i: 7800 , cost :  0.388296 , training accuracy :  0.861953\n",
      "i: 7900 , cost :  0.378113 , training accuracy :  0.861953\n",
      "i: 8000 , cost :  0.417715 , training accuracy :  0.838384\n",
      "i: 8100 , cost :  0.617126 , training accuracy :  0.835017\n",
      "i: 8200 , cost :  0.411343 , training accuracy :  0.860831\n",
      "i: 8300 , cost :  0.379018 , training accuracy :  0.86532\n",
      "i: 8400 , cost :  0.371964 , training accuracy :  0.867565\n",
      "i: 8500 , cost :  0.375281 , training accuracy :  0.856341\n",
      "i: 8600 , cost :  0.478798 , training accuracy :  0.810326\n",
      "i: 8700 , cost :  0.390853 , training accuracy :  0.870932\n",
      "i: 8800 , cost :  0.383359 , training accuracy :  0.875421\n",
      "i: 8900 , cost :  0.373111 , training accuracy :  0.858586\n",
      "i: 9000 , cost :  1.13629 , training accuracy :  0.787879\n",
      "i: 9100 , cost :  0.385035 , training accuracy :  0.866442\n",
      "i: 9200 , cost :  0.366271 , training accuracy :  0.867565\n",
      "i: 9300 , cost :  0.448634 , training accuracy :  0.855219\n",
      "i: 9400 , cost :  0.403675 , training accuracy :  0.849607\n",
      "i: 9500 , cost :  0.363589 , training accuracy :  0.86532\n",
      "i: 9600 , cost :  0.424542 , training accuracy :  0.833894\n",
      "i: 9700 , cost :  0.468143 , training accuracy :  0.81257\n",
      "i: 9800 , cost :  0.38408 , training accuracy :  0.877666\n",
      "i: 9900 , cost :  0.801564 , training accuracy :  0.716049\n",
      "Final training accuracy :  0.870932\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 10000 ][hidden: 100 ][file: bhavul_tr_acc_0.87_prediction.csv ] ACCURACY :  0.870932\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  11.4854 , training accuracy :  0.601571\n",
      "i: 100 , cost :  4.61792 , training accuracy :  0.630752\n",
      "i: 200 , cost :  2.36542 , training accuracy :  0.635241\n",
      "i: 300 , cost :  1.57332 , training accuracy :  0.622896\n",
      "i: 400 , cost :  1.16072 , training accuracy :  0.62514\n",
      "i: 500 , cost :  0.98664 , training accuracy :  0.630752\n",
      "i: 600 , cost :  0.898534 , training accuracy :  0.626263\n",
      "i: 700 , cost :  0.831113 , training accuracy :  0.624018\n",
      "i: 800 , cost :  0.785998 , training accuracy :  0.624018\n",
      "i: 900 , cost :  0.756235 , training accuracy :  0.621773\n",
      "i: 1000 , cost :  0.735051 , training accuracy :  0.619529\n",
      "i: 1100 , cost :  0.717844 , training accuracy :  0.618406\n",
      "i: 1200 , cost :  0.705433 , training accuracy :  0.619529\n",
      "i: 1300 , cost :  0.696026 , training accuracy :  0.617284\n",
      "i: 1400 , cost :  0.689889 , training accuracy :  0.616162\n",
      "i: 1500 , cost :  0.68445 , training accuracy :  0.615039\n",
      "i: 1600 , cost :  0.680353 , training accuracy :  0.616162\n",
      "i: 1700 , cost :  0.676941 , training accuracy :  0.616162\n",
      "i: 1800 , cost :  0.674666 , training accuracy :  0.617284\n",
      "i: 1900 , cost :  0.672843 , training accuracy :  0.617284\n",
      "i: 2000 , cost :  0.671427 , training accuracy :  0.617284\n",
      "i: 2100 , cost :  0.670392 , training accuracy :  0.617284\n",
      "i: 2200 , cost :  0.669364 , training accuracy :  0.618406\n",
      "i: 2300 , cost :  0.668413 , training accuracy :  0.618406\n",
      "i: 2400 , cost :  0.66766 , training accuracy :  0.618406\n",
      "i: 2500 , cost :  0.667037 , training accuracy :  0.619529\n",
      "i: 2600 , cost :  0.666342 , training accuracy :  0.619529\n",
      "i: 2700 , cost :  0.665724 , training accuracy :  0.619529\n",
      "i: 2800 , cost :  0.665228 , training accuracy :  0.619529\n",
      "i: 2900 , cost :  0.664799 , training accuracy :  0.619529\n",
      "i: 3000 , cost :  0.664448 , training accuracy :  0.619529\n",
      "i: 3100 , cost :  0.664156 , training accuracy :  0.619529\n",
      "i: 3200 , cost :  0.663647 , training accuracy :  0.619529\n",
      "i: 3300 , cost :  0.663243 , training accuracy :  0.619529\n",
      "i: 3400 , cost :  0.663003 , training accuracy :  0.619529\n",
      "i: 3500 , cost :  0.662791 , training accuracy :  0.620651\n",
      "i: 3600 , cost :  0.662598 , training accuracy :  0.621773\n",
      "i: 3700 , cost :  0.662418 , training accuracy :  0.621773\n",
      "i: 3800 , cost :  0.66225 , training accuracy :  0.621773\n",
      "i: 3900 , cost :  0.662095 , training accuracy :  0.621773\n",
      "i: 4000 , cost :  0.661928 , training accuracy :  0.621773\n",
      "i: 4100 , cost :  0.661757 , training accuracy :  0.621773\n",
      "i: 4200 , cost :  0.6616 , training accuracy :  0.621773\n",
      "i: 4300 , cost :  0.661452 , training accuracy :  0.620651\n",
      "i: 4400 , cost :  0.661314 , training accuracy :  0.620651\n",
      "i: 4500 , cost :  0.661186 , training accuracy :  0.620651\n",
      "i: 4600 , cost :  0.661065 , training accuracy :  0.620651\n",
      "i: 4700 , cost :  0.660954 , training accuracy :  0.620651\n",
      "i: 4800 , cost :  0.660846 , training accuracy :  0.620651\n",
      "i: 4900 , cost :  0.660744 , training accuracy :  0.620651\n",
      "i: 5000 , cost :  0.660651 , training accuracy :  0.620651\n",
      "i: 5100 , cost :  0.660563 , training accuracy :  0.620651\n",
      "i: 5200 , cost :  0.660479 , training accuracy :  0.619529\n",
      "i: 5300 , cost :  0.660409 , training accuracy :  0.619529\n",
      "i: 5400 , cost :  0.660346 , training accuracy :  0.619529\n",
      "i: 5500 , cost :  0.660285 , training accuracy :  0.619529\n",
      "i: 5600 , cost :  0.660227 , training accuracy :  0.619529\n",
      "i: 5700 , cost :  0.660166 , training accuracy :  0.619529\n",
      "i: 5800 , cost :  0.660113 , training accuracy :  0.619529\n",
      "i: 5900 , cost :  0.660049 , training accuracy :  0.619529\n",
      "i: 6000 , cost :  0.659988 , training accuracy :  0.620651\n",
      "i: 6100 , cost :  0.65993 , training accuracy :  0.620651\n",
      "i: 6200 , cost :  0.659871 , training accuracy :  0.620651\n",
      "i: 6300 , cost :  0.659814 , training accuracy :  0.620651\n",
      "i: 6400 , cost :  0.659762 , training accuracy :  0.620651\n",
      "i: 6500 , cost :  0.659713 , training accuracy :  0.620651\n",
      "i: 6600 , cost :  0.659664 , training accuracy :  0.620651\n",
      "i: 6700 , cost :  0.659616 , training accuracy :  0.620651\n",
      "i: 6800 , cost :  0.659567 , training accuracy :  0.620651\n",
      "i: 6900 , cost :  0.659521 , training accuracy :  0.620651\n",
      "i: 7000 , cost :  0.659477 , training accuracy :  0.620651\n",
      "i: 7100 , cost :  0.659433 , training accuracy :  0.620651\n",
      "i: 7200 , cost :  0.659392 , training accuracy :  0.620651\n",
      "i: 7300 , cost :  0.659351 , training accuracy :  0.620651\n",
      "i: 7400 , cost :  0.659312 , training accuracy :  0.620651\n",
      "i: 7500 , cost :  0.659273 , training accuracy :  0.620651\n",
      "i: 7600 , cost :  0.659222 , training accuracy :  0.620651\n",
      "i: 7700 , cost :  0.65916 , training accuracy :  0.620651\n",
      "i: 7800 , cost :  0.659106 , training accuracy :  0.620651\n",
      "i: 7900 , cost :  0.659044 , training accuracy :  0.620651\n",
      "i: 8000 , cost :  0.658992 , training accuracy :  0.620651\n",
      "i: 8100 , cost :  0.658935 , training accuracy :  0.620651\n",
      "i: 8200 , cost :  0.658877 , training accuracy :  0.620651\n",
      "i: 8300 , cost :  0.658834 , training accuracy :  0.620651\n",
      "i: 8400 , cost :  0.65877 , training accuracy :  0.620651\n",
      "i: 8500 , cost :  0.658717 , training accuracy :  0.620651\n",
      "i: 8600 , cost :  0.658664 , training accuracy :  0.620651\n",
      "i: 8700 , cost :  0.658611 , training accuracy :  0.620651\n",
      "i: 8800 , cost :  0.658229 , training accuracy :  0.620651\n",
      "i: 8900 , cost :  0.657889 , training accuracy :  0.620651\n",
      "i: 9000 , cost :  0.657671 , training accuracy :  0.620651\n",
      "i: 9100 , cost :  0.657509 , training accuracy :  0.621773\n",
      "i: 9200 , cost :  0.657396 , training accuracy :  0.621773\n",
      "i: 9300 , cost :  0.657039 , training accuracy :  0.621773\n",
      "i: 9400 , cost :  0.656527 , training accuracy :  0.621773\n",
      "i: 9500 , cost :  0.656039 , training accuracy :  0.622896\n",
      "i: 9600 , cost :  0.655873 , training accuracy :  0.621773\n",
      "i: 9700 , cost :  0.654763 , training accuracy :  0.622896\n",
      "i: 9800 , cost :  0.647755 , training accuracy :  0.638608\n",
      "i: 9900 , cost :  0.638347 , training accuracy :  0.652076\n",
      "i: 10000 , cost :  0.623629 , training accuracy :  0.670034\n",
      "i: 10100 , cost :  0.616656 , training accuracy :  0.676768\n",
      "i: 10200 , cost :  0.613607 , training accuracy :  0.679012\n",
      "i: 10300 , cost :  0.612325 , training accuracy :  0.684624\n",
      "i: 10400 , cost :  0.611251 , training accuracy :  0.684624\n",
      "i: 10500 , cost :  0.609984 , training accuracy :  0.684624\n",
      "i: 10600 , cost :  0.608886 , training accuracy :  0.684624\n",
      "i: 10700 , cost :  0.608301 , training accuracy :  0.686869\n",
      "i: 10800 , cost :  0.607891 , training accuracy :  0.689113\n",
      "i: 10900 , cost :  0.607526 , training accuracy :  0.690236\n",
      "i: 11000 , cost :  0.607215 , training accuracy :  0.69248\n",
      "i: 11100 , cost :  0.606938 , training accuracy :  0.69248\n",
      "i: 11200 , cost :  0.606692 , training accuracy :  0.69697\n",
      "i: 11300 , cost :  0.606481 , training accuracy :  0.695847\n",
      "i: 11400 , cost :  0.606271 , training accuracy :  0.693603\n",
      "i: 11500 , cost :  0.606077 , training accuracy :  0.693603\n",
      "i: 11600 , cost :  0.6059 , training accuracy :  0.693603\n",
      "i: 11700 , cost :  0.605658 , training accuracy :  0.693603\n",
      "i: 11800 , cost :  0.605428 , training accuracy :  0.693603\n",
      "i: 11900 , cost :  0.605184 , training accuracy :  0.693603\n",
      "i: 12000 , cost :  0.604914 , training accuracy :  0.693603\n",
      "i: 12100 , cost :  0.604606 , training accuracy :  0.693603\n",
      "i: 12200 , cost :  0.604091 , training accuracy :  0.69697\n",
      "i: 12300 , cost :  0.603458 , training accuracy :  0.698092\n",
      "i: 12400 , cost :  0.602642 , training accuracy :  0.700337\n",
      "i: 12500 , cost :  0.600476 , training accuracy :  0.705948\n",
      "i: 12600 , cost :  0.596196 , training accuracy :  0.713805\n",
      "i: 12700 , cost :  0.563558 , training accuracy :  0.732884\n",
      "i: 12800 , cost :  0.529068 , training accuracy :  0.766554\n",
      "i: 12900 , cost :  0.509866 , training accuracy :  0.773288\n",
      "i: 13000 , cost :  0.496537 , training accuracy :  0.785634\n",
      "i: 13100 , cost :  0.486429 , training accuracy :  0.792368\n",
      "i: 13200 , cost :  0.478886 , training accuracy :  0.792368\n",
      "i: 13300 , cost :  0.473054 , training accuracy :  0.79798\n",
      "i: 13400 , cost :  0.466978 , training accuracy :  0.799102\n",
      "i: 13500 , cost :  0.458741 , training accuracy :  0.801347\n",
      "i: 13600 , cost :  0.452798 , training accuracy :  0.806958\n",
      "i: 13700 , cost :  0.449347 , training accuracy :  0.813693\n",
      "i: 13800 , cost :  0.447533 , training accuracy :  0.815937\n",
      "i: 13900 , cost :  0.446396 , training accuracy :  0.814815\n",
      "i: 14000 , cost :  0.445775 , training accuracy :  0.817059\n",
      "i: 14100 , cost :  0.445477 , training accuracy :  0.814815\n",
      "i: 14200 , cost :  0.445272 , training accuracy :  0.813693\n",
      "i: 14300 , cost :  0.444975 , training accuracy :  0.815937\n",
      "i: 14400 , cost :  0.444834 , training accuracy :  0.814815\n",
      "i: 14500 , cost :  0.444711 , training accuracy :  0.814815\n",
      "i: 14600 , cost :  0.444616 , training accuracy :  0.814815\n",
      "i: 14700 , cost :  0.444528 , training accuracy :  0.814815\n",
      "i: 14800 , cost :  0.444432 , training accuracy :  0.81257\n",
      "i: 14900 , cost :  0.444345 , training accuracy :  0.81257\n",
      "i: 15000 , cost :  0.44419 , training accuracy :  0.813693\n",
      "i: 15100 , cost :  0.444116 , training accuracy :  0.814815\n",
      "i: 15200 , cost :  0.443938 , training accuracy :  0.814815\n",
      "i: 15300 , cost :  0.443828 , training accuracy :  0.814815\n",
      "i: 15400 , cost :  0.443355 , training accuracy :  0.811448\n",
      "i: 15500 , cost :  0.442654 , training accuracy :  0.813693\n",
      "i: 15600 , cost :  0.437211 , training accuracy :  0.813693\n",
      "i: 15700 , cost :  0.431572 , training accuracy :  0.81257\n",
      "i: 15800 , cost :  0.428492 , training accuracy :  0.811448\n",
      "i: 15900 , cost :  0.426802 , training accuracy :  0.818182\n",
      "i: 16000 , cost :  0.42594 , training accuracy :  0.818182\n",
      "i: 16100 , cost :  0.425468 , training accuracy :  0.822671\n",
      "i: 16200 , cost :  0.425207 , training accuracy :  0.821549\n",
      "i: 16300 , cost :  0.425804 , training accuracy :  0.815937\n",
      "i: 16400 , cost :  0.425078 , training accuracy :  0.823793\n",
      "i: 16500 , cost :  0.42506 , training accuracy :  0.823793\n",
      "i: 16600 , cost :  0.425063 , training accuracy :  0.823793\n",
      "i: 16700 , cost :  0.425085 , training accuracy :  0.824916\n",
      "i: 16800 , cost :  0.425082 , training accuracy :  0.824916\n",
      "i: 16900 , cost :  0.425078 , training accuracy :  0.824916\n",
      "i: 17000 , cost :  0.425078 , training accuracy :  0.824916\n",
      "i: 17100 , cost :  0.425077 , training accuracy :  0.824916\n",
      "i: 17200 , cost :  0.425074 , training accuracy :  0.824916\n",
      "i: 17300 , cost :  0.425076 , training accuracy :  0.824916\n",
      "i: 17400 , cost :  0.425074 , training accuracy :  0.824916\n",
      "i: 17500 , cost :  0.425072 , training accuracy :  0.824916\n",
      "i: 17600 , cost :  0.425073 , training accuracy :  0.824916\n",
      "i: 17700 , cost :  0.425073 , training accuracy :  0.824916\n",
      "i: 17800 , cost :  0.425072 , training accuracy :  0.824916\n",
      "i: 17900 , cost :  0.425071 , training accuracy :  0.824916\n",
      "i: 18000 , cost :  0.425068 , training accuracy :  0.824916\n",
      "i: 18100 , cost :  0.425071 , training accuracy :  0.824916\n",
      "i: 18200 , cost :  0.425068 , training accuracy :  0.824916\n",
      "i: 18300 , cost :  0.425071 , training accuracy :  0.824916\n",
      "i: 18400 , cost :  0.425069 , training accuracy :  0.824916\n",
      "i: 18500 , cost :  0.425067 , training accuracy :  0.824916\n",
      "i: 18600 , cost :  0.425067 , training accuracy :  0.824916\n",
      "i: 18700 , cost :  0.425066 , training accuracy :  0.824916\n",
      "i: 18800 , cost :  0.425067 , training accuracy :  0.824916\n",
      "i: 18900 , cost :  0.425065 , training accuracy :  0.824916\n",
      "i: 19000 , cost :  0.425067 , training accuracy :  0.824916\n",
      "i: 19100 , cost :  0.425068 , training accuracy :  0.824916\n",
      "i: 19200 , cost :  0.425066 , training accuracy :  0.824916\n",
      "i: 19300 , cost :  0.425067 , training accuracy :  0.824916\n",
      "i: 19400 , cost :  0.425068 , training accuracy :  0.824916\n",
      "i: 19500 , cost :  0.425065 , training accuracy :  0.824916\n",
      "i: 19600 , cost :  0.425066 , training accuracy :  0.824916\n",
      "i: 19700 , cost :  0.425064 , training accuracy :  0.824916\n",
      "i: 19800 , cost :  0.425064 , training accuracy :  0.824916\n",
      "i: 19900 , cost :  0.425069 , training accuracy :  0.824916\n",
      "i: 20000 , cost :  0.425064 , training accuracy :  0.824916\n",
      "i: 20100 , cost :  0.425063 , training accuracy :  0.824916\n",
      "i: 20200 , cost :  0.425066 , training accuracy :  0.824916\n",
      "i: 20300 , cost :  0.425062 , training accuracy :  0.824916\n",
      "i: 20400 , cost :  0.425062 , training accuracy :  0.824916\n",
      "i: 20500 , cost :  0.425062 , training accuracy :  0.824916\n",
      "i: 20600 , cost :  0.425068 , training accuracy :  0.824916\n",
      "i: 20700 , cost :  0.425062 , training accuracy :  0.824916\n",
      "i: 20800 , cost :  0.425066 , training accuracy :  0.824916\n",
      "i: 20900 , cost :  0.425064 , training accuracy :  0.824916\n",
      "i: 21000 , cost :  0.425063 , training accuracy :  0.824916\n",
      "i: 21100 , cost :  0.425061 , training accuracy :  0.824916\n",
      "i: 21200 , cost :  0.425061 , training accuracy :  0.824916\n",
      "i: 21300 , cost :  0.425061 , training accuracy :  0.824916\n",
      "i: 21400 , cost :  0.425061 , training accuracy :  0.824916\n",
      "i: 21500 , cost :  0.42506 , training accuracy :  0.824916\n",
      "i: 21600 , cost :  0.425064 , training accuracy :  0.824916\n",
      "i: 21700 , cost :  0.425065 , training accuracy :  0.824916\n",
      "i: 21800 , cost :  0.425065 , training accuracy :  0.824916\n",
      "i: 21900 , cost :  0.425063 , training accuracy :  0.824916\n",
      "i: 22000 , cost :  0.42506 , training accuracy :  0.824916\n",
      "i: 22100 , cost :  0.42506 , training accuracy :  0.824916\n",
      "i: 22200 , cost :  0.42506 , training accuracy :  0.824916\n",
      "i: 22300 , cost :  0.42506 , training accuracy :  0.824916\n",
      "i: 22400 , cost :  0.42506 , training accuracy :  0.824916\n",
      "i: 22500 , cost :  0.425061 , training accuracy :  0.824916\n",
      "i: 22600 , cost :  0.42506 , training accuracy :  0.824916\n",
      "i: 22700 , cost :  0.425061 , training accuracy :  0.824916\n",
      "i: 22800 , cost :  0.42506 , training accuracy :  0.824916\n",
      "i: 22900 , cost :  0.425063 , training accuracy :  0.824916\n",
      "i: 23000 , cost :  0.425059 , training accuracy :  0.824916\n",
      "i: 23100 , cost :  0.42506 , training accuracy :  0.824916\n",
      "i: 23200 , cost :  0.42506 , training accuracy :  0.824916\n",
      "i: 23300 , cost :  0.42506 , training accuracy :  0.824916\n",
      "i: 23400 , cost :  0.425064 , training accuracy :  0.824916\n",
      "i: 23500 , cost :  0.425059 , training accuracy :  0.824916\n",
      "i: 23600 , cost :  0.425059 , training accuracy :  0.824916\n",
      "i: 23700 , cost :  0.42506 , training accuracy :  0.824916\n",
      "i: 23800 , cost :  0.425067 , training accuracy :  0.824916\n",
      "i: 23900 , cost :  0.425059 , training accuracy :  0.824916\n",
      "i: 24000 , cost :  0.42506 , training accuracy :  0.824916\n",
      "i: 24100 , cost :  0.425063 , training accuracy :  0.824916\n",
      "i: 24200 , cost :  0.425059 , training accuracy :  0.824916\n",
      "i: 24300 , cost :  0.425059 , training accuracy :  0.824916\n",
      "i: 24400 , cost :  0.425063 , training accuracy :  0.824916\n",
      "i: 24500 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 24600 , cost :  0.425059 , training accuracy :  0.824916\n",
      "i: 24700 , cost :  0.425061 , training accuracy :  0.824916\n",
      "i: 24800 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 24900 , cost :  0.425057 , training accuracy :  0.824916\n",
      "i: 25000 , cost :  0.425059 , training accuracy :  0.824916\n",
      "i: 25100 , cost :  0.425059 , training accuracy :  0.824916\n",
      "i: 25200 , cost :  0.425059 , training accuracy :  0.824916\n",
      "i: 25300 , cost :  0.425067 , training accuracy :  0.824916\n",
      "i: 25400 , cost :  0.425059 , training accuracy :  0.824916\n",
      "i: 25500 , cost :  0.425061 , training accuracy :  0.824916\n",
      "i: 25600 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 25700 , cost :  0.425059 , training accuracy :  0.824916\n",
      "i: 25800 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 25900 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 26000 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 26100 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 26200 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 26300 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 26400 , cost :  0.425066 , training accuracy :  0.824916\n",
      "i: 26500 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 26600 , cost :  0.425062 , training accuracy :  0.824916\n",
      "i: 26700 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 26800 , cost :  0.425066 , training accuracy :  0.824916\n",
      "i: 26900 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 27000 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 27100 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 27200 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 27300 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 27400 , cost :  0.425057 , training accuracy :  0.824916\n",
      "i: 27500 , cost :  0.425059 , training accuracy :  0.824916\n",
      "i: 27600 , cost :  0.425057 , training accuracy :  0.824916\n",
      "i: 27700 , cost :  0.425064 , training accuracy :  0.824916\n",
      "i: 27800 , cost :  0.425057 , training accuracy :  0.824916\n",
      "i: 27900 , cost :  0.425057 , training accuracy :  0.824916\n",
      "i: 28000 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 28100 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 28200 , cost :  0.425057 , training accuracy :  0.824916\n",
      "i: 28300 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 28400 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 28500 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 28600 , cost :  0.425062 , training accuracy :  0.824916\n",
      "i: 28700 , cost :  0.425057 , training accuracy :  0.824916\n",
      "i: 28800 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 28900 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 29000 , cost :  0.425057 , training accuracy :  0.824916\n",
      "i: 29100 , cost :  0.425064 , training accuracy :  0.824916\n",
      "i: 29200 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 29300 , cost :  0.425057 , training accuracy :  0.824916\n",
      "i: 29400 , cost :  0.42506 , training accuracy :  0.824916\n",
      "i: 29500 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 29600 , cost :  0.425057 , training accuracy :  0.824916\n",
      "i: 29700 , cost :  0.425057 , training accuracy :  0.824916\n",
      "i: 29800 , cost :  0.425058 , training accuracy :  0.824916\n",
      "i: 29900 , cost :  0.42506 , training accuracy :  0.824916\n",
      "Final training accuracy :  0.824916\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 30000 ][hidden: 3 ][file: bhavul_tr_acc_0.82_prediction.csv ] ACCURACY :  0.824916\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  44.508 , training accuracy :  0.320988\n",
      "i: 100 , cost :  8.82872 , training accuracy :  0.501683\n",
      "i: 200 , cost :  4.29125 , training accuracy :  0.589226\n",
      "i: 300 , cost :  2.57015 , training accuracy :  0.675645\n",
      "i: 400 , cost :  1.80825 , training accuracy :  0.729517\n",
      "i: 500 , cost :  1.39499 , training accuracy :  0.738496\n",
      "i: 600 , cost :  1.14389 , training accuracy :  0.755331\n",
      "i: 700 , cost :  0.984868 , training accuracy :  0.766554\n",
      "i: 800 , cost :  0.86897 , training accuracy :  0.7789\n",
      "i: 900 , cost :  0.814338 , training accuracy :  0.7789\n",
      "i: 1000 , cost :  0.770004 , training accuracy :  0.782267\n",
      "i: 1100 , cost :  0.732777 , training accuracy :  0.780022\n",
      "i: 1200 , cost :  0.704852 , training accuracy :  0.785634\n",
      "i: 1300 , cost :  0.684697 , training accuracy :  0.780022\n",
      "i: 1400 , cost :  0.664018 , training accuracy :  0.783389\n",
      "i: 1500 , cost :  0.644785 , training accuracy :  0.782267\n",
      "i: 1600 , cost :  0.625712 , training accuracy :  0.781145\n",
      "i: 1700 , cost :  0.607759 , training accuracy :  0.781145\n",
      "i: 1800 , cost :  0.592504 , training accuracy :  0.782267\n",
      "i: 1900 , cost :  0.579446 , training accuracy :  0.782267\n",
      "i: 2000 , cost :  0.566869 , training accuracy :  0.787879\n",
      "i: 2100 , cost :  0.552652 , training accuracy :  0.787879\n",
      "i: 2200 , cost :  0.538508 , training accuracy :  0.789001\n",
      "i: 2300 , cost :  0.52449 , training accuracy :  0.787879\n",
      "i: 2400 , cost :  0.511688 , training accuracy :  0.790123\n",
      "i: 2500 , cost :  0.500107 , training accuracy :  0.790123\n",
      "i: 2600 , cost :  0.489871 , training accuracy :  0.790123\n",
      "i: 2700 , cost :  0.481001 , training accuracy :  0.792368\n",
      "i: 2800 , cost :  0.472815 , training accuracy :  0.79349\n",
      "i: 2900 , cost :  0.466172 , training accuracy :  0.79349\n",
      "i: 3000 , cost :  0.460181 , training accuracy :  0.792368\n",
      "i: 3100 , cost :  0.454896 , training accuracy :  0.790123\n",
      "i: 3200 , cost :  0.450242 , training accuracy :  0.790123\n",
      "i: 3300 , cost :  0.446473 , training accuracy :  0.789001\n",
      "i: 3400 , cost :  0.443389 , training accuracy :  0.790123\n",
      "i: 3500 , cost :  0.440818 , training accuracy :  0.790123\n",
      "i: 3600 , cost :  0.438804 , training accuracy :  0.792368\n",
      "i: 3700 , cost :  0.437315 , training accuracy :  0.79349\n",
      "i: 3800 , cost :  0.436022 , training accuracy :  0.79349\n",
      "i: 3900 , cost :  0.434864 , training accuracy :  0.795735\n",
      "i: 4000 , cost :  0.433804 , training accuracy :  0.795735\n",
      "i: 4100 , cost :  0.432722 , training accuracy :  0.795735\n",
      "i: 4200 , cost :  0.431726 , training accuracy :  0.796857\n",
      "i: 4300 , cost :  0.430843 , training accuracy :  0.796857\n",
      "i: 4400 , cost :  0.430085 , training accuracy :  0.794613\n",
      "i: 4500 , cost :  0.42934 , training accuracy :  0.800224\n",
      "i: 4600 , cost :  0.428693 , training accuracy :  0.79798\n",
      "i: 4700 , cost :  0.428077 , training accuracy :  0.799102\n",
      "i: 4800 , cost :  0.427598 , training accuracy :  0.795735\n",
      "i: 4900 , cost :  0.426941 , training accuracy :  0.795735\n",
      "i: 5000 , cost :  0.42602 , training accuracy :  0.802469\n",
      "i: 5100 , cost :  0.425387 , training accuracy :  0.802469\n",
      "i: 5200 , cost :  0.42476 , training accuracy :  0.804714\n",
      "i: 5300 , cost :  0.424567 , training accuracy :  0.795735\n",
      "i: 5400 , cost :  0.424137 , training accuracy :  0.804714\n",
      "i: 5500 , cost :  0.423461 , training accuracy :  0.801347\n",
      "i: 5600 , cost :  0.42307 , training accuracy :  0.803591\n",
      "i: 5700 , cost :  0.422767 , training accuracy :  0.804714\n",
      "i: 5800 , cost :  0.422408 , training accuracy :  0.804714\n",
      "i: 5900 , cost :  0.422106 , training accuracy :  0.805836\n",
      "i: 6000 , cost :  0.421831 , training accuracy :  0.802469\n",
      "i: 6100 , cost :  0.423631 , training accuracy :  0.799102\n",
      "i: 6200 , cost :  0.423167 , training accuracy :  0.810326\n",
      "i: 6300 , cost :  0.421518 , training accuracy :  0.809203\n",
      "i: 6400 , cost :  0.420117 , training accuracy :  0.802469\n",
      "i: 6500 , cost :  0.419394 , training accuracy :  0.804714\n",
      "i: 6600 , cost :  0.419198 , training accuracy :  0.803591\n",
      "i: 6700 , cost :  0.420099 , training accuracy :  0.809203\n",
      "i: 6800 , cost :  0.418783 , training accuracy :  0.81257\n",
      "i: 6900 , cost :  0.418455 , training accuracy :  0.805836\n",
      "i: 7000 , cost :  0.418252 , training accuracy :  0.805836\n",
      "i: 7100 , cost :  0.41844 , training accuracy :  0.810326\n",
      "i: 7200 , cost :  0.41791 , training accuracy :  0.806958\n",
      "i: 7300 , cost :  0.418239 , training accuracy :  0.802469\n",
      "i: 7400 , cost :  0.417723 , training accuracy :  0.805836\n",
      "i: 7500 , cost :  0.417539 , training accuracy :  0.810326\n",
      "i: 7600 , cost :  0.420007 , training accuracy :  0.810326\n",
      "i: 7700 , cost :  0.417193 , training accuracy :  0.810326\n",
      "i: 7800 , cost :  0.416935 , training accuracy :  0.805836\n",
      "i: 7900 , cost :  0.417032 , training accuracy :  0.808081\n",
      "i: 8000 , cost :  0.420455 , training accuracy :  0.794613\n",
      "i: 8100 , cost :  0.416629 , training accuracy :  0.805836\n",
      "i: 8200 , cost :  0.416525 , training accuracy :  0.806958\n",
      "i: 8300 , cost :  0.416552 , training accuracy :  0.808081\n",
      "i: 8400 , cost :  0.416361 , training accuracy :  0.805836\n",
      "i: 8500 , cost :  0.4163 , training accuracy :  0.806958\n",
      "i: 8600 , cost :  0.416228 , training accuracy :  0.806958\n",
      "i: 8700 , cost :  0.416143 , training accuracy :  0.805836\n",
      "i: 8800 , cost :  0.416084 , training accuracy :  0.804714\n",
      "i: 8900 , cost :  0.417678 , training accuracy :  0.796857\n",
      "i: 9000 , cost :  0.415961 , training accuracy :  0.803591\n",
      "i: 9100 , cost :  0.415905 , training accuracy :  0.804714\n",
      "i: 9200 , cost :  0.41687 , training accuracy :  0.81257\n",
      "i: 9300 , cost :  0.41639 , training accuracy :  0.810326\n",
      "i: 9400 , cost :  0.417301 , training accuracy :  0.81257\n",
      "i: 9500 , cost :  0.416112 , training accuracy :  0.800224\n",
      "i: 9600 , cost :  0.415663 , training accuracy :  0.803591\n",
      "i: 9700 , cost :  0.4157 , training accuracy :  0.803591\n",
      "i: 9800 , cost :  0.416038 , training accuracy :  0.801347\n",
      "i: 9900 , cost :  0.417124 , training accuracy :  0.811448\n",
      "i: 10000 , cost :  0.416012 , training accuracy :  0.801347\n",
      "i: 10100 , cost :  0.415587 , training accuracy :  0.805836\n",
      "i: 10200 , cost :  0.415429 , training accuracy :  0.802469\n",
      "i: 10300 , cost :  0.415363 , training accuracy :  0.804714\n",
      "i: 10400 , cost :  0.415396 , training accuracy :  0.802469\n",
      "i: 10500 , cost :  0.415292 , training accuracy :  0.803591\n",
      "i: 10600 , cost :  0.415957 , training accuracy :  0.802469\n",
      "i: 10700 , cost :  0.41569 , training accuracy :  0.802469\n",
      "i: 10800 , cost :  0.415174 , training accuracy :  0.802469\n",
      "i: 10900 , cost :  0.415318 , training accuracy :  0.806958\n",
      "i: 11000 , cost :  0.415091 , training accuracy :  0.805836\n",
      "i: 11100 , cost :  0.415788 , training accuracy :  0.802469\n",
      "i: 11200 , cost :  0.415057 , training accuracy :  0.805836\n",
      "i: 11300 , cost :  0.415007 , training accuracy :  0.805836\n",
      "i: 11400 , cost :  0.414977 , training accuracy :  0.802469\n",
      "i: 11500 , cost :  0.414902 , training accuracy :  0.803591\n",
      "i: 11600 , cost :  0.414861 , training accuracy :  0.803591\n",
      "i: 11700 , cost :  0.415301 , training accuracy :  0.809203\n",
      "i: 11800 , cost :  0.415281 , training accuracy :  0.802469\n",
      "i: 11900 , cost :  0.416454 , training accuracy :  0.811448\n",
      "i: 12000 , cost :  0.414698 , training accuracy :  0.803591\n",
      "i: 12100 , cost :  0.414663 , training accuracy :  0.806958\n",
      "i: 12200 , cost :  0.414625 , training accuracy :  0.806958\n",
      "i: 12300 , cost :  0.414619 , training accuracy :  0.804714\n",
      "i: 12400 , cost :  0.416213 , training accuracy :  0.811448\n",
      "i: 12500 , cost :  0.414756 , training accuracy :  0.806958\n",
      "i: 12600 , cost :  0.414528 , training accuracy :  0.802469\n",
      "i: 12700 , cost :  0.416041 , training accuracy :  0.811448\n",
      "i: 12800 , cost :  0.415515 , training accuracy :  0.803591\n",
      "i: 12900 , cost :  0.414287 , training accuracy :  0.804714\n",
      "i: 13000 , cost :  0.414182 , training accuracy :  0.806958\n",
      "i: 13100 , cost :  0.414103 , training accuracy :  0.804714\n",
      "i: 13200 , cost :  0.414099 , training accuracy :  0.802469\n",
      "i: 13300 , cost :  0.415089 , training accuracy :  0.810326\n",
      "i: 13400 , cost :  0.414045 , training accuracy :  0.804714\n",
      "i: 13500 , cost :  0.414369 , training accuracy :  0.806958\n",
      "i: 13600 , cost :  0.413364 , training accuracy :  0.806958\n",
      "i: 13700 , cost :  0.413267 , training accuracy :  0.806958\n",
      "i: 13800 , cost :  0.413177 , training accuracy :  0.806958\n",
      "i: 13900 , cost :  0.413088 , training accuracy :  0.805836\n",
      "i: 14000 , cost :  0.41303 , training accuracy :  0.810326\n",
      "i: 14100 , cost :  0.412992 , training accuracy :  0.808081\n",
      "i: 14200 , cost :  0.413236 , training accuracy :  0.806958\n",
      "i: 14300 , cost :  0.412814 , training accuracy :  0.805836\n",
      "i: 14400 , cost :  0.414066 , training accuracy :  0.806958\n",
      "i: 14500 , cost :  0.412884 , training accuracy :  0.804714\n",
      "i: 14600 , cost :  0.412798 , training accuracy :  0.808081\n",
      "i: 14700 , cost :  0.412908 , training accuracy :  0.806958\n",
      "i: 14800 , cost :  0.412577 , training accuracy :  0.806958\n",
      "i: 14900 , cost :  0.412557 , training accuracy :  0.805836\n",
      "i: 15000 , cost :  0.412519 , training accuracy :  0.805836\n",
      "i: 15100 , cost :  0.412516 , training accuracy :  0.805836\n",
      "i: 15200 , cost :  0.41246 , training accuracy :  0.805836\n",
      "i: 15300 , cost :  0.41248 , training accuracy :  0.808081\n",
      "i: 15400 , cost :  0.412409 , training accuracy :  0.805836\n",
      "i: 15500 , cost :  0.412639 , training accuracy :  0.811448\n",
      "i: 15600 , cost :  0.412736 , training accuracy :  0.806958\n",
      "i: 15700 , cost :  0.416973 , training accuracy :  0.808081\n",
      "i: 15800 , cost :  0.41232 , training accuracy :  0.809203\n",
      "i: 15900 , cost :  0.412313 , training accuracy :  0.809203\n",
      "i: 16000 , cost :  0.412284 , training accuracy :  0.808081\n",
      "i: 16100 , cost :  0.412267 , training accuracy :  0.809203\n",
      "i: 16200 , cost :  0.41259 , training accuracy :  0.809203\n",
      "i: 16300 , cost :  0.413278 , training accuracy :  0.810326\n",
      "i: 16400 , cost :  0.412433 , training accuracy :  0.804714\n",
      "i: 16500 , cost :  0.412535 , training accuracy :  0.806958\n",
      "i: 16600 , cost :  0.412234 , training accuracy :  0.805836\n",
      "i: 16700 , cost :  0.412208 , training accuracy :  0.808081\n",
      "i: 16800 , cost :  0.412504 , training accuracy :  0.806958\n",
      "i: 16900 , cost :  0.41217 , training accuracy :  0.806958\n",
      "i: 17000 , cost :  0.412163 , training accuracy :  0.809203\n",
      "i: 17100 , cost :  0.412151 , training accuracy :  0.805836\n",
      "i: 17200 , cost :  0.412442 , training accuracy :  0.809203\n",
      "i: 17300 , cost :  0.412154 , training accuracy :  0.809203\n",
      "i: 17400 , cost :  0.412135 , training accuracy :  0.806958\n",
      "i: 17500 , cost :  0.412274 , training accuracy :  0.804714\n",
      "i: 17600 , cost :  0.41237 , training accuracy :  0.810326\n",
      "i: 17700 , cost :  0.412132 , training accuracy :  0.805836\n",
      "i: 17800 , cost :  0.412633 , training accuracy :  0.805836\n",
      "i: 17900 , cost :  0.414306 , training accuracy :  0.814815\n",
      "i: 18000 , cost :  0.414726 , training accuracy :  0.802469\n",
      "i: 18100 , cost :  0.412074 , training accuracy :  0.808081\n",
      "i: 18200 , cost :  0.412059 , training accuracy :  0.805836\n",
      "i: 18300 , cost :  0.412096 , training accuracy :  0.805836\n",
      "i: 18400 , cost :  0.41218 , training accuracy :  0.804714\n",
      "i: 18500 , cost :  0.412165 , training accuracy :  0.804714\n",
      "i: 18600 , cost :  0.413722 , training accuracy :  0.813693\n",
      "i: 18700 , cost :  0.412942 , training accuracy :  0.806958\n",
      "i: 18800 , cost :  0.413702 , training accuracy :  0.803591\n",
      "i: 18900 , cost :  0.41203 , training accuracy :  0.804714\n",
      "i: 19000 , cost :  0.412003 , training accuracy :  0.808081\n",
      "i: 19100 , cost :  0.412018 , training accuracy :  0.804714\n",
      "i: 19200 , cost :  0.412017 , training accuracy :  0.805836\n",
      "i: 19300 , cost :  0.412009 , training accuracy :  0.805836\n",
      "i: 19400 , cost :  0.41203 , training accuracy :  0.804714\n",
      "i: 19500 , cost :  0.412119 , training accuracy :  0.804714\n",
      "i: 19600 , cost :  0.413124 , training accuracy :  0.806958\n",
      "i: 19700 , cost :  0.412096 , training accuracy :  0.804714\n",
      "i: 19800 , cost :  0.412176 , training accuracy :  0.804714\n",
      "i: 19900 , cost :  0.412265 , training accuracy :  0.805836\n",
      "i: 20000 , cost :  0.412211 , training accuracy :  0.809203\n",
      "i: 20100 , cost :  0.412132 , training accuracy :  0.804714\n",
      "i: 20200 , cost :  0.411956 , training accuracy :  0.804714\n",
      "i: 20300 , cost :  0.412011 , training accuracy :  0.803591\n",
      "i: 20400 , cost :  0.413433 , training accuracy :  0.804714\n",
      "i: 20500 , cost :  0.412078 , training accuracy :  0.803591\n",
      "i: 20600 , cost :  0.411929 , training accuracy :  0.804714\n",
      "i: 20700 , cost :  0.411911 , training accuracy :  0.808081\n",
      "i: 20800 , cost :  0.411936 , training accuracy :  0.808081\n",
      "i: 20900 , cost :  0.412427 , training accuracy :  0.805836\n",
      "i: 21000 , cost :  0.411954 , training accuracy :  0.802469\n",
      "i: 21100 , cost :  0.411888 , training accuracy :  0.806958\n",
      "i: 21200 , cost :  0.411907 , training accuracy :  0.804714\n",
      "i: 21300 , cost :  0.41188 , training accuracy :  0.808081\n",
      "i: 21400 , cost :  0.412065 , training accuracy :  0.808081\n",
      "i: 21500 , cost :  0.41245 , training accuracy :  0.811448\n",
      "i: 21600 , cost :  0.411961 , training accuracy :  0.806958\n",
      "i: 21700 , cost :  0.41195 , training accuracy :  0.802469\n",
      "i: 21800 , cost :  0.411857 , training accuracy :  0.805836\n",
      "i: 21900 , cost :  0.411876 , training accuracy :  0.805836\n",
      "i: 22000 , cost :  0.412218 , training accuracy :  0.806958\n",
      "i: 22100 , cost :  0.411862 , training accuracy :  0.806958\n",
      "i: 22200 , cost :  0.412502 , training accuracy :  0.811448\n",
      "i: 22300 , cost :  0.412357 , training accuracy :  0.811448\n",
      "i: 22400 , cost :  0.41217 , training accuracy :  0.809203\n",
      "i: 22500 , cost :  0.411899 , training accuracy :  0.803591\n",
      "i: 22600 , cost :  0.41182 , training accuracy :  0.804714\n",
      "i: 22700 , cost :  0.41204 , training accuracy :  0.808081\n",
      "i: 22800 , cost :  0.412545 , training accuracy :  0.805836\n",
      "i: 22900 , cost :  0.414094 , training accuracy :  0.801347\n",
      "i: 23000 , cost :  0.412288 , training accuracy :  0.805836\n",
      "i: 23100 , cost :  0.412571 , training accuracy :  0.811448\n",
      "i: 23200 , cost :  0.411821 , training accuracy :  0.804714\n",
      "i: 23300 , cost :  0.412608 , training accuracy :  0.806958\n",
      "i: 23400 , cost :  0.411789 , training accuracy :  0.804714\n",
      "i: 23500 , cost :  0.412078 , training accuracy :  0.809203\n",
      "i: 23600 , cost :  0.41203 , training accuracy :  0.804714\n",
      "i: 23700 , cost :  0.411786 , training accuracy :  0.806958\n",
      "i: 23800 , cost :  0.412151 , training accuracy :  0.809203\n",
      "i: 23900 , cost :  0.411887 , training accuracy :  0.806958\n",
      "i: 24000 , cost :  0.411827 , training accuracy :  0.803591\n",
      "i: 24100 , cost :  0.411802 , training accuracy :  0.805836\n",
      "i: 24200 , cost :  0.41176 , training accuracy :  0.804714\n",
      "i: 24300 , cost :  0.41176 , training accuracy :  0.804714\n",
      "i: 24400 , cost :  0.411758 , training accuracy :  0.808081\n",
      "i: 24500 , cost :  0.411793 , training accuracy :  0.804714\n",
      "i: 24600 , cost :  0.412189 , training accuracy :  0.81257\n",
      "i: 24700 , cost :  0.41175 , training accuracy :  0.808081\n",
      "i: 24800 , cost :  0.411922 , training accuracy :  0.808081\n",
      "i: 24900 , cost :  0.412843 , training accuracy :  0.806958\n",
      "i: 25000 , cost :  0.411767 , training accuracy :  0.805836\n",
      "i: 25100 , cost :  0.412212 , training accuracy :  0.811448\n",
      "i: 25200 , cost :  0.411902 , training accuracy :  0.803591\n",
      "i: 25300 , cost :  0.412182 , training accuracy :  0.81257\n",
      "i: 25400 , cost :  0.411723 , training accuracy :  0.804714\n",
      "i: 25500 , cost :  0.411848 , training accuracy :  0.803591\n",
      "i: 25600 , cost :  0.412805 , training accuracy :  0.806958\n",
      "i: 25700 , cost :  0.411845 , training accuracy :  0.808081\n",
      "i: 25800 , cost :  0.41288 , training accuracy :  0.810326\n",
      "i: 25900 , cost :  0.411716 , training accuracy :  0.805836\n",
      "i: 26000 , cost :  0.412794 , training accuracy :  0.810326\n",
      "i: 26100 , cost :  0.412227 , training accuracy :  0.811448\n",
      "i: 26200 , cost :  0.411897 , training accuracy :  0.803591\n",
      "i: 26300 , cost :  0.411738 , training accuracy :  0.805836\n",
      "i: 26400 , cost :  0.411716 , training accuracy :  0.804714\n",
      "i: 26500 , cost :  0.411675 , training accuracy :  0.804714\n",
      "i: 26600 , cost :  0.411683 , training accuracy :  0.806958\n",
      "i: 26700 , cost :  0.411898 , training accuracy :  0.808081\n",
      "i: 26800 , cost :  0.411838 , training accuracy :  0.803591\n",
      "i: 26900 , cost :  0.411749 , training accuracy :  0.806958\n",
      "i: 27000 , cost :  0.41234 , training accuracy :  0.805836\n",
      "i: 27100 , cost :  0.411838 , training accuracy :  0.804714\n",
      "i: 27200 , cost :  0.411692 , training accuracy :  0.805836\n",
      "i: 27300 , cost :  0.411921 , training accuracy :  0.804714\n",
      "i: 27400 , cost :  0.411656 , training accuracy :  0.808081\n",
      "i: 27500 , cost :  0.412806 , training accuracy :  0.810326\n",
      "i: 27600 , cost :  0.411689 , training accuracy :  0.805836\n",
      "i: 27700 , cost :  0.412092 , training accuracy :  0.811448\n",
      "i: 27800 , cost :  0.411811 , training accuracy :  0.808081\n",
      "i: 27900 , cost :  0.411919 , training accuracy :  0.809203\n",
      "i: 28000 , cost :  0.411696 , training accuracy :  0.803591\n",
      "i: 28100 , cost :  0.411717 , training accuracy :  0.803591\n",
      "i: 28200 , cost :  0.411613 , training accuracy :  0.808081\n",
      "i: 28300 , cost :  0.411612 , training accuracy :  0.808081\n",
      "i: 28400 , cost :  0.412629 , training accuracy :  0.811448\n",
      "i: 28500 , cost :  0.411612 , training accuracy :  0.808081\n",
      "i: 28600 , cost :  0.41163 , training accuracy :  0.806958\n",
      "i: 28700 , cost :  0.4122 , training accuracy :  0.811448\n",
      "i: 28800 , cost :  0.412608 , training accuracy :  0.808081\n",
      "i: 28900 , cost :  0.411615 , training accuracy :  0.806958\n",
      "i: 29000 , cost :  0.41222 , training accuracy :  0.811448\n",
      "i: 29100 , cost :  0.41167 , training accuracy :  0.803591\n",
      "i: 29200 , cost :  0.411658 , training accuracy :  0.806958\n",
      "i: 29300 , cost :  0.412063 , training accuracy :  0.811448\n",
      "i: 29400 , cost :  0.411623 , training accuracy :  0.806958\n",
      "i: 29500 , cost :  0.411605 , training accuracy :  0.806958\n",
      "i: 29600 , cost :  0.411597 , training accuracy :  0.805836\n",
      "i: 29700 , cost :  0.411689 , training accuracy :  0.808081\n",
      "i: 29800 , cost :  0.411892 , training accuracy :  0.809203\n",
      "i: 29900 , cost :  0.411936 , training accuracy :  0.810326\n",
      "Final training accuracy :  0.801347\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 30000 ][hidden: 10 ][file: bhavul_tr_acc_0.80_prediction.csv ] ACCURACY :  0.801347\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  9.94956 , training accuracy :  0.47138\n",
      "i: 100 , cost :  4.21179 , training accuracy :  0.640853\n",
      "i: 200 , cost :  2.80901 , training accuracy :  0.639731\n",
      "i: 300 , cost :  1.85436 , training accuracy :  0.661055\n",
      "i: 400 , cost :  1.32236 , training accuracy :  0.676768\n",
      "i: 500 , cost :  1.12095 , training accuracy :  0.680135\n",
      "i: 600 , cost :  0.991713 , training accuracy :  0.682379\n",
      "i: 700 , cost :  0.893141 , training accuracy :  0.681257\n",
      "i: 800 , cost :  0.827927 , training accuracy :  0.685746\n",
      "i: 900 , cost :  0.786317 , training accuracy :  0.687991\n",
      "i: 1000 , cost :  0.754464 , training accuracy :  0.690236\n",
      "i: 1100 , cost :  0.726971 , training accuracy :  0.691358\n",
      "i: 1200 , cost :  0.703995 , training accuracy :  0.705948\n",
      "i: 1300 , cost :  0.684268 , training accuracy :  0.708193\n",
      "i: 1400 , cost :  0.666066 , training accuracy :  0.708193\n",
      "i: 1500 , cost :  0.649762 , training accuracy :  0.700337\n",
      "i: 1600 , cost :  0.633963 , training accuracy :  0.702581\n",
      "i: 1700 , cost :  0.618858 , training accuracy :  0.701459\n",
      "i: 1800 , cost :  0.606582 , training accuracy :  0.702581\n",
      "i: 1900 , cost :  0.597909 , training accuracy :  0.703704\n",
      "i: 2000 , cost :  0.591891 , training accuracy :  0.703704\n",
      "i: 2100 , cost :  0.586948 , training accuracy :  0.705948\n",
      "i: 2200 , cost :  0.580939 , training accuracy :  0.708193\n",
      "i: 2300 , cost :  0.575502 , training accuracy :  0.708193\n",
      "i: 2400 , cost :  0.571517 , training accuracy :  0.709315\n",
      "i: 2500 , cost :  0.568226 , training accuracy :  0.71156\n",
      "i: 2600 , cost :  0.565261 , training accuracy :  0.714927\n",
      "i: 2700 , cost :  0.56249 , training accuracy :  0.720539\n",
      "i: 2800 , cost :  0.559884 , training accuracy :  0.725028\n",
      "i: 2900 , cost :  0.557626 , training accuracy :  0.727273\n",
      "i: 3000 , cost :  0.555234 , training accuracy :  0.723906\n",
      "i: 3100 , cost :  0.552925 , training accuracy :  0.723906\n",
      "i: 3200 , cost :  0.550889 , training accuracy :  0.718294\n",
      "i: 3300 , cost :  0.549341 , training accuracy :  0.719416\n",
      "i: 3400 , cost :  0.547925 , training accuracy :  0.720539\n",
      "i: 3500 , cost :  0.54661 , training accuracy :  0.719416\n",
      "i: 3600 , cost :  0.545042 , training accuracy :  0.719416\n",
      "i: 3700 , cost :  0.542742 , training accuracy :  0.72615\n",
      "i: 3800 , cost :  0.541184 , training accuracy :  0.727273\n",
      "i: 3900 , cost :  0.539673 , training accuracy :  0.731762\n",
      "i: 4000 , cost :  0.538026 , training accuracy :  0.734007\n",
      "i: 4100 , cost :  0.536233 , training accuracy :  0.734007\n",
      "i: 4200 , cost :  0.534169 , training accuracy :  0.735129\n",
      "i: 4300 , cost :  0.531591 , training accuracy :  0.736251\n",
      "i: 4400 , cost :  0.528643 , training accuracy :  0.737374\n",
      "i: 4500 , cost :  0.525826 , training accuracy :  0.740741\n",
      "i: 4600 , cost :  0.523305 , training accuracy :  0.740741\n",
      "i: 4700 , cost :  0.520475 , training accuracy :  0.742985\n",
      "i: 4800 , cost :  0.518227 , training accuracy :  0.746352\n",
      "i: 4900 , cost :  0.516296 , training accuracy :  0.746352\n",
      "i: 5000 , cost :  0.514097 , training accuracy :  0.749719\n",
      "i: 5100 , cost :  0.511934 , training accuracy :  0.747475\n",
      "i: 5200 , cost :  0.509519 , training accuracy :  0.751964\n",
      "i: 5300 , cost :  0.506805 , training accuracy :  0.760943\n",
      "i: 5400 , cost :  0.503655 , training accuracy :  0.768799\n",
      "i: 5500 , cost :  0.499546 , training accuracy :  0.769921\n",
      "i: 5600 , cost :  0.493978 , training accuracy :  0.777778\n",
      "i: 5700 , cost :  0.485988 , training accuracy :  0.776655\n",
      "i: 5800 , cost :  0.476652 , training accuracy :  0.783389\n",
      "i: 5900 , cost :  0.467916 , training accuracy :  0.7789\n",
      "i: 6000 , cost :  0.460228 , training accuracy :  0.781145\n",
      "i: 6100 , cost :  0.452555 , training accuracy :  0.776655\n",
      "i: 6200 , cost :  0.446238 , training accuracy :  0.783389\n",
      "i: 6300 , cost :  0.440139 , training accuracy :  0.79349\n",
      "i: 6400 , cost :  0.433555 , training accuracy :  0.804714\n",
      "i: 6500 , cost :  0.427585 , training accuracy :  0.806958\n",
      "i: 6600 , cost :  0.422853 , training accuracy :  0.811448\n",
      "i: 6700 , cost :  0.418822 , training accuracy :  0.813693\n",
      "i: 6800 , cost :  0.415543 , training accuracy :  0.814815\n",
      "i: 6900 , cost :  0.412479 , training accuracy :  0.815937\n",
      "i: 7000 , cost :  0.40997 , training accuracy :  0.820426\n",
      "i: 7100 , cost :  0.407828 , training accuracy :  0.821549\n",
      "i: 7200 , cost :  0.406035 , training accuracy :  0.823793\n",
      "i: 7300 , cost :  0.404651 , training accuracy :  0.824916\n",
      "i: 7400 , cost :  0.40335 , training accuracy :  0.82716\n",
      "i: 7500 , cost :  0.402332 , training accuracy :  0.824916\n",
      "i: 7600 , cost :  0.398359 , training accuracy :  0.828283\n",
      "i: 7700 , cost :  0.396664 , training accuracy :  0.829405\n",
      "i: 7800 , cost :  0.39494 , training accuracy :  0.82716\n",
      "i: 7900 , cost :  0.393616 , training accuracy :  0.826038\n",
      "i: 8000 , cost :  0.392697 , training accuracy :  0.824916\n",
      "i: 8100 , cost :  0.391816 , training accuracy :  0.82716\n",
      "i: 8200 , cost :  0.389877 , training accuracy :  0.830527\n",
      "i: 8300 , cost :  0.388299 , training accuracy :  0.83165\n",
      "i: 8400 , cost :  0.386986 , training accuracy :  0.83165\n",
      "i: 8500 , cost :  0.385616 , training accuracy :  0.830527\n",
      "i: 8600 , cost :  0.38382 , training accuracy :  0.829405\n",
      "i: 8700 , cost :  0.38057 , training accuracy :  0.83165\n",
      "i: 8800 , cost :  0.379259 , training accuracy :  0.835017\n",
      "i: 8900 , cost :  0.378718 , training accuracy :  0.832772\n",
      "i: 9000 , cost :  0.378138 , training accuracy :  0.832772\n",
      "i: 9100 , cost :  0.377648 , training accuracy :  0.835017\n",
      "i: 9200 , cost :  0.377338 , training accuracy :  0.837261\n",
      "i: 9300 , cost :  0.377049 , training accuracy :  0.837261\n",
      "i: 9400 , cost :  0.376691 , training accuracy :  0.836139\n",
      "i: 9500 , cost :  0.376428 , training accuracy :  0.836139\n",
      "i: 9600 , cost :  0.376177 , training accuracy :  0.837261\n",
      "i: 9700 , cost :  0.376025 , training accuracy :  0.836139\n",
      "i: 9800 , cost :  0.375795 , training accuracy :  0.837261\n",
      "i: 9900 , cost :  0.375574 , training accuracy :  0.837261\n",
      "i: 10000 , cost :  0.375309 , training accuracy :  0.837261\n",
      "i: 10100 , cost :  0.375206 , training accuracy :  0.837261\n",
      "i: 10200 , cost :  0.374954 , training accuracy :  0.839506\n",
      "i: 10300 , cost :  0.37475 , training accuracy :  0.840629\n",
      "i: 10400 , cost :  0.374635 , training accuracy :  0.839506\n",
      "i: 10500 , cost :  0.374454 , training accuracy :  0.839506\n",
      "i: 10600 , cost :  0.374345 , training accuracy :  0.841751\n",
      "i: 10700 , cost :  0.373491 , training accuracy :  0.836139\n",
      "i: 10800 , cost :  0.373288 , training accuracy :  0.837261\n",
      "i: 10900 , cost :  0.373191 , training accuracy :  0.839506\n",
      "i: 11000 , cost :  0.372892 , training accuracy :  0.837261\n",
      "i: 11100 , cost :  0.372802 , training accuracy :  0.840629\n",
      "i: 11200 , cost :  0.372603 , training accuracy :  0.837261\n",
      "i: 11300 , cost :  0.372537 , training accuracy :  0.840629\n",
      "i: 11400 , cost :  0.372344 , training accuracy :  0.837261\n",
      "i: 11500 , cost :  0.37221 , training accuracy :  0.838384\n",
      "i: 11600 , cost :  0.372226 , training accuracy :  0.837261\n",
      "i: 11700 , cost :  0.372055 , training accuracy :  0.836139\n",
      "i: 11800 , cost :  0.371909 , training accuracy :  0.837261\n",
      "i: 11900 , cost :  0.371699 , training accuracy :  0.839506\n",
      "i: 12000 , cost :  0.371609 , training accuracy :  0.838384\n",
      "i: 12100 , cost :  0.371371 , training accuracy :  0.840629\n",
      "i: 12200 , cost :  0.371296 , training accuracy :  0.839506\n",
      "i: 12300 , cost :  0.371107 , training accuracy :  0.840629\n",
      "i: 12400 , cost :  0.371023 , training accuracy :  0.840629\n",
      "i: 12500 , cost :  0.370999 , training accuracy :  0.838384\n",
      "i: 12600 , cost :  0.370861 , training accuracy :  0.838384\n",
      "i: 12700 , cost :  0.370738 , training accuracy :  0.842873\n",
      "i: 12800 , cost :  0.370765 , training accuracy :  0.841751\n",
      "i: 12900 , cost :  0.370552 , training accuracy :  0.841751\n",
      "i: 13000 , cost :  0.370491 , training accuracy :  0.840629\n",
      "i: 13100 , cost :  0.370554 , training accuracy :  0.843996\n",
      "i: 13200 , cost :  0.370295 , training accuracy :  0.840629\n",
      "i: 13300 , cost :  0.37034 , training accuracy :  0.843996\n",
      "i: 13400 , cost :  0.370112 , training accuracy :  0.839506\n",
      "i: 13500 , cost :  0.370072 , training accuracy :  0.841751\n",
      "i: 13600 , cost :  0.369956 , training accuracy :  0.841751\n",
      "i: 13700 , cost :  0.369949 , training accuracy :  0.839506\n",
      "i: 13800 , cost :  0.369848 , training accuracy :  0.842873\n",
      "i: 13900 , cost :  0.369657 , training accuracy :  0.840629\n",
      "i: 14000 , cost :  0.369568 , training accuracy :  0.839506\n",
      "i: 14100 , cost :  0.36942 , training accuracy :  0.839506\n",
      "i: 14200 , cost :  0.369324 , training accuracy :  0.842873\n",
      "i: 14300 , cost :  0.369266 , training accuracy :  0.842873\n",
      "i: 14400 , cost :  0.369031 , training accuracy :  0.840629\n",
      "i: 14500 , cost :  0.368891 , training accuracy :  0.840629\n",
      "i: 14600 , cost :  0.368924 , training accuracy :  0.841751\n",
      "i: 14700 , cost :  0.368615 , training accuracy :  0.839506\n",
      "i: 14800 , cost :  0.368619 , training accuracy :  0.841751\n",
      "i: 14900 , cost :  0.368418 , training accuracy :  0.840629\n",
      "i: 15000 , cost :  0.368234 , training accuracy :  0.840629\n",
      "i: 15100 , cost :  0.36815 , training accuracy :  0.840629\n",
      "i: 15200 , cost :  0.367997 , training accuracy :  0.842873\n",
      "i: 15300 , cost :  0.367878 , training accuracy :  0.839506\n",
      "i: 15400 , cost :  0.367832 , training accuracy :  0.838384\n",
      "i: 15500 , cost :  0.367736 , training accuracy :  0.841751\n",
      "i: 15600 , cost :  0.367581 , training accuracy :  0.842873\n",
      "i: 15700 , cost :  0.367468 , training accuracy :  0.843996\n",
      "i: 15800 , cost :  0.367382 , training accuracy :  0.842873\n",
      "i: 15900 , cost :  0.367274 , training accuracy :  0.845118\n",
      "i: 16000 , cost :  0.367106 , training accuracy :  0.843996\n",
      "i: 16100 , cost :  0.367044 , training accuracy :  0.845118\n",
      "i: 16200 , cost :  0.366926 , training accuracy :  0.842873\n",
      "i: 16300 , cost :  0.366821 , training accuracy :  0.841751\n",
      "i: 16400 , cost :  0.366534 , training accuracy :  0.845118\n",
      "i: 16500 , cost :  0.366362 , training accuracy :  0.84624\n",
      "i: 16600 , cost :  0.366267 , training accuracy :  0.848485\n",
      "i: 16700 , cost :  0.366136 , training accuracy :  0.849607\n",
      "i: 16800 , cost :  0.365968 , training accuracy :  0.85073\n",
      "i: 16900 , cost :  0.365979 , training accuracy :  0.85073\n",
      "i: 17000 , cost :  0.365846 , training accuracy :  0.848485\n",
      "i: 17100 , cost :  0.365597 , training accuracy :  0.845118\n",
      "i: 17200 , cost :  0.365561 , training accuracy :  0.845118\n",
      "i: 17300 , cost :  0.365422 , training accuracy :  0.847363\n",
      "i: 17400 , cost :  0.365434 , training accuracy :  0.847363\n",
      "i: 17500 , cost :  0.36526 , training accuracy :  0.847363\n",
      "i: 17600 , cost :  0.364999 , training accuracy :  0.847363\n",
      "i: 17700 , cost :  0.364528 , training accuracy :  0.845118\n",
      "i: 17800 , cost :  0.364136 , training accuracy :  0.843996\n",
      "i: 17900 , cost :  0.3639 , training accuracy :  0.842873\n",
      "i: 18000 , cost :  0.363732 , training accuracy :  0.845118\n",
      "i: 18100 , cost :  0.36368 , training accuracy :  0.842873\n",
      "i: 18200 , cost :  0.363576 , training accuracy :  0.843996\n",
      "i: 18300 , cost :  0.36351 , training accuracy :  0.843996\n",
      "i: 18400 , cost :  0.36353 , training accuracy :  0.842873\n",
      "i: 18500 , cost :  0.363398 , training accuracy :  0.842873\n",
      "i: 18600 , cost :  0.363419 , training accuracy :  0.842873\n",
      "i: 18700 , cost :  0.363272 , training accuracy :  0.84624\n",
      "i: 18800 , cost :  0.363227 , training accuracy :  0.847363\n",
      "i: 18900 , cost :  0.363225 , training accuracy :  0.84624\n",
      "i: 19000 , cost :  0.363135 , training accuracy :  0.843996\n",
      "i: 19100 , cost :  0.363084 , training accuracy :  0.847363\n",
      "i: 19200 , cost :  0.363066 , training accuracy :  0.845118\n",
      "i: 19300 , cost :  0.363092 , training accuracy :  0.849607\n",
      "i: 19400 , cost :  0.362982 , training accuracy :  0.84624\n",
      "i: 19500 , cost :  0.362956 , training accuracy :  0.845118\n",
      "i: 19600 , cost :  0.362871 , training accuracy :  0.84624\n",
      "i: 19700 , cost :  0.362782 , training accuracy :  0.847363\n",
      "i: 19800 , cost :  0.362859 , training accuracy :  0.847363\n",
      "i: 19900 , cost :  0.362707 , training accuracy :  0.84624\n",
      "i: 20000 , cost :  0.3627 , training accuracy :  0.84624\n",
      "i: 20100 , cost :  0.362624 , training accuracy :  0.84624\n",
      "i: 20200 , cost :  0.362612 , training accuracy :  0.848485\n",
      "i: 20300 , cost :  0.362534 , training accuracy :  0.84624\n",
      "i: 20400 , cost :  0.362525 , training accuracy :  0.845118\n",
      "i: 20500 , cost :  0.362522 , training accuracy :  0.845118\n",
      "i: 20600 , cost :  0.362513 , training accuracy :  0.849607\n",
      "i: 20700 , cost :  0.362415 , training accuracy :  0.845118\n",
      "i: 20800 , cost :  0.362441 , training accuracy :  0.845118\n",
      "i: 20900 , cost :  0.362407 , training accuracy :  0.848485\n",
      "i: 21000 , cost :  0.362307 , training accuracy :  0.847363\n",
      "i: 21100 , cost :  0.362328 , training accuracy :  0.847363\n",
      "i: 21200 , cost :  0.362352 , training accuracy :  0.84624\n",
      "i: 21300 , cost :  0.36228 , training accuracy :  0.849607\n",
      "i: 21400 , cost :  0.362119 , training accuracy :  0.84624\n",
      "i: 21500 , cost :  0.362181 , training accuracy :  0.845118\n",
      "i: 21600 , cost :  0.362078 , training accuracy :  0.84624\n",
      "i: 21700 , cost :  0.362091 , training accuracy :  0.845118\n",
      "i: 21800 , cost :  0.362026 , training accuracy :  0.847363\n",
      "i: 21900 , cost :  0.361995 , training accuracy :  0.84624\n",
      "i: 22000 , cost :  0.361994 , training accuracy :  0.84624\n",
      "i: 22100 , cost :  0.361996 , training accuracy :  0.845118\n",
      "i: 22200 , cost :  0.361974 , training accuracy :  0.847363\n",
      "i: 22300 , cost :  0.361936 , training accuracy :  0.848485\n",
      "i: 22400 , cost :  0.361923 , training accuracy :  0.847363\n",
      "i: 22500 , cost :  0.361896 , training accuracy :  0.851852\n",
      "i: 22600 , cost :  0.361848 , training accuracy :  0.845118\n",
      "i: 22700 , cost :  0.361824 , training accuracy :  0.851852\n",
      "i: 22800 , cost :  0.361806 , training accuracy :  0.849607\n",
      "i: 22900 , cost :  0.361751 , training accuracy :  0.851852\n",
      "i: 23000 , cost :  0.36183 , training accuracy :  0.848485\n",
      "i: 23100 , cost :  0.361706 , training accuracy :  0.85073\n",
      "i: 23200 , cost :  0.361688 , training accuracy :  0.85073\n",
      "i: 23300 , cost :  0.361696 , training accuracy :  0.85073\n",
      "i: 23400 , cost :  0.361634 , training accuracy :  0.849607\n",
      "i: 23500 , cost :  0.361718 , training accuracy :  0.854097\n",
      "i: 23600 , cost :  0.361694 , training accuracy :  0.849607\n",
      "i: 23700 , cost :  0.361629 , training accuracy :  0.851852\n",
      "i: 23800 , cost :  0.361608 , training accuracy :  0.85073\n",
      "i: 23900 , cost :  0.361752 , training accuracy :  0.849607\n",
      "i: 24000 , cost :  0.361582 , training accuracy :  0.85073\n",
      "i: 24100 , cost :  0.361582 , training accuracy :  0.851852\n",
      "i: 24200 , cost :  0.361657 , training accuracy :  0.851852\n",
      "i: 24300 , cost :  0.361574 , training accuracy :  0.849607\n",
      "i: 24400 , cost :  0.361573 , training accuracy :  0.849607\n",
      "i: 24500 , cost :  0.361538 , training accuracy :  0.85073\n",
      "i: 24600 , cost :  0.361511 , training accuracy :  0.85073\n",
      "i: 24700 , cost :  0.361479 , training accuracy :  0.85073\n",
      "i: 24800 , cost :  0.361584 , training accuracy :  0.851852\n",
      "i: 24900 , cost :  0.361517 , training accuracy :  0.85073\n",
      "i: 25000 , cost :  0.361559 , training accuracy :  0.848485\n",
      "i: 25100 , cost :  0.36159 , training accuracy :  0.847363\n",
      "i: 25200 , cost :  0.361476 , training accuracy :  0.85073\n",
      "i: 25300 , cost :  0.361444 , training accuracy :  0.85073\n",
      "i: 25400 , cost :  0.361422 , training accuracy :  0.851852\n",
      "i: 25500 , cost :  0.361602 , training accuracy :  0.852974\n",
      "i: 25600 , cost :  0.361435 , training accuracy :  0.852974\n",
      "i: 25700 , cost :  0.361446 , training accuracy :  0.85073\n",
      "i: 25800 , cost :  0.361449 , training accuracy :  0.851852\n",
      "i: 25900 , cost :  0.361515 , training accuracy :  0.85073\n",
      "i: 26000 , cost :  0.361406 , training accuracy :  0.852974\n",
      "i: 26100 , cost :  0.361351 , training accuracy :  0.852974\n",
      "i: 26200 , cost :  0.361309 , training accuracy :  0.852974\n",
      "i: 26300 , cost :  0.361319 , training accuracy :  0.852974\n",
      "i: 26400 , cost :  0.361369 , training accuracy :  0.85073\n",
      "i: 26500 , cost :  0.361379 , training accuracy :  0.854097\n",
      "i: 26600 , cost :  0.361309 , training accuracy :  0.852974\n",
      "i: 26700 , cost :  0.361343 , training accuracy :  0.855219\n",
      "i: 26800 , cost :  0.361295 , training accuracy :  0.852974\n",
      "i: 26900 , cost :  0.361263 , training accuracy :  0.852974\n",
      "i: 27000 , cost :  0.361287 , training accuracy :  0.851852\n",
      "i: 27100 , cost :  0.36127 , training accuracy :  0.851852\n",
      "i: 27200 , cost :  0.361265 , training accuracy :  0.852974\n",
      "i: 27300 , cost :  0.36128 , training accuracy :  0.855219\n",
      "i: 27400 , cost :  0.361229 , training accuracy :  0.852974\n",
      "i: 27500 , cost :  0.361214 , training accuracy :  0.852974\n",
      "i: 27600 , cost :  0.361312 , training accuracy :  0.85073\n",
      "i: 27700 , cost :  0.361201 , training accuracy :  0.852974\n",
      "i: 27800 , cost :  0.361181 , training accuracy :  0.852974\n",
      "i: 27900 , cost :  0.361219 , training accuracy :  0.851852\n",
      "i: 28000 , cost :  0.361157 , training accuracy :  0.852974\n",
      "i: 28100 , cost :  0.361125 , training accuracy :  0.852974\n",
      "i: 28200 , cost :  0.361123 , training accuracy :  0.852974\n",
      "i: 28300 , cost :  0.36108 , training accuracy :  0.852974\n",
      "i: 28400 , cost :  0.361101 , training accuracy :  0.855219\n",
      "i: 28500 , cost :  0.361136 , training accuracy :  0.855219\n",
      "i: 28600 , cost :  0.361151 , training accuracy :  0.851852\n",
      "i: 28700 , cost :  0.361122 , training accuracy :  0.856341\n",
      "i: 28800 , cost :  0.361064 , training accuracy :  0.854097\n",
      "i: 28900 , cost :  0.361045 , training accuracy :  0.854097\n",
      "i: 29000 , cost :  0.361034 , training accuracy :  0.854097\n",
      "i: 29100 , cost :  0.361202 , training accuracy :  0.855219\n",
      "i: 29200 , cost :  0.361002 , training accuracy :  0.852974\n",
      "i: 29300 , cost :  0.361077 , training accuracy :  0.85073\n",
      "i: 29400 , cost :  0.361027 , training accuracy :  0.854097\n",
      "i: 29500 , cost :  0.361109 , training accuracy :  0.855219\n",
      "i: 29600 , cost :  0.361003 , training accuracy :  0.852974\n",
      "i: 29700 , cost :  0.361005 , training accuracy :  0.854097\n",
      "i: 29800 , cost :  0.361028 , training accuracy :  0.852974\n",
      "i: 29900 , cost :  0.360934 , training accuracy :  0.855219\n",
      "Final training accuracy :  0.855219\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 30000 ][hidden: 15 ][file: bhavul_tr_acc_0.86_prediction.csv ] ACCURACY :  0.855219\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  805.013 , training accuracy :  0.616162\n",
      "i: 100 , cost :  131.619 , training accuracy :  0.615039\n",
      "i: 200 , cost :  21.597 , training accuracy :  0.545455\n",
      "i: 300 , cost :  12.1341 , training accuracy :  0.604938\n",
      "i: 400 , cost :  9.1515 , training accuracy :  0.626263\n",
      "i: 500 , cost :  7.03332 , training accuracy :  0.631874\n",
      "i: 600 , cost :  5.65633 , training accuracy :  0.631874\n",
      "i: 700 , cost :  4.47305 , training accuracy :  0.67789\n",
      "i: 800 , cost :  3.72224 , training accuracy :  0.698092\n",
      "i: 900 , cost :  3.19966 , training accuracy :  0.72615\n",
      "i: 1000 , cost :  2.80808 , training accuracy :  0.742985\n",
      "i: 1100 , cost :  2.47378 , training accuracy :  0.753086\n",
      "i: 1200 , cost :  2.17261 , training accuracy :  0.766554\n",
      "i: 1300 , cost :  1.89187 , training accuracy :  0.765432\n",
      "i: 1400 , cost :  1.68808 , training accuracy :  0.769921\n",
      "i: 1500 , cost :  1.53368 , training accuracy :  0.776655\n",
      "i: 1600 , cost :  1.40482 , training accuracy :  0.775533\n",
      "i: 1700 , cost :  1.30134 , training accuracy :  0.776655\n",
      "i: 1800 , cost :  1.23525 , training accuracy :  0.787879\n",
      "i: 1900 , cost :  1.1595 , training accuracy :  0.776655\n",
      "i: 2000 , cost :  1.1077 , training accuracy :  0.790123\n",
      "i: 2100 , cost :  1.08097 , training accuracy :  0.769921\n",
      "i: 2200 , cost :  1.06455 , training accuracy :  0.777778\n",
      "i: 2300 , cost :  1.02285 , training accuracy :  0.772166\n",
      "i: 2400 , cost :  0.96792 , training accuracy :  0.780022\n",
      "i: 2500 , cost :  0.935354 , training accuracy :  0.790123\n",
      "i: 2600 , cost :  0.919174 , training accuracy :  0.7789\n",
      "i: 2700 , cost :  0.888226 , training accuracy :  0.782267\n",
      "i: 2800 , cost :  0.867484 , training accuracy :  0.786756\n",
      "i: 2900 , cost :  0.880623 , training accuracy :  0.773288\n",
      "i: 3000 , cost :  0.883851 , training accuracy :  0.785634\n",
      "i: 3100 , cost :  0.808603 , training accuracy :  0.796857\n",
      "i: 3200 , cost :  0.795743 , training accuracy :  0.792368\n",
      "i: 3300 , cost :  0.798502 , training accuracy :  0.803591\n",
      "i: 3400 , cost :  0.76261 , training accuracy :  0.811448\n",
      "i: 3500 , cost :  0.752427 , training accuracy :  0.810326\n",
      "i: 3600 , cost :  0.739516 , training accuracy :  0.808081\n",
      "i: 3700 , cost :  0.728598 , training accuracy :  0.811448\n",
      "i: 3800 , cost :  0.715733 , training accuracy :  0.81257\n",
      "i: 3900 , cost :  0.70361 , training accuracy :  0.811448\n",
      "i: 4000 , cost :  0.68938 , training accuracy :  0.806958\n",
      "i: 4100 , cost :  0.669942 , training accuracy :  0.81257\n",
      "i: 4200 , cost :  0.655988 , training accuracy :  0.813693\n",
      "i: 4300 , cost :  0.639155 , training accuracy :  0.81257\n",
      "i: 4400 , cost :  0.617233 , training accuracy :  0.821549\n",
      "i: 4500 , cost :  0.604037 , training accuracy :  0.824916\n",
      "i: 4600 , cost :  0.59683 , training accuracy :  0.82716\n",
      "i: 4700 , cost :  0.596951 , training accuracy :  0.814815\n",
      "i: 4800 , cost :  0.60049 , training accuracy :  0.814815\n",
      "i: 4900 , cost :  0.599346 , training accuracy :  0.81257\n",
      "i: 5000 , cost :  0.594218 , training accuracy :  0.811448\n",
      "i: 5100 , cost :  0.583727 , training accuracy :  0.813693\n",
      "i: 5200 , cost :  0.569208 , training accuracy :  0.815937\n",
      "i: 5300 , cost :  0.553213 , training accuracy :  0.821549\n",
      "i: 5400 , cost :  0.545666 , training accuracy :  0.823793\n",
      "i: 5500 , cost :  0.537803 , training accuracy :  0.821549\n",
      "i: 5600 , cost :  0.532858 , training accuracy :  0.82716\n",
      "i: 5700 , cost :  0.522728 , training accuracy :  0.823793\n",
      "i: 5800 , cost :  0.515008 , training accuracy :  0.822671\n",
      "i: 5900 , cost :  0.508439 , training accuracy :  0.822671\n",
      "i: 6000 , cost :  0.50585 , training accuracy :  0.824916\n",
      "i: 6100 , cost :  0.496426 , training accuracy :  0.826038\n",
      "i: 6200 , cost :  0.484145 , training accuracy :  0.830527\n",
      "i: 6300 , cost :  0.476805 , training accuracy :  0.83165\n",
      "i: 6400 , cost :  0.471425 , training accuracy :  0.83165\n",
      "i: 6500 , cost :  0.466061 , training accuracy :  0.833894\n",
      "i: 6600 , cost :  0.459965 , training accuracy :  0.836139\n",
      "i: 6700 , cost :  0.454759 , training accuracy :  0.836139\n",
      "i: 6800 , cost :  0.450021 , training accuracy :  0.836139\n",
      "i: 6900 , cost :  0.44689 , training accuracy :  0.835017\n",
      "i: 7000 , cost :  0.440701 , training accuracy :  0.836139\n",
      "i: 7100 , cost :  0.437545 , training accuracy :  0.837261\n",
      "i: 7200 , cost :  0.435246 , training accuracy :  0.837261\n",
      "i: 7300 , cost :  0.431454 , training accuracy :  0.840629\n",
      "i: 7400 , cost :  0.425677 , training accuracy :  0.839506\n",
      "i: 7500 , cost :  0.420647 , training accuracy :  0.839506\n",
      "i: 7600 , cost :  0.420204 , training accuracy :  0.843996\n",
      "i: 7700 , cost :  0.416199 , training accuracy :  0.845118\n",
      "i: 7800 , cost :  0.411065 , training accuracy :  0.842873\n",
      "i: 7900 , cost :  0.407253 , training accuracy :  0.84624\n",
      "i: 8000 , cost :  0.40339 , training accuracy :  0.851852\n",
      "i: 8100 , cost :  0.401586 , training accuracy :  0.854097\n",
      "i: 8200 , cost :  0.397276 , training accuracy :  0.855219\n",
      "i: 8300 , cost :  0.393411 , training accuracy :  0.855219\n",
      "i: 8400 , cost :  0.391806 , training accuracy :  0.857464\n",
      "i: 8500 , cost :  0.38484 , training accuracy :  0.854097\n",
      "i: 8600 , cost :  0.382029 , training accuracy :  0.856341\n",
      "i: 8700 , cost :  0.382915 , training accuracy :  0.854097\n",
      "i: 8800 , cost :  0.3851 , training accuracy :  0.860831\n",
      "i: 8900 , cost :  0.382102 , training accuracy :  0.861953\n",
      "i: 9000 , cost :  0.380703 , training accuracy :  0.863075\n",
      "i: 9100 , cost :  0.381345 , training accuracy :  0.863075\n",
      "i: 9200 , cost :  0.37717 , training accuracy :  0.860831\n",
      "i: 9300 , cost :  0.374583 , training accuracy :  0.858586\n",
      "i: 9400 , cost :  0.374502 , training accuracy :  0.860831\n",
      "i: 9500 , cost :  0.371138 , training accuracy :  0.858586\n",
      "i: 9600 , cost :  0.369981 , training accuracy :  0.858586\n",
      "i: 9700 , cost :  0.368676 , training accuracy :  0.858586\n",
      "i: 9800 , cost :  0.370297 , training accuracy :  0.864198\n",
      "i: 9900 , cost :  0.367721 , training accuracy :  0.860831\n",
      "i: 10000 , cost :  0.366708 , training accuracy :  0.860831\n",
      "i: 10100 , cost :  0.364585 , training accuracy :  0.858586\n",
      "i: 10200 , cost :  0.364677 , training accuracy :  0.860831\n",
      "i: 10300 , cost :  0.364079 , training accuracy :  0.860831\n",
      "i: 10400 , cost :  0.364152 , training accuracy :  0.860831\n",
      "i: 10500 , cost :  0.363098 , training accuracy :  0.860831\n",
      "i: 10600 , cost :  0.36283 , training accuracy :  0.861953\n",
      "i: 10700 , cost :  0.360995 , training accuracy :  0.861953\n",
      "i: 10800 , cost :  0.358632 , training accuracy :  0.858586\n",
      "i: 10900 , cost :  0.358089 , training accuracy :  0.858586\n",
      "i: 11000 , cost :  0.357641 , training accuracy :  0.858586\n",
      "i: 11100 , cost :  0.356145 , training accuracy :  0.860831\n",
      "i: 11200 , cost :  0.35378 , training accuracy :  0.860831\n",
      "i: 11300 , cost :  0.352822 , training accuracy :  0.860831\n",
      "i: 11400 , cost :  0.353665 , training accuracy :  0.860831\n",
      "i: 11500 , cost :  0.351866 , training accuracy :  0.860831\n",
      "i: 11600 , cost :  0.353323 , training accuracy :  0.858586\n",
      "i: 11700 , cost :  0.350646 , training accuracy :  0.860831\n",
      "i: 11800 , cost :  0.352654 , training accuracy :  0.859708\n",
      "i: 11900 , cost :  0.355799 , training accuracy :  0.857464\n",
      "i: 12000 , cost :  0.353402 , training accuracy :  0.859708\n",
      "i: 12100 , cost :  0.353499 , training accuracy :  0.858586\n",
      "i: 12200 , cost :  0.349512 , training accuracy :  0.858586\n",
      "i: 12300 , cost :  0.349392 , training accuracy :  0.858586\n",
      "i: 12400 , cost :  0.347691 , training accuracy :  0.860831\n",
      "i: 12500 , cost :  0.347678 , training accuracy :  0.859708\n",
      "i: 12600 , cost :  0.346992 , training accuracy :  0.860831\n",
      "i: 12700 , cost :  0.346293 , training accuracy :  0.860831\n",
      "i: 12800 , cost :  0.34731 , training accuracy :  0.858586\n",
      "i: 12900 , cost :  0.345798 , training accuracy :  0.858586\n",
      "i: 13000 , cost :  0.344544 , training accuracy :  0.861953\n",
      "i: 13100 , cost :  0.341713 , training accuracy :  0.864198\n",
      "i: 13200 , cost :  0.344218 , training accuracy :  0.861953\n",
      "i: 13300 , cost :  0.344452 , training accuracy :  0.858586\n",
      "i: 13400 , cost :  0.343637 , training accuracy :  0.861953\n",
      "i: 13500 , cost :  0.343647 , training accuracy :  0.860831\n",
      "i: 13600 , cost :  0.342661 , training accuracy :  0.861953\n",
      "i: 13700 , cost :  0.342149 , training accuracy :  0.861953\n",
      "i: 13800 , cost :  0.33815 , training accuracy :  0.86532\n",
      "i: 13900 , cost :  0.340717 , training accuracy :  0.863075\n",
      "i: 14000 , cost :  0.33887 , training accuracy :  0.86532\n",
      "i: 14100 , cost :  0.341595 , training accuracy :  0.859708\n",
      "i: 14200 , cost :  0.339811 , training accuracy :  0.864198\n",
      "i: 14300 , cost :  0.344658 , training accuracy :  0.860831\n",
      "i: 14400 , cost :  0.340416 , training accuracy :  0.859708\n",
      "i: 14500 , cost :  0.33842 , training accuracy :  0.864198\n",
      "i: 14600 , cost :  0.340168 , training accuracy :  0.860831\n",
      "i: 14700 , cost :  0.339247 , training accuracy :  0.861953\n",
      "i: 14800 , cost :  0.337872 , training accuracy :  0.86532\n",
      "i: 14900 , cost :  0.341245 , training accuracy :  0.861953\n",
      "i: 15000 , cost :  0.338962 , training accuracy :  0.859708\n",
      "i: 15100 , cost :  0.33771 , training accuracy :  0.863075\n",
      "i: 15200 , cost :  0.337139 , training accuracy :  0.863075\n",
      "i: 15300 , cost :  0.335939 , training accuracy :  0.86532\n",
      "i: 15400 , cost :  0.33798 , training accuracy :  0.860831\n",
      "i: 15500 , cost :  0.333748 , training accuracy :  0.866442\n",
      "i: 15600 , cost :  0.338882 , training accuracy :  0.863075\n",
      "i: 15700 , cost :  0.339502 , training accuracy :  0.861953\n",
      "i: 15800 , cost :  0.335987 , training accuracy :  0.860831\n",
      "i: 15900 , cost :  0.335312 , training accuracy :  0.864198\n",
      "i: 16000 , cost :  0.334405 , training accuracy :  0.86532\n",
      "i: 16100 , cost :  0.331296 , training accuracy :  0.867565\n",
      "i: 16200 , cost :  0.333311 , training accuracy :  0.86532\n",
      "i: 16300 , cost :  0.332693 , training accuracy :  0.86532\n",
      "i: 16400 , cost :  0.332812 , training accuracy :  0.86532\n",
      "i: 16500 , cost :  0.335286 , training accuracy :  0.861953\n",
      "i: 16600 , cost :  0.333777 , training accuracy :  0.860831\n",
      "i: 16700 , cost :  0.332913 , training accuracy :  0.864198\n",
      "i: 16800 , cost :  0.332348 , training accuracy :  0.86532\n",
      "i: 16900 , cost :  0.330517 , training accuracy :  0.866442\n",
      "i: 17000 , cost :  0.333742 , training accuracy :  0.863075\n",
      "i: 17100 , cost :  0.330002 , training accuracy :  0.866442\n",
      "i: 17200 , cost :  0.330164 , training accuracy :  0.866442\n",
      "i: 17300 , cost :  0.332451 , training accuracy :  0.861953\n",
      "i: 17400 , cost :  0.334839 , training accuracy :  0.863075\n",
      "i: 17500 , cost :  0.335084 , training accuracy :  0.86532\n",
      "i: 17600 , cost :  0.331597 , training accuracy :  0.861953\n",
      "i: 17700 , cost :  0.330675 , training accuracy :  0.861953\n",
      "i: 17800 , cost :  0.329245 , training accuracy :  0.86532\n",
      "i: 17900 , cost :  0.330836 , training accuracy :  0.863075\n",
      "i: 18000 , cost :  0.331828 , training accuracy :  0.86532\n",
      "i: 18100 , cost :  0.329995 , training accuracy :  0.863075\n",
      "i: 18200 , cost :  0.330607 , training accuracy :  0.864198\n",
      "i: 18300 , cost :  0.329763 , training accuracy :  0.863075\n",
      "i: 18400 , cost :  0.3265 , training accuracy :  0.868687\n",
      "i: 18500 , cost :  0.328355 , training accuracy :  0.86532\n",
      "i: 18600 , cost :  0.328333 , training accuracy :  0.86532\n",
      "i: 18700 , cost :  0.329385 , training accuracy :  0.864198\n",
      "i: 18800 , cost :  0.328616 , training accuracy :  0.863075\n",
      "i: 18900 , cost :  0.32569 , training accuracy :  0.867565\n",
      "i: 19000 , cost :  0.32854 , training accuracy :  0.863075\n",
      "i: 19100 , cost :  0.326825 , training accuracy :  0.86532\n",
      "i: 19200 , cost :  0.325791 , training accuracy :  0.867565\n",
      "i: 19300 , cost :  0.326283 , training accuracy :  0.86532\n",
      "i: 19400 , cost :  0.323915 , training accuracy :  0.868687\n",
      "i: 19500 , cost :  0.325778 , training accuracy :  0.86532\n",
      "i: 19600 , cost :  0.325831 , training accuracy :  0.86532\n",
      "i: 19700 , cost :  0.326501 , training accuracy :  0.863075\n",
      "i: 19800 , cost :  0.326652 , training accuracy :  0.863075\n",
      "i: 19900 , cost :  0.324373 , training accuracy :  0.86532\n",
      "i: 20000 , cost :  0.324776 , training accuracy :  0.86532\n",
      "i: 20100 , cost :  0.325476 , training accuracy :  0.864198\n",
      "i: 20200 , cost :  0.325389 , training accuracy :  0.864198\n",
      "i: 20300 , cost :  0.32406 , training accuracy :  0.86532\n",
      "i: 20400 , cost :  0.325134 , training accuracy :  0.864198\n",
      "i: 20500 , cost :  0.324766 , training accuracy :  0.864198\n",
      "i: 20600 , cost :  0.323944 , training accuracy :  0.86532\n",
      "i: 20700 , cost :  0.323873 , training accuracy :  0.86532\n",
      "i: 20800 , cost :  0.322777 , training accuracy :  0.86532\n",
      "i: 20900 , cost :  0.32288 , training accuracy :  0.86532\n",
      "i: 21000 , cost :  0.322379 , training accuracy :  0.86532\n",
      "i: 21100 , cost :  0.324267 , training accuracy :  0.864198\n",
      "i: 21200 , cost :  0.322679 , training accuracy :  0.86532\n",
      "i: 21300 , cost :  0.323125 , training accuracy :  0.864198\n",
      "i: 21400 , cost :  0.321975 , training accuracy :  0.866442\n",
      "i: 21500 , cost :  0.32104 , training accuracy :  0.868687\n",
      "i: 21600 , cost :  0.322209 , training accuracy :  0.866442\n",
      "i: 21700 , cost :  0.322782 , training accuracy :  0.864198\n",
      "i: 21800 , cost :  0.322679 , training accuracy :  0.864198\n",
      "i: 21900 , cost :  0.322167 , training accuracy :  0.864198\n",
      "i: 22000 , cost :  0.321944 , training accuracy :  0.866442\n",
      "i: 22100 , cost :  0.321348 , training accuracy :  0.866442\n",
      "i: 22200 , cost :  0.321088 , training accuracy :  0.866442\n",
      "i: 22300 , cost :  0.321855 , training accuracy :  0.866442\n",
      "i: 22400 , cost :  0.32118 , training accuracy :  0.867565\n",
      "i: 22500 , cost :  0.321128 , training accuracy :  0.867565\n",
      "i: 22600 , cost :  0.321663 , training accuracy :  0.866442\n",
      "i: 22700 , cost :  0.320728 , training accuracy :  0.868687\n",
      "i: 22800 , cost :  0.320424 , training accuracy :  0.868687\n",
      "i: 22900 , cost :  0.321031 , training accuracy :  0.867565\n",
      "i: 23000 , cost :  0.320477 , training accuracy :  0.868687\n",
      "i: 23100 , cost :  0.319139 , training accuracy :  0.867565\n",
      "i: 23200 , cost :  0.319571 , training accuracy :  0.868687\n",
      "i: 23300 , cost :  0.320759 , training accuracy :  0.867565\n",
      "i: 23400 , cost :  0.319252 , training accuracy :  0.868687\n",
      "i: 23500 , cost :  0.319153 , training accuracy :  0.868687\n",
      "i: 23600 , cost :  0.317824 , training accuracy :  0.869809\n",
      "i: 23700 , cost :  0.318982 , training accuracy :  0.868687\n",
      "i: 23800 , cost :  0.320071 , training accuracy :  0.867565\n",
      "i: 23900 , cost :  0.319125 , training accuracy :  0.868687\n",
      "i: 24000 , cost :  0.319509 , training accuracy :  0.867565\n",
      "i: 24100 , cost :  0.317795 , training accuracy :  0.868687\n",
      "i: 24200 , cost :  0.319031 , training accuracy :  0.867565\n",
      "i: 24300 , cost :  0.318282 , training accuracy :  0.868687\n",
      "i: 24400 , cost :  0.318239 , training accuracy :  0.868687\n",
      "i: 24500 , cost :  0.318204 , training accuracy :  0.868687\n",
      "i: 24600 , cost :  0.317992 , training accuracy :  0.868687\n",
      "i: 24700 , cost :  0.319233 , training accuracy :  0.867565\n",
      "i: 24800 , cost :  0.319432 , training accuracy :  0.867565\n",
      "i: 24900 , cost :  0.318344 , training accuracy :  0.867565\n",
      "i: 25000 , cost :  0.317089 , training accuracy :  0.868687\n",
      "i: 25100 , cost :  0.316773 , training accuracy :  0.868687\n",
      "i: 25200 , cost :  0.317638 , training accuracy :  0.868687\n",
      "i: 25300 , cost :  0.317302 , training accuracy :  0.868687\n",
      "i: 25400 , cost :  0.31883 , training accuracy :  0.867565\n",
      "i: 25500 , cost :  0.318169 , training accuracy :  0.867565\n",
      "i: 25600 , cost :  0.315694 , training accuracy :  0.868687\n",
      "i: 25700 , cost :  0.316214 , training accuracy :  0.868687\n",
      "i: 25800 , cost :  0.316846 , training accuracy :  0.868687\n",
      "i: 25900 , cost :  0.318629 , training accuracy :  0.866442\n",
      "i: 26000 , cost :  0.317712 , training accuracy :  0.867565\n",
      "i: 26100 , cost :  0.316901 , training accuracy :  0.867565\n",
      "i: 26200 , cost :  0.314456 , training accuracy :  0.872054\n",
      "i: 26300 , cost :  0.315752 , training accuracy :  0.868687\n",
      "i: 26400 , cost :  0.317421 , training accuracy :  0.867565\n",
      "i: 26500 , cost :  0.314996 , training accuracy :  0.869809\n",
      "i: 26600 , cost :  0.315498 , training accuracy :  0.869809\n",
      "i: 26700 , cost :  0.316612 , training accuracy :  0.868687\n",
      "i: 26800 , cost :  0.315572 , training accuracy :  0.869809\n",
      "i: 26900 , cost :  0.315088 , training accuracy :  0.869809\n",
      "i: 27000 , cost :  0.315068 , training accuracy :  0.869809\n",
      "i: 27100 , cost :  0.315402 , training accuracy :  0.869809\n",
      "i: 27200 , cost :  0.315925 , training accuracy :  0.868687\n",
      "i: 27300 , cost :  0.315912 , training accuracy :  0.868687\n",
      "i: 27400 , cost :  0.317526 , training accuracy :  0.867565\n",
      "i: 27500 , cost :  0.315859 , training accuracy :  0.869809\n",
      "i: 27600 , cost :  0.314192 , training accuracy :  0.870932\n",
      "i: 27700 , cost :  0.314662 , training accuracy :  0.870932\n",
      "i: 27800 , cost :  0.314842 , training accuracy :  0.869809\n",
      "i: 27900 , cost :  0.314963 , training accuracy :  0.868687\n",
      "i: 28000 , cost :  0.314575 , training accuracy :  0.869809\n",
      "i: 28100 , cost :  0.314139 , training accuracy :  0.869809\n",
      "i: 28200 , cost :  0.31471 , training accuracy :  0.868687\n",
      "i: 28300 , cost :  0.314416 , training accuracy :  0.869809\n",
      "i: 28400 , cost :  0.313875 , training accuracy :  0.869809\n",
      "i: 28500 , cost :  0.314206 , training accuracy :  0.869809\n",
      "i: 28600 , cost :  0.314356 , training accuracy :  0.869809\n",
      "i: 28700 , cost :  0.314558 , training accuracy :  0.868687\n",
      "i: 28800 , cost :  0.314488 , training accuracy :  0.868687\n",
      "i: 28900 , cost :  0.313793 , training accuracy :  0.870932\n",
      "i: 29000 , cost :  0.312815 , training accuracy :  0.870932\n",
      "i: 29100 , cost :  0.313375 , training accuracy :  0.870932\n",
      "i: 29200 , cost :  0.315094 , training accuracy :  0.868687\n",
      "i: 29300 , cost :  0.312643 , training accuracy :  0.872054\n",
      "i: 29400 , cost :  0.312706 , training accuracy :  0.870932\n",
      "i: 29500 , cost :  0.312726 , training accuracy :  0.872054\n",
      "i: 29600 , cost :  0.312788 , training accuracy :  0.870932\n",
      "i: 29700 , cost :  0.31249 , training accuracy :  0.870932\n",
      "i: 29800 , cost :  0.312387 , training accuracy :  0.872054\n",
      "i: 29900 , cost :  0.310486 , training accuracy :  0.873176\n",
      "Final training accuracy :  0.872054\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 30000 ][hidden: 50 ][file: bhavul_tr_acc_0.87_prediction.csv ] ACCURACY :  0.872054\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1731.65 , training accuracy :  0.383838\n",
      "i: 100 , cost :  93.6058 , training accuracy :  0.649832\n",
      "i: 200 , cost :  45.5946 , training accuracy :  0.620651\n",
      "i: 300 , cost :  18.3721 , training accuracy :  0.637486\n",
      "i: 400 , cost :  7.51188 , training accuracy :  0.67789\n",
      "i: 500 , cost :  4.63312 , training accuracy :  0.668911\n",
      "i: 600 , cost :  3.67018 , training accuracy :  0.701459\n",
      "i: 700 , cost :  3.02052 , training accuracy :  0.712682\n",
      "i: 800 , cost :  2.47338 , training accuracy :  0.738496\n",
      "i: 900 , cost :  2.09952 , training accuracy :  0.756453\n",
      "i: 1000 , cost :  1.83695 , training accuracy :  0.772166\n",
      "i: 1100 , cost :  1.65701 , training accuracy :  0.781145\n",
      "i: 1200 , cost :  1.48659 , training accuracy :  0.783389\n",
      "i: 1300 , cost :  1.34328 , training accuracy :  0.786756\n",
      "i: 1400 , cost :  1.21351 , training accuracy :  0.785634\n",
      "i: 1500 , cost :  1.08867 , training accuracy :  0.799102\n",
      "i: 1600 , cost :  1.00878 , training accuracy :  0.796857\n",
      "i: 1700 , cost :  0.943742 , training accuracy :  0.805836\n",
      "i: 1800 , cost :  0.860074 , training accuracy :  0.810326\n",
      "i: 1900 , cost :  0.809902 , training accuracy :  0.811448\n",
      "i: 2000 , cost :  0.782731 , training accuracy :  0.81257\n",
      "i: 2100 , cost :  0.733968 , training accuracy :  0.828283\n",
      "i: 2200 , cost :  0.74645 , training accuracy :  0.803591\n",
      "i: 2300 , cost :  0.685851 , training accuracy :  0.820426\n",
      "i: 2400 , cost :  0.733557 , training accuracy :  0.819304\n",
      "i: 2500 , cost :  0.625474 , training accuracy :  0.832772\n",
      "i: 2600 , cost :  0.657568 , training accuracy :  0.803591\n",
      "i: 2700 , cost :  0.583041 , training accuracy :  0.835017\n",
      "i: 2800 , cost :  0.585516 , training accuracy :  0.837261\n",
      "i: 2900 , cost :  0.610234 , training accuracy :  0.836139\n",
      "i: 3000 , cost :  0.616054 , training accuracy :  0.826038\n",
      "i: 3100 , cost :  0.601555 , training accuracy :  0.830527\n",
      "i: 3200 , cost :  0.627357 , training accuracy :  0.821549\n",
      "i: 3300 , cost :  0.527995 , training accuracy :  0.839506\n",
      "i: 3400 , cost :  0.488946 , training accuracy :  0.85073\n",
      "i: 3500 , cost :  0.480762 , training accuracy :  0.85073\n",
      "i: 3600 , cost :  0.532165 , training accuracy :  0.820426\n",
      "i: 3700 , cost :  0.481834 , training accuracy :  0.843996\n",
      "i: 3800 , cost :  0.457247 , training accuracy :  0.855219\n",
      "i: 3900 , cost :  0.549914 , training accuracy :  0.842873\n",
      "i: 4000 , cost :  0.484204 , training accuracy :  0.85073\n",
      "i: 4100 , cost :  0.428398 , training accuracy :  0.861953\n",
      "i: 4200 , cost :  0.436006 , training accuracy :  0.859708\n",
      "i: 4300 , cost :  0.463284 , training accuracy :  0.841751\n",
      "i: 4400 , cost :  0.428139 , training accuracy :  0.858586\n",
      "i: 4500 , cost :  0.461623 , training accuracy :  0.854097\n",
      "i: 4600 , cost :  0.503737 , training accuracy :  0.848485\n",
      "i: 4700 , cost :  0.430377 , training accuracy :  0.851852\n",
      "i: 4800 , cost :  0.39636 , training accuracy :  0.872054\n",
      "i: 4900 , cost :  0.411886 , training accuracy :  0.859708\n",
      "i: 5000 , cost :  0.389972 , training accuracy :  0.872054\n",
      "i: 5100 , cost :  0.387582 , training accuracy :  0.870932\n",
      "i: 5200 , cost :  0.384434 , training accuracy :  0.873176\n",
      "i: 5300 , cost :  0.384557 , training accuracy :  0.873176\n",
      "i: 5400 , cost :  0.400288 , training accuracy :  0.863075\n",
      "i: 5500 , cost :  0.387634 , training accuracy :  0.86532\n",
      "i: 5600 , cost :  0.405459 , training accuracy :  0.855219\n",
      "i: 5700 , cost :  0.433581 , training accuracy :  0.835017\n",
      "i: 5800 , cost :  0.417075 , training accuracy :  0.842873\n",
      "i: 5900 , cost :  0.403341 , training accuracy :  0.847363\n",
      "i: 6000 , cost :  0.40331 , training accuracy :  0.84624\n",
      "i: 6100 , cost :  0.403492 , training accuracy :  0.847363\n",
      "i: 6200 , cost :  0.408074 , training accuracy :  0.843996\n",
      "i: 6300 , cost :  0.41783 , training accuracy :  0.838384\n",
      "i: 6400 , cost :  0.410023 , training accuracy :  0.841751\n",
      "i: 6500 , cost :  0.372818 , training accuracy :  0.863075\n",
      "i: 6600 , cost :  0.351427 , training accuracy :  0.873176\n",
      "i: 6700 , cost :  0.341393 , training accuracy :  0.882155\n",
      "i: 6800 , cost :  0.360299 , training accuracy :  0.868687\n",
      "i: 6900 , cost :  0.403722 , training accuracy :  0.866442\n",
      "i: 7000 , cost :  0.432914 , training accuracy :  0.873176\n",
      "i: 7100 , cost :  0.339487 , training accuracy :  0.875421\n",
      "i: 7200 , cost :  0.342308 , training accuracy :  0.870932\n",
      "i: 7300 , cost :  0.391673 , training accuracy :  0.848485\n",
      "i: 7400 , cost :  0.347643 , training accuracy :  0.867565\n",
      "i: 7500 , cost :  0.330727 , training accuracy :  0.876543\n",
      "i: 7600 , cost :  0.416272 , training accuracy :  0.872054\n",
      "i: 7700 , cost :  0.332756 , training accuracy :  0.875421\n",
      "i: 7800 , cost :  0.336723 , training accuracy :  0.872054\n",
      "i: 7900 , cost :  0.37761 , training accuracy :  0.852974\n",
      "i: 8000 , cost :  0.317573 , training accuracy :  0.885522\n",
      "i: 8100 , cost :  0.331535 , training accuracy :  0.876543\n",
      "i: 8200 , cost :  0.422941 , training accuracy :  0.874299\n",
      "i: 8300 , cost :  0.324128 , training accuracy :  0.876543\n",
      "i: 8400 , cost :  0.309829 , training accuracy :  0.883277\n",
      "i: 8500 , cost :  0.340757 , training accuracy :  0.864198\n",
      "i: 8600 , cost :  0.353654 , training accuracy :  0.854097\n",
      "i: 8700 , cost :  0.306069 , training accuracy :  0.8844\n",
      "i: 8800 , cost :  0.321567 , training accuracy :  0.877666\n",
      "i: 8900 , cost :  0.415148 , training accuracy :  0.870932\n",
      "i: 9000 , cost :  0.302195 , training accuracy :  0.882155\n",
      "i: 9100 , cost :  0.307477 , training accuracy :  0.87991\n",
      "i: 9200 , cost :  0.346115 , training accuracy :  0.858586\n",
      "i: 9300 , cost :  0.356443 , training accuracy :  0.852974\n",
      "i: 9400 , cost :  0.329814 , training accuracy :  0.867565\n",
      "i: 9500 , cost :  0.320855 , training accuracy :  0.867565\n",
      "i: 9600 , cost :  0.333025 , training accuracy :  0.864198\n",
      "i: 9700 , cost :  0.430534 , training accuracy :  0.86532\n",
      "i: 9800 , cost :  0.291188 , training accuracy :  0.8844\n",
      "i: 9900 , cost :  0.410439 , training accuracy :  0.869809\n",
      "i: 10000 , cost :  0.276486 , training accuracy :  0.900112\n",
      "i: 10100 , cost :  0.279009 , training accuracy :  0.890011\n",
      "i: 10200 , cost :  0.375438 , training accuracy :  0.878788\n",
      "i: 10300 , cost :  0.280581 , training accuracy :  0.891134\n",
      "i: 10400 , cost :  0.928102 , training accuracy :  0.818182\n",
      "i: 10500 , cost :  0.288453 , training accuracy :  0.900112\n",
      "i: 10600 , cost :  0.270998 , training accuracy :  0.897868\n",
      "i: 10700 , cost :  0.268155 , training accuracy :  0.900112\n",
      "i: 10800 , cost :  0.283807 , training accuracy :  0.886644\n",
      "i: 10900 , cost :  0.317576 , training accuracy :  0.866442\n",
      "i: 11000 , cost :  0.312341 , training accuracy :  0.870932\n",
      "i: 11100 , cost :  0.365752 , training accuracy :  0.881033\n",
      "i: 11200 , cost :  0.320271 , training accuracy :  0.867565\n",
      "i: 11300 , cost :  0.365119 , training accuracy :  0.881033\n",
      "i: 11400 , cost :  0.29982 , training accuracy :  0.87991\n",
      "i: 11500 , cost :  0.279929 , training accuracy :  0.887767\n",
      "i: 11600 , cost :  0.264796 , training accuracy :  0.904602\n",
      "i: 11700 , cost :  0.260717 , training accuracy :  0.904602\n",
      "i: 11800 , cost :  0.262151 , training accuracy :  0.897868\n",
      "i: 11900 , cost :  0.259934 , training accuracy :  0.905724\n",
      "i: 12000 , cost :  0.290315 , training accuracy :  0.8844\n",
      "i: 12100 , cost :  0.26382 , training accuracy :  0.900112\n",
      "i: 12200 , cost :  0.264327 , training accuracy :  0.89899\n",
      "i: 12300 , cost :  0.409901 , training accuracy :  0.86532\n",
      "i: 12400 , cost :  0.255064 , training accuracy :  0.904602\n",
      "i: 12500 , cost :  0.32619 , training accuracy :  0.854097\n",
      "i: 12600 , cost :  0.390834 , training accuracy :  0.870932\n",
      "i: 12700 , cost :  0.326462 , training accuracy :  0.85073\n",
      "i: 12800 , cost :  0.255268 , training accuracy :  0.906846\n",
      "i: 12900 , cost :  0.345467 , training accuracy :  0.883277\n",
      "i: 13000 , cost :  0.265952 , training accuracy :  0.89899\n",
      "i: 13100 , cost :  0.250275 , training accuracy :  0.906846\n",
      "i: 13200 , cost :  0.335783 , training accuracy :  0.849607\n",
      "i: 13300 , cost :  0.307895 , training accuracy :  0.866442\n",
      "i: 13400 , cost :  0.323009 , training accuracy :  0.851852\n",
      "i: 13500 , cost :  0.26328 , training accuracy :  0.89899\n",
      "i: 13600 , cost :  0.252583 , training accuracy :  0.903479\n",
      "i: 13700 , cost :  0.389529 , training accuracy :  0.868687\n",
      "i: 13800 , cost :  0.268585 , training accuracy :  0.894501\n",
      "i: 13900 , cost :  0.326408 , training accuracy :  0.855219\n",
      "i: 14000 , cost :  0.246831 , training accuracy :  0.909091\n",
      "i: 14100 , cost :  0.249422 , training accuracy :  0.904602\n",
      "i: 14200 , cost :  0.280156 , training accuracy :  0.890011\n",
      "i: 14300 , cost :  0.28012 , training accuracy :  0.892256\n",
      "i: 14400 , cost :  0.32865 , training accuracy :  0.886644\n",
      "i: 14500 , cost :  0.298458 , training accuracy :  0.890011\n",
      "i: 14600 , cost :  0.249432 , training accuracy :  0.901235\n",
      "i: 14700 , cost :  0.246635 , training accuracy :  0.904602\n",
      "i: 14800 , cost :  0.27275 , training accuracy :  0.895623\n",
      "i: 14900 , cost :  0.243549 , training accuracy :  0.910213\n",
      "i: 15000 , cost :  0.243218 , training accuracy :  0.911336\n",
      "i: 15100 , cost :  0.253034 , training accuracy :  0.89899\n",
      "i: 15200 , cost :  0.250418 , training accuracy :  0.897868\n",
      "i: 15300 , cost :  0.238447 , training accuracy :  0.914703\n",
      "i: 15400 , cost :  0.458852 , training accuracy :  0.858586\n",
      "i: 15500 , cost :  0.242206 , training accuracy :  0.905724\n",
      "i: 15600 , cost :  0.239513 , training accuracy :  0.91358\n",
      "i: 15700 , cost :  0.254559 , training accuracy :  0.897868\n",
      "i: 15800 , cost :  0.271198 , training accuracy :  0.894501\n",
      "i: 15900 , cost :  0.2373 , training accuracy :  0.911336\n",
      "i: 16000 , cost :  0.268479 , training accuracy :  0.89899\n",
      "i: 16100 , cost :  0.255025 , training accuracy :  0.904602\n",
      "i: 16200 , cost :  0.294949 , training accuracy :  0.870932\n",
      "i: 16300 , cost :  0.275338 , training accuracy :  0.894501\n",
      "i: 16400 , cost :  0.235873 , training accuracy :  0.912458\n",
      "i: 16500 , cost :  0.258217 , training accuracy :  0.903479\n",
      "i: 16600 , cost :  0.285522 , training accuracy :  0.877666\n",
      "i: 16700 , cost :  0.234001 , training accuracy :  0.911336\n",
      "i: 16800 , cost :  0.232515 , training accuracy :  0.912458\n",
      "i: 16900 , cost :  0.288689 , training accuracy :  0.872054\n",
      "i: 17000 , cost :  0.234422 , training accuracy :  0.907969\n",
      "i: 17100 , cost :  0.301 , training accuracy :  0.866442\n",
      "i: 17200 , cost :  0.236706 , training accuracy :  0.907969\n",
      "i: 17300 , cost :  0.283041 , training accuracy :  0.894501\n",
      "i: 17400 , cost :  0.263693 , training accuracy :  0.895623\n",
      "i: 17500 , cost :  0.354953 , training accuracy :  0.877666\n",
      "i: 17600 , cost :  0.23133 , training accuracy :  0.91358\n",
      "i: 17700 , cost :  0.261222 , training accuracy :  0.900112\n",
      "i: 17800 , cost :  0.265484 , training accuracy :  0.891134\n",
      "i: 17900 , cost :  0.239184 , training accuracy :  0.903479\n",
      "i: 18000 , cost :  0.237316 , training accuracy :  0.906846\n",
      "i: 18100 , cost :  0.239869 , training accuracy :  0.905724\n",
      "i: 18200 , cost :  0.242043 , training accuracy :  0.902357\n",
      "i: 18300 , cost :  0.301786 , training accuracy :  0.864198\n",
      "i: 18400 , cost :  0.230912 , training accuracy :  0.911336\n",
      "i: 18500 , cost :  0.287439 , training accuracy :  0.872054\n",
      "i: 18600 , cost :  0.228806 , training accuracy :  0.912458\n",
      "i: 18700 , cost :  0.229436 , training accuracy :  0.910213\n",
      "i: 18800 , cost :  0.29604 , training accuracy :  0.868687\n",
      "i: 18900 , cost :  0.240347 , training accuracy :  0.903479\n",
      "i: 19000 , cost :  0.235448 , training accuracy :  0.906846\n",
      "i: 19100 , cost :  0.26236 , training accuracy :  0.888889\n",
      "i: 19200 , cost :  0.315464 , training accuracy :  0.887767\n",
      "i: 19300 , cost :  0.227215 , training accuracy :  0.914703\n",
      "i: 19400 , cost :  0.234771 , training accuracy :  0.907969\n",
      "i: 19500 , cost :  0.546524 , training accuracy :  0.864198\n",
      "i: 19600 , cost :  0.227874 , training accuracy :  0.912458\n",
      "i: 19700 , cost :  0.222881 , training accuracy :  0.91807\n",
      "i: 19800 , cost :  0.222126 , training accuracy :  0.915825\n",
      "i: 19900 , cost :  0.256478 , training accuracy :  0.893378\n",
      "i: 20000 , cost :  0.223033 , training accuracy :  0.916947\n",
      "i: 20100 , cost :  0.228772 , training accuracy :  0.911336\n",
      "i: 20200 , cost :  0.320987 , training accuracy :  0.886644\n",
      "i: 20300 , cost :  0.254755 , training accuracy :  0.896745\n",
      "i: 20400 , cost :  0.261688 , training accuracy :  0.893378\n",
      "i: 20500 , cost :  0.28111 , training accuracy :  0.893378\n",
      "i: 20600 , cost :  0.231277 , training accuracy :  0.907969\n",
      "i: 20700 , cost :  0.249655 , training accuracy :  0.900112\n",
      "i: 20800 , cost :  0.344526 , training accuracy :  0.87991\n",
      "i: 20900 , cost :  0.278797 , training accuracy :  0.882155\n",
      "i: 21000 , cost :  0.226193 , training accuracy :  0.91358\n",
      "i: 21100 , cost :  0.232325 , training accuracy :  0.906846\n",
      "i: 21200 , cost :  0.224218 , training accuracy :  0.914703\n",
      "i: 21300 , cost :  0.221032 , training accuracy :  0.915825\n",
      "i: 21400 , cost :  0.265969 , training accuracy :  0.89899\n",
      "i: 21500 , cost :  0.224998 , training accuracy :  0.914703\n",
      "i: 21600 , cost :  0.261144 , training accuracy :  0.892256\n",
      "i: 21700 , cost :  0.219404 , training accuracy :  0.916947\n",
      "i: 21800 , cost :  0.222922 , training accuracy :  0.914703\n",
      "i: 21900 , cost :  0.249661 , training accuracy :  0.900112\n",
      "i: 22000 , cost :  0.227501 , training accuracy :  0.910213\n",
      "i: 22100 , cost :  0.292364 , training accuracy :  0.890011\n",
      "i: 22200 , cost :  0.219146 , training accuracy :  0.921437\n",
      "i: 22300 , cost :  0.288889 , training accuracy :  0.876543\n",
      "i: 22400 , cost :  0.234801 , training accuracy :  0.906846\n",
      "i: 22500 , cost :  0.218412 , training accuracy :  0.919192\n",
      "i: 22600 , cost :  0.27343 , training accuracy :  0.900112\n",
      "i: 22700 , cost :  0.270438 , training accuracy :  0.882155\n",
      "i: 22800 , cost :  0.273272 , training accuracy :  0.893378\n",
      "i: 22900 , cost :  0.221161 , training accuracy :  0.91358\n",
      "i: 23000 , cost :  0.285793 , training accuracy :  0.894501\n",
      "i: 23100 , cost :  0.226592 , training accuracy :  0.912458\n",
      "i: 23200 , cost :  0.244068 , training accuracy :  0.905724\n",
      "i: 23300 , cost :  0.289992 , training accuracy :  0.894501\n",
      "i: 23400 , cost :  0.222676 , training accuracy :  0.916947\n",
      "i: 23500 , cost :  0.223877 , training accuracy :  0.916947\n",
      "i: 23600 , cost :  0.2258 , training accuracy :  0.907969\n",
      "i: 23700 , cost :  0.229138 , training accuracy :  0.906846\n",
      "i: 23800 , cost :  0.28542 , training accuracy :  0.894501\n",
      "i: 23900 , cost :  0.244788 , training accuracy :  0.902357\n",
      "i: 24000 , cost :  0.276844 , training accuracy :  0.87991\n",
      "i: 24100 , cost :  0.236706 , training accuracy :  0.907969\n",
      "i: 24200 , cost :  0.268744 , training accuracy :  0.89899\n",
      "i: 24300 , cost :  0.233585 , training accuracy :  0.906846\n",
      "i: 24400 , cost :  0.21701 , training accuracy :  0.91807\n",
      "i: 24500 , cost :  0.232831 , training accuracy :  0.906846\n",
      "i: 24600 , cost :  0.241053 , training accuracy :  0.906846\n",
      "i: 24700 , cost :  0.302436 , training accuracy :  0.892256\n",
      "i: 24800 , cost :  0.216347 , training accuracy :  0.916947\n",
      "i: 24900 , cost :  0.251405 , training accuracy :  0.896745\n",
      "i: 25000 , cost :  0.217688 , training accuracy :  0.914703\n",
      "i: 25100 , cost :  0.214064 , training accuracy :  0.920314\n",
      "i: 25200 , cost :  0.217034 , training accuracy :  0.915825\n",
      "i: 25300 , cost :  0.233322 , training accuracy :  0.907969\n",
      "i: 25400 , cost :  0.216492 , training accuracy :  0.919192\n",
      "i: 25500 , cost :  0.26005 , training accuracy :  0.888889\n",
      "i: 25600 , cost :  0.213315 , training accuracy :  0.919192\n",
      "i: 25700 , cost :  0.218723 , training accuracy :  0.91358\n",
      "i: 25800 , cost :  0.242833 , training accuracy :  0.905724\n",
      "i: 25900 , cost :  0.236225 , training accuracy :  0.902357\n",
      "i: 26000 , cost :  0.213039 , training accuracy :  0.922559\n",
      "i: 26100 , cost :  0.221837 , training accuracy :  0.916947\n",
      "i: 26200 , cost :  0.215158 , training accuracy :  0.921437\n",
      "i: 26300 , cost :  0.23626 , training accuracy :  0.907969\n",
      "i: 26400 , cost :  0.275416 , training accuracy :  0.87991\n",
      "i: 26500 , cost :  0.986794 , training accuracy :  0.835017\n",
      "i: 26600 , cost :  0.215163 , training accuracy :  0.920314\n",
      "i: 26700 , cost :  0.209983 , training accuracy :  0.921437\n",
      "i: 26800 , cost :  0.212067 , training accuracy :  0.915825\n",
      "i: 26900 , cost :  0.21325 , training accuracy :  0.922559\n",
      "i: 27000 , cost :  0.229111 , training accuracy :  0.907969\n",
      "i: 27100 , cost :  0.209521 , training accuracy :  0.923681\n",
      "i: 27200 , cost :  0.220841 , training accuracy :  0.915825\n",
      "i: 27300 , cost :  0.224032 , training accuracy :  0.905724\n",
      "i: 27400 , cost :  0.226956 , training accuracy :  0.912458\n",
      "i: 27500 , cost :  0.240179 , training accuracy :  0.903479\n",
      "i: 27600 , cost :  0.225845 , training accuracy :  0.907969\n",
      "i: 27700 , cost :  0.211345 , training accuracy :  0.921437\n",
      "i: 27800 , cost :  0.217594 , training accuracy :  0.910213\n",
      "i: 27900 , cost :  0.222895 , training accuracy :  0.916947\n",
      "i: 28000 , cost :  0.215458 , training accuracy :  0.920314\n",
      "i: 28100 , cost :  0.212616 , training accuracy :  0.920314\n",
      "i: 28200 , cost :  0.275196 , training accuracy :  0.895623\n",
      "i: 28300 , cost :  0.243227 , training accuracy :  0.902357\n",
      "i: 28400 , cost :  0.339539 , training accuracy :  0.8844\n",
      "i: 28500 , cost :  0.246995 , training accuracy :  0.903479\n",
      "i: 28600 , cost :  0.219832 , training accuracy :  0.907969\n",
      "i: 28700 , cost :  0.213342 , training accuracy :  0.916947\n",
      "i: 28800 , cost :  0.224617 , training accuracy :  0.914703\n",
      "i: 28900 , cost :  0.244214 , training accuracy :  0.901235\n",
      "i: 29000 , cost :  0.234124 , training accuracy :  0.906846\n",
      "i: 29100 , cost :  0.21533 , training accuracy :  0.920314\n",
      "i: 29200 , cost :  0.241122 , training accuracy :  0.900112\n",
      "i: 29300 , cost :  0.207224 , training accuracy :  0.920314\n",
      "i: 29400 , cost :  0.215532 , training accuracy :  0.91358\n",
      "i: 29500 , cost :  0.234334 , training accuracy :  0.91358\n",
      "i: 29600 , cost :  0.207684 , training accuracy :  0.924804\n",
      "i: 29700 , cost :  0.225619 , training accuracy :  0.906846\n",
      "i: 29800 , cost :  0.22192 , training accuracy :  0.916947\n",
      "i: 29900 , cost :  0.330418 , training accuracy :  0.882155\n",
      "Final training accuracy :  0.912458\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.001 ][epoch: 30000 ][hidden: 100 ][file: bhavul_tr_acc_0.91_prediction.csv ] ACCURACY :  0.912458\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  83.4587 , training accuracy :  0.383838\n",
      "i: 100 , cost :  9.27655 , training accuracy :  0.383838\n",
      "i: 200 , cost :  2.75507 , training accuracy :  0.649832\n",
      "i: 300 , cost :  1.66521 , training accuracy :  0.649832\n",
      "i: 400 , cost :  1.14226 , training accuracy :  0.65881\n",
      "i: 500 , cost :  0.86489 , training accuracy :  0.655443\n",
      "i: 600 , cost :  0.709174 , training accuracy :  0.650954\n",
      "i: 700 , cost :  0.653956 , training accuracy :  0.648709\n",
      "i: 800 , cost :  0.649919 , training accuracy :  0.634119\n",
      "i: 900 , cost :  0.64964 , training accuracy :  0.635241\n",
      "Final training accuracy :  0.635241\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 1000 ][hidden: 3 ][file: bhavul_tr_acc_0.64_prediction.csv ] ACCURACY :  0.635241\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  207.887 , training accuracy :  0.314254\n",
      "i: 100 , cost :  51.129 , training accuracy :  0.50505\n",
      "i: 200 , cost :  26.6029 , training accuracy :  0.518519\n",
      "i: 300 , cost :  18.5168 , training accuracy :  0.540965\n",
      "i: 400 , cost :  12.0369 , training accuracy :  0.554433\n",
      "i: 500 , cost :  6.64597 , training accuracy :  0.56229\n",
      "i: 600 , cost :  4.44018 , training accuracy :  0.611672\n",
      "i: 700 , cost :  3.17663 , training accuracy :  0.648709\n",
      "i: 800 , cost :  2.29872 , training accuracy :  0.683502\n",
      "i: 900 , cost :  1.81137 , training accuracy :  0.746352\n",
      "Final training accuracy :  0.754209\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 1000 ][hidden: 10 ][file: bhavul_tr_acc_0.75_prediction.csv ] ACCURACY :  0.754209\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  244.102 , training accuracy :  0.61055\n",
      "i: 100 , cost :  79.4875 , training accuracy :  0.627385\n",
      "i: 200 , cost :  13.1574 , training accuracy :  0.599327\n",
      "i: 300 , cost :  6.30548 , training accuracy :  0.626263\n",
      "i: 400 , cost :  3.71649 , training accuracy :  0.693603\n",
      "i: 500 , cost :  2.32942 , training accuracy :  0.742985\n",
      "i: 600 , cost :  1.40294 , training accuracy :  0.749719\n",
      "i: 700 , cost :  0.976667 , training accuracy :  0.784512\n",
      "i: 800 , cost :  0.812541 , training accuracy :  0.786756\n",
      "i: 900 , cost :  0.684919 , training accuracy :  0.792368\n",
      "Final training accuracy :  0.782267\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 1000 ][hidden: 15 ][file: bhavul_tr_acc_0.78_prediction.csv ] ACCURACY :  0.782267\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  220.933 , training accuracy :  0.62514\n",
      "i: 100 , cost :  17.4891 , training accuracy :  0.638608\n",
      "i: 200 , cost :  4.26511 , training accuracy :  0.627385\n",
      "i: 300 , cost :  2.54688 , training accuracy :  0.699214\n",
      "i: 400 , cost :  1.70449 , training accuracy :  0.742985\n",
      "i: 500 , cost :  1.27204 , training accuracy :  0.762065\n",
      "i: 600 , cost :  0.975734 , training accuracy :  0.780022\n",
      "i: 700 , cost :  0.783077 , training accuracy :  0.795735\n",
      "i: 800 , cost :  0.749368 , training accuracy :  0.775533\n",
      "i: 900 , cost :  0.624158 , training accuracy :  0.808081\n",
      "Final training accuracy :  0.79798\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 1000 ][hidden: 50 ][file: bhavul_tr_acc_0.80_prediction.csv ] ACCURACY :  0.79798\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1055.63 , training accuracy :  0.616162\n",
      "i: 100 , cost :  63.7366 , training accuracy :  0.634119\n",
      "i: 200 , cost :  10.7909 , training accuracy :  0.650954\n",
      "i: 300 , cost :  4.25986 , training accuracy :  0.716049\n",
      "i: 400 , cost :  2.61165 , training accuracy :  0.754209\n",
      "i: 500 , cost :  2.24785 , training accuracy :  0.749719\n",
      "i: 600 , cost :  1.81857 , training accuracy :  0.765432\n",
      "i: 700 , cost :  1.46603 , training accuracy :  0.767677\n",
      "i: 800 , cost :  1.40408 , training accuracy :  0.76431\n",
      "i: 900 , cost :  1.24137 , training accuracy :  0.768799\n",
      "Final training accuracy :  0.805836\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 1000 ][hidden: 100 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.805836\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  52.7437 , training accuracy :  0.631874\n",
      "i: 100 , cost :  20.1788 , training accuracy :  0.643098\n",
      "i: 200 , cost :  5.44879 , training accuracy :  0.511784\n",
      "i: 300 , cost :  1.57654 , training accuracy :  0.386083\n",
      "i: 400 , cost :  1.13807 , training accuracy :  0.388328\n",
      "i: 500 , cost :  1.01543 , training accuracy :  0.646465\n",
      "i: 600 , cost :  0.941862 , training accuracy :  0.643098\n",
      "i: 700 , cost :  0.891915 , training accuracy :  0.643098\n",
      "i: 800 , cost :  0.847758 , training accuracy :  0.641975\n",
      "i: 900 , cost :  0.803791 , training accuracy :  0.640853\n",
      "i: 1000 , cost :  0.767142 , training accuracy :  0.639731\n",
      "i: 1100 , cost :  0.743256 , training accuracy :  0.643098\n",
      "i: 1200 , cost :  0.719705 , training accuracy :  0.655443\n",
      "i: 1300 , cost :  0.704547 , training accuracy :  0.657688\n",
      "i: 1400 , cost :  0.689437 , training accuracy :  0.656566\n",
      "i: 1500 , cost :  0.673962 , training accuracy :  0.652076\n",
      "i: 1600 , cost :  0.66145 , training accuracy :  0.654321\n",
      "i: 1700 , cost :  0.654389 , training accuracy :  0.665544\n",
      "i: 1800 , cost :  0.647167 , training accuracy :  0.668911\n",
      "i: 1900 , cost :  0.636213 , training accuracy :  0.67789\n",
      "i: 2000 , cost :  0.63309 , training accuracy :  0.67789\n",
      "i: 2100 , cost :  0.631514 , training accuracy :  0.676768\n",
      "i: 2200 , cost :  0.63027 , training accuracy :  0.676768\n",
      "i: 2300 , cost :  0.629022 , training accuracy :  0.676768\n",
      "i: 2400 , cost :  0.627789 , training accuracy :  0.676768\n",
      "i: 2500 , cost :  0.626657 , training accuracy :  0.676768\n",
      "i: 2600 , cost :  0.625731 , training accuracy :  0.67789\n",
      "i: 2700 , cost :  0.624952 , training accuracy :  0.679012\n",
      "i: 2800 , cost :  0.623521 , training accuracy :  0.67789\n",
      "i: 2900 , cost :  0.622055 , training accuracy :  0.676768\n",
      "i: 3000 , cost :  0.620576 , training accuracy :  0.675645\n",
      "i: 3100 , cost :  0.619203 , training accuracy :  0.675645\n",
      "i: 3200 , cost :  0.618071 , training accuracy :  0.675645\n",
      "i: 3300 , cost :  0.616994 , training accuracy :  0.67789\n",
      "i: 3400 , cost :  0.616231 , training accuracy :  0.682379\n",
      "i: 3500 , cost :  0.615557 , training accuracy :  0.681257\n",
      "i: 3600 , cost :  0.614835 , training accuracy :  0.683502\n",
      "i: 3700 , cost :  0.614064 , training accuracy :  0.685746\n",
      "i: 3800 , cost :  0.613383 , training accuracy :  0.683502\n",
      "i: 3900 , cost :  0.612639 , training accuracy :  0.683502\n",
      "i: 4000 , cost :  0.611615 , training accuracy :  0.684624\n",
      "i: 4100 , cost :  0.610637 , training accuracy :  0.685746\n",
      "i: 4200 , cost :  0.609809 , training accuracy :  0.685746\n",
      "i: 4300 , cost :  0.608926 , training accuracy :  0.690236\n",
      "i: 4400 , cost :  0.607632 , training accuracy :  0.690236\n",
      "i: 4500 , cost :  0.604872 , training accuracy :  0.69248\n",
      "i: 4600 , cost :  0.600233 , training accuracy :  0.69697\n",
      "i: 4700 , cost :  0.582702 , training accuracy :  0.720539\n",
      "i: 4800 , cost :  0.501307 , training accuracy :  0.781145\n",
      "i: 4900 , cost :  0.486731 , training accuracy :  0.790123\n",
      "Final training accuracy :  0.79798\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 5000 ][hidden: 3 ][file: bhavul_tr_acc_0.80_prediction.csv ] ACCURACY :  0.79798\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  451.553 , training accuracy :  0.38945\n",
      "i: 100 , cost :  77.5447 , training accuracy :  0.413019\n",
      "i: 200 , cost :  23.5762 , training accuracy :  0.646465\n",
      "i: 300 , cost :  19.1103 , training accuracy :  0.641975\n",
      "i: 400 , cost :  12.1432 , training accuracy :  0.634119\n",
      "i: 500 , cost :  7.22336 , training accuracy :  0.655443\n",
      "i: 600 , cost :  3.97353 , training accuracy :  0.675645\n",
      "i: 700 , cost :  2.2704 , training accuracy :  0.772166\n",
      "i: 800 , cost :  1.87024 , training accuracy :  0.771044\n",
      "i: 900 , cost :  1.56446 , training accuracy :  0.781145\n",
      "i: 1000 , cost :  1.34811 , training accuracy :  0.785634\n",
      "i: 1100 , cost :  1.20263 , training accuracy :  0.789001\n",
      "i: 1200 , cost :  1.07406 , training accuracy :  0.791246\n",
      "i: 1300 , cost :  0.978437 , training accuracy :  0.794613\n",
      "i: 1400 , cost :  0.883032 , training accuracy :  0.794613\n",
      "i: 1500 , cost :  0.784187 , training accuracy :  0.796857\n",
      "i: 1600 , cost :  0.714433 , training accuracy :  0.801347\n",
      "i: 1700 , cost :  0.662434 , training accuracy :  0.791246\n",
      "i: 1800 , cost :  0.6072 , training accuracy :  0.804714\n",
      "i: 1900 , cost :  0.567339 , training accuracy :  0.796857\n",
      "i: 2000 , cost :  0.533602 , training accuracy :  0.799102\n",
      "i: 2100 , cost :  0.504888 , training accuracy :  0.79798\n",
      "i: 2200 , cost :  0.478627 , training accuracy :  0.795735\n",
      "i: 2300 , cost :  0.461607 , training accuracy :  0.799102\n",
      "i: 2400 , cost :  0.449264 , training accuracy :  0.794613\n",
      "i: 2500 , cost :  0.442023 , training accuracy :  0.799102\n",
      "i: 2600 , cost :  0.440256 , training accuracy :  0.790123\n",
      "i: 2700 , cost :  0.433866 , training accuracy :  0.805836\n",
      "i: 2800 , cost :  0.437116 , training accuracy :  0.792368\n",
      "i: 2900 , cost :  0.431921 , training accuracy :  0.805836\n",
      "i: 3000 , cost :  0.433713 , training accuracy :  0.795735\n",
      "i: 3100 , cost :  0.436647 , training accuracy :  0.804714\n",
      "i: 3200 , cost :  0.432794 , training accuracy :  0.799102\n",
      "i: 3300 , cost :  0.433544 , training accuracy :  0.795735\n",
      "i: 3400 , cost :  0.430331 , training accuracy :  0.801347\n",
      "i: 3500 , cost :  0.4366 , training accuracy :  0.791246\n",
      "i: 3600 , cost :  0.429834 , training accuracy :  0.802469\n",
      "i: 3700 , cost :  0.430907 , training accuracy :  0.803591\n",
      "i: 3800 , cost :  0.44116 , training accuracy :  0.805836\n",
      "i: 3900 , cost :  0.429431 , training accuracy :  0.801347\n",
      "i: 4000 , cost :  0.430846 , training accuracy :  0.809203\n",
      "i: 4100 , cost :  0.448126 , training accuracy :  0.810326\n",
      "i: 4200 , cost :  0.429445 , training accuracy :  0.806958\n",
      "i: 4300 , cost :  0.435131 , training accuracy :  0.79349\n",
      "i: 4400 , cost :  0.431162 , training accuracy :  0.796857\n",
      "i: 4500 , cost :  0.447582 , training accuracy :  0.811448\n",
      "i: 4600 , cost :  0.428774 , training accuracy :  0.804714\n",
      "i: 4700 , cost :  0.439801 , training accuracy :  0.791246\n",
      "i: 4800 , cost :  0.428755 , training accuracy :  0.806958\n",
      "i: 4900 , cost :  0.442629 , training accuracy :  0.790123\n",
      "Final training accuracy :  0.806958\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 5000 ][hidden: 10 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.806958\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1209.65 , training accuracy :  0.387205\n",
      "i: 100 , cost :  549.835 , training accuracy :  0.338945\n",
      "i: 200 , cost :  59.0339 , training accuracy :  0.556678\n",
      "i: 300 , cost :  22.0585 , training accuracy :  0.616162\n",
      "i: 400 , cost :  14.4413 , training accuracy :  0.622896\n",
      "i: 500 , cost :  10.2423 , training accuracy :  0.617284\n",
      "i: 600 , cost :  7.13976 , training accuracy :  0.650954\n",
      "i: 700 , cost :  4.98537 , training accuracy :  0.672278\n",
      "i: 800 , cost :  3.53335 , training accuracy :  0.684624\n",
      "i: 900 , cost :  2.58208 , training accuracy :  0.722783\n",
      "i: 1000 , cost :  2.03259 , training accuracy :  0.751964\n",
      "i: 1100 , cost :  1.41424 , training accuracy :  0.753086\n",
      "i: 1200 , cost :  1.11753 , training accuracy :  0.774411\n",
      "i: 1300 , cost :  0.971738 , training accuracy :  0.774411\n",
      "i: 1400 , cost :  0.859049 , training accuracy :  0.775533\n",
      "i: 1500 , cost :  0.773843 , training accuracy :  0.7789\n",
      "i: 1600 , cost :  0.71126 , training accuracy :  0.780022\n",
      "i: 1700 , cost :  0.672363 , training accuracy :  0.783389\n",
      "i: 1800 , cost :  0.650948 , training accuracy :  0.79349\n",
      "i: 1900 , cost :  0.639739 , training accuracy :  0.796857\n",
      "i: 2000 , cost :  0.628693 , training accuracy :  0.795735\n",
      "i: 2100 , cost :  0.631232 , training accuracy :  0.787879\n",
      "i: 2200 , cost :  0.603977 , training accuracy :  0.781145\n",
      "i: 2300 , cost :  0.602876 , training accuracy :  0.79798\n",
      "i: 2400 , cost :  0.586558 , training accuracy :  0.791246\n",
      "i: 2500 , cost :  0.580184 , training accuracy :  0.795735\n",
      "i: 2600 , cost :  0.578467 , training accuracy :  0.800224\n",
      "i: 2700 , cost :  0.589435 , training accuracy :  0.803591\n",
      "i: 2800 , cost :  0.586878 , training accuracy :  0.801347\n",
      "i: 2900 , cost :  0.556064 , training accuracy :  0.794613\n",
      "i: 3000 , cost :  0.555528 , training accuracy :  0.794613\n",
      "i: 3100 , cost :  0.562253 , training accuracy :  0.787879\n",
      "i: 3200 , cost :  0.54264 , training accuracy :  0.795735\n",
      "i: 3300 , cost :  0.553568 , training accuracy :  0.796857\n",
      "i: 3400 , cost :  0.538012 , training accuracy :  0.794613\n",
      "i: 3500 , cost :  0.553427 , training accuracy :  0.805836\n",
      "i: 3600 , cost :  0.529906 , training accuracy :  0.799102\n",
      "i: 3700 , cost :  0.531543 , training accuracy :  0.791246\n",
      "i: 3800 , cost :  0.523644 , training accuracy :  0.786756\n",
      "i: 3900 , cost :  0.524856 , training accuracy :  0.805836\n",
      "i: 4000 , cost :  0.522035 , training accuracy :  0.794613\n",
      "i: 4100 , cost :  0.511007 , training accuracy :  0.794613\n",
      "i: 4200 , cost :  0.508235 , training accuracy :  0.795735\n",
      "i: 4300 , cost :  0.512666 , training accuracy :  0.804714\n",
      "i: 4400 , cost :  0.506997 , training accuracy :  0.794613\n",
      "i: 4500 , cost :  0.50331 , training accuracy :  0.79349\n",
      "i: 4600 , cost :  0.520891 , training accuracy :  0.809203\n",
      "i: 4700 , cost :  0.498275 , training accuracy :  0.796857\n",
      "i: 4800 , cost :  0.499838 , training accuracy :  0.790123\n",
      "i: 4900 , cost :  0.501332 , training accuracy :  0.805836\n",
      "Final training accuracy :  0.79798\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 5000 ][hidden: 15 ][file: bhavul_tr_acc_0.80_prediction.csv ] ACCURACY :  0.79798\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1101.38 , training accuracy :  0.340067\n",
      "i: 100 , cost :  69.9483 , training accuracy :  0.493827\n",
      "i: 200 , cost :  19.32 , training accuracy :  0.608305\n",
      "i: 300 , cost :  6.93489 , training accuracy :  0.619529\n",
      "i: 400 , cost :  3.61845 , training accuracy :  0.689113\n",
      "i: 500 , cost :  2.86389 , training accuracy :  0.723906\n",
      "i: 600 , cost :  2.35442 , training accuracy :  0.740741\n",
      "i: 700 , cost :  2.01762 , training accuracy :  0.755331\n",
      "i: 800 , cost :  1.72414 , training accuracy :  0.750842\n",
      "i: 900 , cost :  1.49694 , training accuracy :  0.760943\n",
      "i: 1000 , cost :  1.33828 , training accuracy :  0.771044\n",
      "i: 1100 , cost :  1.2247 , training accuracy :  0.775533\n",
      "i: 1200 , cost :  1.13177 , training accuracy :  0.7789\n",
      "i: 1300 , cost :  1.07286 , training accuracy :  0.7789\n",
      "i: 1400 , cost :  0.94225 , training accuracy :  0.79798\n",
      "i: 1500 , cost :  0.8592 , training accuracy :  0.804714\n",
      "i: 1600 , cost :  0.788542 , training accuracy :  0.814815\n",
      "i: 1700 , cost :  0.814144 , training accuracy :  0.773288\n",
      "i: 1800 , cost :  0.74233 , training accuracy :  0.802469\n",
      "i: 1900 , cost :  0.707544 , training accuracy :  0.810326\n",
      "i: 2000 , cost :  0.675271 , training accuracy :  0.808081\n",
      "i: 2100 , cost :  0.648185 , training accuracy :  0.810326\n",
      "i: 2200 , cost :  0.625297 , training accuracy :  0.815937\n",
      "i: 2300 , cost :  0.608375 , training accuracy :  0.815937\n",
      "i: 2400 , cost :  0.594041 , training accuracy :  0.818182\n",
      "i: 2500 , cost :  0.58112 , training accuracy :  0.815937\n",
      "i: 2600 , cost :  0.572202 , training accuracy :  0.81257\n",
      "i: 2700 , cost :  0.56049 , training accuracy :  0.810326\n",
      "i: 2800 , cost :  0.550449 , training accuracy :  0.813693\n",
      "i: 2900 , cost :  0.541654 , training accuracy :  0.810326\n",
      "i: 3000 , cost :  0.532036 , training accuracy :  0.810326\n",
      "i: 3100 , cost :  0.522799 , training accuracy :  0.81257\n",
      "i: 3200 , cost :  0.513075 , training accuracy :  0.814815\n",
      "i: 3300 , cost :  0.503826 , training accuracy :  0.814815\n",
      "i: 3400 , cost :  0.495364 , training accuracy :  0.817059\n",
      "i: 3500 , cost :  0.485033 , training accuracy :  0.820426\n",
      "i: 3600 , cost :  0.481331 , training accuracy :  0.818182\n",
      "i: 3700 , cost :  0.470211 , training accuracy :  0.817059\n",
      "i: 3800 , cost :  0.461621 , training accuracy :  0.819304\n",
      "i: 3900 , cost :  0.4531 , training accuracy :  0.824916\n",
      "i: 4000 , cost :  0.448085 , training accuracy :  0.824916\n",
      "i: 4100 , cost :  0.442758 , training accuracy :  0.826038\n",
      "i: 4200 , cost :  0.439167 , training accuracy :  0.824916\n",
      "i: 4300 , cost :  0.434614 , training accuracy :  0.824916\n",
      "i: 4400 , cost :  0.427276 , training accuracy :  0.829405\n",
      "i: 4500 , cost :  0.423749 , training accuracy :  0.828283\n",
      "i: 4600 , cost :  0.417138 , training accuracy :  0.830527\n",
      "i: 4700 , cost :  0.414577 , training accuracy :  0.829405\n",
      "i: 4800 , cost :  0.412347 , training accuracy :  0.83165\n",
      "i: 4900 , cost :  0.408648 , training accuracy :  0.832772\n",
      "Final training accuracy :  0.833894\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 5000 ][hidden: 50 ][file: bhavul_tr_acc_0.83_prediction.csv ] ACCURACY :  0.833894\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  614.331 , training accuracy :  0.396184\n",
      "i: 100 , cost :  16.3378 , training accuracy :  0.65881\n",
      "i: 200 , cost :  5.80249 , training accuracy :  0.695847\n",
      "i: 300 , cost :  4.01963 , training accuracy :  0.728395\n",
      "i: 400 , cost :  2.90593 , training accuracy :  0.742985\n",
      "i: 500 , cost :  2.23134 , training accuracy :  0.767677\n",
      "i: 600 , cost :  2.3044 , training accuracy :  0.714927\n",
      "i: 700 , cost :  1.75075 , training accuracy :  0.773288\n",
      "i: 800 , cost :  1.68634 , training accuracy :  0.767677\n",
      "i: 900 , cost :  1.83354 , training accuracy :  0.773288\n",
      "i: 1000 , cost :  1.60065 , training accuracy :  0.716049\n",
      "i: 1100 , cost :  1.60274 , training accuracy :  0.773288\n",
      "i: 1200 , cost :  1.32018 , training accuracy :  0.768799\n",
      "i: 1300 , cost :  1.25333 , training accuracy :  0.79798\n",
      "i: 1400 , cost :  1.32016 , training accuracy :  0.768799\n",
      "i: 1500 , cost :  1.06471 , training accuracy :  0.796857\n",
      "i: 1600 , cost :  1.02422 , training accuracy :  0.782267\n",
      "i: 1700 , cost :  1.01002 , training accuracy :  0.801347\n",
      "i: 1800 , cost :  0.900733 , training accuracy :  0.804714\n",
      "i: 1900 , cost :  6.13755 , training accuracy :  0.703704\n",
      "i: 2000 , cost :  0.862976 , training accuracy :  0.820426\n",
      "i: 2100 , cost :  0.805657 , training accuracy :  0.810326\n",
      "i: 2200 , cost :  1.11433 , training accuracy :  0.795735\n",
      "i: 2300 , cost :  0.774324 , training accuracy :  0.824916\n",
      "i: 2400 , cost :  0.721891 , training accuracy :  0.813693\n",
      "i: 2500 , cost :  3.20602 , training accuracy :  0.512907\n",
      "i: 2600 , cost :  0.709193 , training accuracy :  0.832772\n",
      "i: 2700 , cost :  0.660111 , training accuracy :  0.828283\n",
      "i: 2800 , cost :  1.63711 , training accuracy :  0.803591\n",
      "i: 2900 , cost :  0.654473 , training accuracy :  0.842873\n",
      "i: 3000 , cost :  0.613656 , training accuracy :  0.829405\n",
      "i: 3100 , cost :  0.602637 , training accuracy :  0.840629\n",
      "i: 3200 , cost :  1.39747 , training accuracy :  0.796857\n",
      "i: 3300 , cost :  0.600733 , training accuracy :  0.847363\n",
      "i: 3400 , cost :  0.625416 , training accuracy :  0.845118\n",
      "i: 3500 , cost :  1.59652 , training accuracy :  0.794613\n",
      "i: 3600 , cost :  0.596943 , training accuracy :  0.849607\n",
      "i: 3700 , cost :  0.554591 , training accuracy :  0.843996\n",
      "i: 3800 , cost :  0.834758 , training accuracy :  0.822671\n",
      "i: 3900 , cost :  0.55683 , training accuracy :  0.845118\n",
      "i: 4000 , cost :  0.527693 , training accuracy :  0.845118\n",
      "i: 4100 , cost :  0.755188 , training accuracy :  0.787879\n",
      "i: 4200 , cost :  0.586367 , training accuracy :  0.854097\n",
      "i: 4300 , cost :  0.520373 , training accuracy :  0.85073\n",
      "i: 4400 , cost :  0.50812 , training accuracy :  0.85073\n",
      "i: 4500 , cost :  0.989181 , training accuracy :  0.814815\n",
      "i: 4600 , cost :  0.5356 , training accuracy :  0.855219\n",
      "i: 4700 , cost :  0.49525 , training accuracy :  0.849607\n",
      "i: 4800 , cost :  0.488247 , training accuracy :  0.85073\n",
      "i: 4900 , cost :  0.984673 , training accuracy :  0.821549\n",
      "Final training accuracy :  0.857464\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 5000 ][hidden: 100 ][file: bhavul_tr_acc_0.86_prediction.csv ] ACCURACY :  0.857464\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  232.979 , training accuracy :  0.383838\n",
      "i: 100 , cost :  99.3246 , training accuracy :  0.356902\n",
      "i: 200 , cost :  13.3869 , training accuracy :  0.515152\n",
      "i: 300 , cost :  8.99817 , training accuracy :  0.551066\n",
      "i: 400 , cost :  7.45935 , training accuracy :  0.555556\n",
      "i: 500 , cost :  5.83451 , training accuracy :  0.574635\n",
      "i: 600 , cost :  3.98598 , training accuracy :  0.592593\n",
      "i: 700 , cost :  2.71083 , training accuracy :  0.627385\n",
      "i: 800 , cost :  2.01427 , training accuracy :  0.637486\n",
      "i: 900 , cost :  1.53548 , training accuracy :  0.659933\n",
      "i: 1000 , cost :  1.26103 , training accuracy :  0.653199\n",
      "i: 1100 , cost :  1.07005 , training accuracy :  0.657688\n",
      "i: 1200 , cost :  0.941623 , training accuracy :  0.65881\n",
      "i: 1300 , cost :  0.837382 , training accuracy :  0.656566\n",
      "i: 1400 , cost :  0.779167 , training accuracy :  0.662177\n",
      "i: 1500 , cost :  0.749561 , training accuracy :  0.668911\n",
      "i: 1600 , cost :  0.732841 , training accuracy :  0.671156\n",
      "i: 1700 , cost :  0.720998 , training accuracy :  0.675645\n",
      "i: 1800 , cost :  0.713517 , training accuracy :  0.675645\n",
      "i: 1900 , cost :  0.704938 , training accuracy :  0.673401\n",
      "i: 2000 , cost :  0.697111 , training accuracy :  0.673401\n",
      "i: 2100 , cost :  0.691535 , training accuracy :  0.675645\n",
      "i: 2200 , cost :  0.687923 , training accuracy :  0.679012\n",
      "i: 2300 , cost :  0.685422 , training accuracy :  0.679012\n",
      "i: 2400 , cost :  0.682727 , training accuracy :  0.680135\n",
      "i: 2500 , cost :  0.678638 , training accuracy :  0.679012\n",
      "i: 2600 , cost :  0.674841 , training accuracy :  0.680135\n",
      "i: 2700 , cost :  0.671483 , training accuracy :  0.685746\n",
      "i: 2800 , cost :  0.66841 , training accuracy :  0.686869\n",
      "i: 2900 , cost :  0.665635 , training accuracy :  0.689113\n",
      "i: 3000 , cost :  0.662724 , training accuracy :  0.691358\n",
      "i: 3100 , cost :  0.659586 , training accuracy :  0.690236\n",
      "i: 3200 , cost :  0.656207 , training accuracy :  0.691358\n",
      "i: 3300 , cost :  0.652641 , training accuracy :  0.691358\n",
      "i: 3400 , cost :  0.648739 , training accuracy :  0.690236\n",
      "i: 3500 , cost :  0.644488 , training accuracy :  0.694725\n",
      "i: 3600 , cost :  0.639729 , training accuracy :  0.69697\n",
      "i: 3700 , cost :  0.633902 , training accuracy :  0.700337\n",
      "i: 3800 , cost :  0.627058 , training accuracy :  0.704826\n",
      "i: 3900 , cost :  0.618215 , training accuracy :  0.703704\n",
      "i: 4000 , cost :  0.605918 , training accuracy :  0.713805\n",
      "i: 4100 , cost :  0.593241 , training accuracy :  0.722783\n",
      "i: 4200 , cost :  0.581295 , training accuracy :  0.73064\n",
      "i: 4300 , cost :  0.570887 , training accuracy :  0.736251\n",
      "i: 4400 , cost :  0.562288 , training accuracy :  0.736251\n",
      "i: 4500 , cost :  0.555122 , training accuracy :  0.731762\n",
      "i: 4600 , cost :  0.550058 , training accuracy :  0.734007\n",
      "i: 4700 , cost :  0.546395 , training accuracy :  0.736251\n",
      "i: 4800 , cost :  0.543504 , training accuracy :  0.737374\n",
      "i: 4900 , cost :  0.541041 , training accuracy :  0.735129\n",
      "i: 5000 , cost :  0.538754 , training accuracy :  0.737374\n",
      "i: 5100 , cost :  0.536654 , training accuracy :  0.736251\n",
      "i: 5200 , cost :  0.534667 , training accuracy :  0.735129\n",
      "i: 5300 , cost :  0.53282 , training accuracy :  0.734007\n",
      "i: 5400 , cost :  0.531015 , training accuracy :  0.735129\n",
      "i: 5500 , cost :  0.529215 , training accuracy :  0.735129\n",
      "i: 5600 , cost :  0.527446 , training accuracy :  0.737374\n",
      "i: 5700 , cost :  0.525758 , training accuracy :  0.737374\n",
      "i: 5800 , cost :  0.524196 , training accuracy :  0.739618\n",
      "i: 5900 , cost :  0.522727 , training accuracy :  0.737374\n",
      "i: 6000 , cost :  0.521396 , training accuracy :  0.739618\n",
      "i: 6100 , cost :  0.520178 , training accuracy :  0.740741\n",
      "i: 6200 , cost :  0.519055 , training accuracy :  0.741863\n",
      "i: 6300 , cost :  0.518017 , training accuracy :  0.741863\n",
      "i: 6400 , cost :  0.517108 , training accuracy :  0.741863\n",
      "i: 6500 , cost :  0.516333 , training accuracy :  0.740741\n",
      "i: 6600 , cost :  0.515637 , training accuracy :  0.739618\n",
      "i: 6700 , cost :  0.515045 , training accuracy :  0.740741\n",
      "i: 6800 , cost :  0.514527 , training accuracy :  0.739618\n",
      "i: 6900 , cost :  0.514113 , training accuracy :  0.740741\n",
      "i: 7000 , cost :  0.513775 , training accuracy :  0.739618\n",
      "i: 7100 , cost :  0.513488 , training accuracy :  0.744108\n",
      "i: 7200 , cost :  0.513199 , training accuracy :  0.742985\n",
      "i: 7300 , cost :  0.513044 , training accuracy :  0.742985\n",
      "i: 7400 , cost :  0.512887 , training accuracy :  0.740741\n",
      "i: 7500 , cost :  0.51289 , training accuracy :  0.744108\n",
      "i: 7600 , cost :  0.512697 , training accuracy :  0.738496\n",
      "i: 7700 , cost :  0.512809 , training accuracy :  0.741863\n",
      "i: 7800 , cost :  0.512627 , training accuracy :  0.739618\n",
      "i: 7900 , cost :  0.512549 , training accuracy :  0.739618\n",
      "i: 8000 , cost :  0.512525 , training accuracy :  0.739618\n",
      "i: 8100 , cost :  0.512489 , training accuracy :  0.739618\n",
      "i: 8200 , cost :  0.512474 , training accuracy :  0.739618\n",
      "i: 8300 , cost :  0.512495 , training accuracy :  0.737374\n",
      "i: 8400 , cost :  0.512437 , training accuracy :  0.737374\n",
      "i: 8500 , cost :  0.512521 , training accuracy :  0.738496\n",
      "i: 8600 , cost :  0.512397 , training accuracy :  0.737374\n",
      "i: 8700 , cost :  0.512453 , training accuracy :  0.738496\n",
      "i: 8800 , cost :  0.512349 , training accuracy :  0.739618\n",
      "i: 8900 , cost :  0.512269 , training accuracy :  0.737374\n",
      "i: 9000 , cost :  0.512245 , training accuracy :  0.738496\n",
      "i: 9100 , cost :  0.512251 , training accuracy :  0.737374\n",
      "i: 9200 , cost :  0.512195 , training accuracy :  0.737374\n",
      "i: 9300 , cost :  0.51219 , training accuracy :  0.738496\n",
      "i: 9400 , cost :  0.512172 , training accuracy :  0.737374\n",
      "i: 9500 , cost :  0.5122 , training accuracy :  0.737374\n",
      "i: 9600 , cost :  0.512194 , training accuracy :  0.737374\n",
      "i: 9700 , cost :  0.512172 , training accuracy :  0.737374\n",
      "i: 9800 , cost :  0.512172 , training accuracy :  0.739618\n",
      "i: 9900 , cost :  0.512174 , training accuracy :  0.738496\n",
      "Final training accuracy :  0.738496\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 10000 ][hidden: 3 ][file: bhavul_tr_acc_0.74_prediction.csv ] ACCURACY :  0.738496\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  89.2508 , training accuracy :  0.545455\n",
      "i: 100 , cost :  42.3066 , training accuracy :  0.62963\n",
      "i: 200 , cost :  16.8122 , training accuracy :  0.648709\n",
      "i: 300 , cost :  4.7699 , training accuracy :  0.621773\n",
      "i: 400 , cost :  1.92016 , training accuracy :  0.662177\n",
      "i: 500 , cost :  1.08297 , training accuracy :  0.720539\n",
      "i: 600 , cost :  0.781868 , training accuracy :  0.762065\n",
      "i: 700 , cost :  0.656666 , training accuracy :  0.769921\n",
      "i: 800 , cost :  0.618891 , training accuracy :  0.776655\n",
      "i: 900 , cost :  0.586484 , training accuracy :  0.792368\n",
      "i: 1000 , cost :  0.57079 , training accuracy :  0.792368\n",
      "i: 1100 , cost :  0.561383 , training accuracy :  0.791246\n",
      "i: 1200 , cost :  0.551849 , training accuracy :  0.792368\n",
      "i: 1300 , cost :  0.544982 , training accuracy :  0.794613\n",
      "i: 1400 , cost :  0.540289 , training accuracy :  0.790123\n",
      "i: 1500 , cost :  0.544726 , training accuracy :  0.791246\n",
      "i: 1600 , cost :  0.533966 , training accuracy :  0.79349\n",
      "i: 1700 , cost :  0.526183 , training accuracy :  0.79798\n",
      "i: 1800 , cost :  0.522895 , training accuracy :  0.79349\n",
      "i: 1900 , cost :  0.525996 , training accuracy :  0.796857\n",
      "i: 2000 , cost :  0.516554 , training accuracy :  0.799102\n",
      "i: 2100 , cost :  0.513776 , training accuracy :  0.801347\n",
      "i: 2200 , cost :  0.51604 , training accuracy :  0.792368\n",
      "i: 2300 , cost :  0.511593 , training accuracy :  0.804714\n",
      "i: 2400 , cost :  0.508206 , training accuracy :  0.795735\n",
      "i: 2500 , cost :  0.504636 , training accuracy :  0.801347\n",
      "i: 2600 , cost :  0.502896 , training accuracy :  0.800224\n",
      "i: 2700 , cost :  0.502659 , training accuracy :  0.803591\n",
      "i: 2800 , cost :  0.499716 , training accuracy :  0.801347\n",
      "i: 2900 , cost :  0.497308 , training accuracy :  0.800224\n",
      "i: 3000 , cost :  0.49581 , training accuracy :  0.79798\n",
      "i: 3100 , cost :  0.494351 , training accuracy :  0.802469\n",
      "i: 3200 , cost :  0.4929 , training accuracy :  0.803591\n",
      "i: 3300 , cost :  0.491219 , training accuracy :  0.805836\n",
      "i: 3400 , cost :  0.490224 , training accuracy :  0.800224\n",
      "i: 3500 , cost :  0.488626 , training accuracy :  0.799102\n",
      "i: 3600 , cost :  0.49217 , training accuracy :  0.813693\n",
      "i: 3700 , cost :  0.48712 , training accuracy :  0.806958\n",
      "i: 3800 , cost :  0.487538 , training accuracy :  0.808081\n",
      "i: 3900 , cost :  0.483491 , training accuracy :  0.799102\n",
      "i: 4000 , cost :  0.48539 , training accuracy :  0.79349\n",
      "i: 4100 , cost :  0.485577 , training accuracy :  0.786756\n",
      "i: 4200 , cost :  0.480394 , training accuracy :  0.796857\n",
      "i: 4300 , cost :  0.478952 , training accuracy :  0.803591\n",
      "i: 4400 , cost :  0.488533 , training accuracy :  0.808081\n",
      "i: 4500 , cost :  0.477072 , training accuracy :  0.79798\n",
      "i: 4600 , cost :  0.480434 , training accuracy :  0.787879\n",
      "i: 4700 , cost :  0.475138 , training accuracy :  0.796857\n",
      "i: 4800 , cost :  0.473767 , training accuracy :  0.800224\n",
      "i: 4900 , cost :  0.473309 , training accuracy :  0.79349\n",
      "i: 5000 , cost :  0.472841 , training accuracy :  0.792368\n",
      "i: 5100 , cost :  0.475657 , training accuracy :  0.790123\n",
      "i: 5200 , cost :  0.471696 , training accuracy :  0.805836\n",
      "i: 5300 , cost :  0.470828 , training accuracy :  0.790123\n",
      "i: 5400 , cost :  0.467585 , training accuracy :  0.802469\n",
      "i: 5500 , cost :  0.48127 , training accuracy :  0.800224\n",
      "i: 5600 , cost :  0.467011 , training accuracy :  0.803591\n",
      "i: 5700 , cost :  0.465338 , training accuracy :  0.804714\n",
      "i: 5800 , cost :  0.464083 , training accuracy :  0.802469\n",
      "i: 5900 , cost :  0.464118 , training accuracy :  0.792368\n",
      "i: 6000 , cost :  0.464339 , training accuracy :  0.79349\n",
      "i: 6100 , cost :  0.464386 , training accuracy :  0.792368\n",
      "i: 6200 , cost :  0.466207 , training accuracy :  0.805836\n",
      "i: 6300 , cost :  0.463962 , training accuracy :  0.806958\n",
      "i: 6400 , cost :  0.46053 , training accuracy :  0.796857\n",
      "i: 6500 , cost :  0.458866 , training accuracy :  0.805836\n",
      "i: 6600 , cost :  0.459123 , training accuracy :  0.79798\n",
      "i: 6700 , cost :  0.459699 , training accuracy :  0.795735\n",
      "i: 6800 , cost :  0.457444 , training accuracy :  0.794613\n",
      "i: 6900 , cost :  0.455541 , training accuracy :  0.805836\n",
      "i: 7000 , cost :  0.45507 , training accuracy :  0.808081\n",
      "i: 7100 , cost :  0.453093 , training accuracy :  0.808081\n",
      "i: 7200 , cost :  0.451312 , training accuracy :  0.809203\n",
      "i: 7300 , cost :  0.450894 , training accuracy :  0.795735\n",
      "i: 7400 , cost :  0.45634 , training accuracy :  0.79349\n",
      "i: 7500 , cost :  0.45444 , training accuracy :  0.81257\n",
      "i: 7600 , cost :  0.45249 , training accuracy :  0.808081\n",
      "i: 7700 , cost :  0.450115 , training accuracy :  0.809203\n",
      "i: 7800 , cost :  0.454297 , training accuracy :  0.791246\n",
      "i: 7900 , cost :  0.446512 , training accuracy :  0.804714\n",
      "i: 8000 , cost :  0.453622 , training accuracy :  0.791246\n",
      "i: 8100 , cost :  0.446724 , training accuracy :  0.791246\n",
      "i: 8200 , cost :  0.444745 , training accuracy :  0.81257\n",
      "i: 8300 , cost :  0.446144 , training accuracy :  0.796857\n",
      "i: 8400 , cost :  0.442616 , training accuracy :  0.800224\n",
      "i: 8500 , cost :  0.442361 , training accuracy :  0.810326\n",
      "i: 8600 , cost :  0.441249 , training accuracy :  0.810326\n",
      "i: 8700 , cost :  0.440889 , training accuracy :  0.808081\n",
      "i: 8800 , cost :  0.444286 , training accuracy :  0.79798\n",
      "i: 8900 , cost :  0.441735 , training accuracy :  0.79349\n",
      "i: 9000 , cost :  0.434624 , training accuracy :  0.808081\n",
      "i: 9100 , cost :  0.432986 , training accuracy :  0.811448\n",
      "i: 9200 , cost :  0.435259 , training accuracy :  0.813693\n",
      "i: 9300 , cost :  0.434626 , training accuracy :  0.814815\n",
      "i: 9400 , cost :  0.430653 , training accuracy :  0.81257\n",
      "i: 9500 , cost :  0.432052 , training accuracy :  0.805836\n",
      "i: 9600 , cost :  0.429517 , training accuracy :  0.814815\n",
      "i: 9700 , cost :  0.433215 , training accuracy :  0.815937\n",
      "i: 9800 , cost :  0.429061 , training accuracy :  0.81257\n",
      "i: 9900 , cost :  0.429234 , training accuracy :  0.814815\n",
      "Final training accuracy :  0.817059\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 10000 ][hidden: 10 ][file: bhavul_tr_acc_0.82_prediction.csv ] ACCURACY :  0.817059\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  67.523 , training accuracy :  0.649832\n",
      "i: 100 , cost :  31.6957 , training accuracy :  0.6633\n",
      "i: 200 , cost :  7.47368 , training accuracy :  0.637486\n",
      "i: 300 , cost :  5.3228 , training accuracy :  0.718294\n",
      "i: 400 , cost :  4.45627 , training accuracy :  0.742985\n",
      "i: 500 , cost :  3.82944 , training accuracy :  0.757576\n",
      "i: 600 , cost :  3.35327 , training accuracy :  0.758698\n",
      "i: 700 , cost :  2.9093 , training accuracy :  0.768799\n",
      "i: 800 , cost :  2.50564 , training accuracy :  0.768799\n",
      "i: 900 , cost :  2.17875 , training accuracy :  0.766554\n",
      "i: 1000 , cost :  1.869 , training accuracy :  0.782267\n",
      "i: 1100 , cost :  1.60619 , training accuracy :  0.800224\n",
      "i: 1200 , cost :  1.41735 , training accuracy :  0.799102\n",
      "i: 1300 , cost :  1.26938 , training accuracy :  0.79798\n",
      "i: 1400 , cost :  1.1491 , training accuracy :  0.767677\n",
      "i: 1500 , cost :  1.00195 , training accuracy :  0.799102\n",
      "i: 1600 , cost :  0.902907 , training accuracy :  0.783389\n",
      "i: 1700 , cost :  0.810433 , training accuracy :  0.789001\n",
      "i: 1800 , cost :  0.722529 , training accuracy :  0.783389\n",
      "i: 1900 , cost :  0.6277 , training accuracy :  0.804714\n",
      "i: 2000 , cost :  0.570674 , training accuracy :  0.808081\n",
      "i: 2100 , cost :  0.535282 , training accuracy :  0.805836\n",
      "i: 2200 , cost :  0.518817 , training accuracy :  0.795735\n",
      "i: 2300 , cost :  0.49662 , training accuracy :  0.804714\n",
      "i: 2400 , cost :  0.504211 , training accuracy :  0.796857\n",
      "i: 2500 , cost :  0.484866 , training accuracy :  0.803591\n",
      "i: 2600 , cost :  0.487127 , training accuracy :  0.789001\n",
      "i: 2700 , cost :  0.465258 , training accuracy :  0.806958\n",
      "i: 2800 , cost :  0.461362 , training accuracy :  0.810326\n",
      "i: 2900 , cost :  0.466243 , training accuracy :  0.79349\n",
      "i: 3000 , cost :  0.478173 , training accuracy :  0.79349\n",
      "i: 3100 , cost :  0.47393 , training accuracy :  0.785634\n",
      "i: 3200 , cost :  0.481626 , training accuracy :  0.795735\n",
      "i: 3300 , cost :  0.471118 , training accuracy :  0.785634\n",
      "i: 3400 , cost :  0.472772 , training accuracy :  0.794613\n",
      "i: 3500 , cost :  0.456766 , training accuracy :  0.795735\n",
      "i: 3600 , cost :  0.445413 , training accuracy :  0.817059\n",
      "i: 3700 , cost :  0.443489 , training accuracy :  0.815937\n",
      "i: 3800 , cost :  0.453098 , training accuracy :  0.795735\n",
      "i: 3900 , cost :  0.469425 , training accuracy :  0.804714\n",
      "i: 4000 , cost :  0.460763 , training accuracy :  0.791246\n",
      "i: 4100 , cost :  0.441084 , training accuracy :  0.818182\n",
      "i: 4200 , cost :  0.449156 , training accuracy :  0.813693\n",
      "i: 4300 , cost :  0.463246 , training accuracy :  0.790123\n",
      "i: 4400 , cost :  0.448765 , training accuracy :  0.817059\n",
      "i: 4500 , cost :  0.441097 , training accuracy :  0.817059\n",
      "i: 4600 , cost :  0.459087 , training accuracy :  0.79349\n",
      "i: 4700 , cost :  0.452215 , training accuracy :  0.81257\n",
      "i: 4800 , cost :  0.437777 , training accuracy :  0.823793\n",
      "i: 4900 , cost :  0.457082 , training accuracy :  0.795735\n",
      "i: 5000 , cost :  0.451131 , training accuracy :  0.815937\n",
      "i: 5100 , cost :  0.438286 , training accuracy :  0.819304\n",
      "i: 5200 , cost :  0.457934 , training accuracy :  0.79349\n",
      "i: 5300 , cost :  0.442303 , training accuracy :  0.814815\n",
      "i: 5400 , cost :  0.444342 , training accuracy :  0.814815\n",
      "i: 5500 , cost :  0.458364 , training accuracy :  0.791246\n",
      "i: 5600 , cost :  0.433675 , training accuracy :  0.817059\n",
      "i: 5700 , cost :  0.456371 , training accuracy :  0.810326\n",
      "i: 5800 , cost :  0.447086 , training accuracy :  0.794613\n",
      "i: 5900 , cost :  0.43919 , training accuracy :  0.803591\n",
      "i: 6000 , cost :  0.465518 , training accuracy :  0.810326\n",
      "i: 6100 , cost :  0.43258 , training accuracy :  0.818182\n",
      "i: 6200 , cost :  0.453541 , training accuracy :  0.79798\n",
      "i: 6300 , cost :  0.441336 , training accuracy :  0.813693\n",
      "i: 6400 , cost :  0.444732 , training accuracy :  0.810326\n",
      "i: 6500 , cost :  0.45158 , training accuracy :  0.794613\n",
      "i: 6600 , cost :  0.434004 , training accuracy :  0.806958\n",
      "i: 6700 , cost :  0.46229 , training accuracy :  0.810326\n",
      "i: 6800 , cost :  0.430446 , training accuracy :  0.817059\n",
      "i: 6900 , cost :  0.452001 , training accuracy :  0.79798\n",
      "i: 7000 , cost :  0.436616 , training accuracy :  0.819304\n",
      "i: 7100 , cost :  0.446302 , training accuracy :  0.81257\n",
      "i: 7200 , cost :  0.44631 , training accuracy :  0.799102\n",
      "i: 7300 , cost :  0.435188 , training accuracy :  0.802469\n",
      "i: 7400 , cost :  0.460442 , training accuracy :  0.81257\n",
      "i: 7500 , cost :  0.42957 , training accuracy :  0.821549\n",
      "i: 7600 , cost :  0.45313 , training accuracy :  0.795735\n",
      "i: 7700 , cost :  0.430123 , training accuracy :  0.823793\n",
      "i: 7800 , cost :  0.451964 , training accuracy :  0.81257\n",
      "i: 7900 , cost :  0.43818 , training accuracy :  0.800224\n",
      "i: 8000 , cost :  0.440135 , training accuracy :  0.802469\n",
      "i: 8100 , cost :  0.45312 , training accuracy :  0.814815\n",
      "i: 8200 , cost :  0.433035 , training accuracy :  0.821549\n",
      "i: 8300 , cost :  0.453413 , training accuracy :  0.796857\n",
      "i: 8400 , cost :  0.428321 , training accuracy :  0.818182\n",
      "i: 8500 , cost :  0.457267 , training accuracy :  0.809203\n",
      "i: 8600 , cost :  0.430927 , training accuracy :  0.806958\n",
      "i: 8700 , cost :  0.446247 , training accuracy :  0.800224\n",
      "i: 8800 , cost :  0.440687 , training accuracy :  0.814815\n",
      "i: 8900 , cost :  0.441259 , training accuracy :  0.815937\n",
      "i: 9000 , cost :  0.447422 , training accuracy :  0.799102\n",
      "i: 9100 , cost :  0.432505 , training accuracy :  0.806958\n",
      "i: 9200 , cost :  0.459227 , training accuracy :  0.813693\n",
      "i: 9300 , cost :  0.428038 , training accuracy :  0.821549\n",
      "i: 9400 , cost :  0.451606 , training accuracy :  0.799102\n",
      "i: 9500 , cost :  0.429414 , training accuracy :  0.828283\n",
      "i: 9600 , cost :  0.450862 , training accuracy :  0.813693\n",
      "i: 9700 , cost :  0.436694 , training accuracy :  0.801347\n",
      "i: 9800 , cost :  0.440216 , training accuracy :  0.803591\n",
      "i: 9900 , cost :  0.450488 , training accuracy :  0.813693\n",
      "Final training accuracy :  0.826038\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 10000 ][hidden: 15 ][file: bhavul_tr_acc_0.83_prediction.csv ] ACCURACY :  0.826038\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  420.073 , training accuracy :  0.621773\n",
      "i: 100 , cost :  29.6352 , training accuracy :  0.680135\n",
      "i: 200 , cost :  11.6243 , training accuracy :  0.653199\n",
      "i: 300 , cost :  6.28724 , training accuracy :  0.681257\n",
      "i: 400 , cost :  4.23317 , training accuracy :  0.741863\n",
      "i: 500 , cost :  3.05589 , training accuracy :  0.76431\n",
      "i: 600 , cost :  2.46602 , training accuracy :  0.757576\n",
      "i: 700 , cost :  2.13453 , training accuracy :  0.771044\n",
      "i: 800 , cost :  1.76006 , training accuracy :  0.766554\n",
      "i: 900 , cost :  1.58565 , training accuracy :  0.772166\n",
      "i: 1000 , cost :  1.81903 , training accuracy :  0.739618\n",
      "i: 1100 , cost :  1.33262 , training accuracy :  0.766554\n",
      "i: 1200 , cost :  1.16316 , training accuracy :  0.780022\n",
      "i: 1300 , cost :  1.78151 , training accuracy :  0.575758\n",
      "i: 1400 , cost :  0.929362 , training accuracy :  0.785634\n",
      "i: 1500 , cost :  0.922562 , training accuracy :  0.74523\n",
      "i: 1600 , cost :  1.30059 , training accuracy :  0.758698\n",
      "i: 1700 , cost :  0.860083 , training accuracy :  0.755331\n",
      "i: 1800 , cost :  0.710983 , training accuracy :  0.79349\n",
      "i: 1900 , cost :  0.679925 , training accuracy :  0.79349\n",
      "i: 2000 , cost :  0.670888 , training accuracy :  0.808081\n",
      "i: 2100 , cost :  0.645585 , training accuracy :  0.805836\n",
      "i: 2200 , cost :  0.620007 , training accuracy :  0.810326\n",
      "i: 2300 , cost :  0.647922 , training accuracy :  0.802469\n",
      "i: 2400 , cost :  0.583623 , training accuracy :  0.785634\n",
      "i: 2500 , cost :  0.579939 , training accuracy :  0.81257\n",
      "i: 2600 , cost :  0.539065 , training accuracy :  0.802469\n",
      "i: 2700 , cost :  0.533777 , training accuracy :  0.794613\n",
      "i: 2800 , cost :  0.539907 , training accuracy :  0.818182\n",
      "i: 2900 , cost :  0.499676 , training accuracy :  0.79798\n",
      "i: 3000 , cost :  0.488185 , training accuracy :  0.822671\n",
      "i: 3100 , cost :  0.479175 , training accuracy :  0.826038\n",
      "i: 3200 , cost :  0.477433 , training accuracy :  0.821549\n",
      "i: 3300 , cost :  0.564808 , training accuracy :  0.768799\n",
      "i: 3400 , cost :  0.554906 , training accuracy :  0.771044\n",
      "i: 3500 , cost :  0.460883 , training accuracy :  0.810326\n",
      "i: 3600 , cost :  0.589727 , training accuracy :  0.79349\n",
      "i: 3700 , cost :  0.505194 , training accuracy :  0.785634\n",
      "i: 3800 , cost :  0.495306 , training accuracy :  0.785634\n",
      "i: 3900 , cost :  0.443219 , training accuracy :  0.838384\n",
      "i: 4000 , cost :  0.515564 , training accuracy :  0.810326\n",
      "i: 4100 , cost :  0.436106 , training accuracy :  0.836139\n",
      "i: 4200 , cost :  0.422985 , training accuracy :  0.829405\n",
      "i: 4300 , cost :  0.476937 , training accuracy :  0.826038\n",
      "i: 4400 , cost :  0.420647 , training accuracy :  0.833894\n",
      "i: 4500 , cost :  0.494537 , training accuracy :  0.790123\n",
      "i: 4600 , cost :  0.418834 , training accuracy :  0.835017\n",
      "i: 4700 , cost :  0.415816 , training accuracy :  0.829405\n",
      "i: 4800 , cost :  0.440047 , training accuracy :  0.815937\n",
      "i: 4900 , cost :  0.488343 , training accuracy :  0.795735\n",
      "i: 5000 , cost :  0.567736 , training accuracy :  0.767677\n",
      "i: 5100 , cost :  0.549386 , training accuracy :  0.805836\n",
      "i: 5200 , cost :  0.454691 , training accuracy :  0.833894\n",
      "i: 5300 , cost :  0.475779 , training accuracy :  0.828283\n",
      "i: 5400 , cost :  0.557468 , training accuracy :  0.79798\n",
      "i: 5500 , cost :  0.416086 , training accuracy :  0.843996\n",
      "i: 5600 , cost :  0.458594 , training accuracy :  0.815937\n",
      "i: 5700 , cost :  0.512559 , training accuracy :  0.820426\n",
      "i: 5800 , cost :  0.429428 , training accuracy :  0.849607\n",
      "i: 5900 , cost :  0.447146 , training accuracy :  0.813693\n",
      "i: 6000 , cost :  0.401136 , training accuracy :  0.836139\n",
      "i: 6100 , cost :  0.403222 , training accuracy :  0.828283\n",
      "i: 6200 , cost :  0.458872 , training accuracy :  0.830527\n",
      "i: 6300 , cost :  0.499524 , training accuracy :  0.79349\n",
      "i: 6400 , cost :  0.415277 , training accuracy :  0.82716\n",
      "i: 6500 , cost :  0.568934 , training accuracy :  0.799102\n",
      "i: 6600 , cost :  0.441814 , training accuracy :  0.839506\n",
      "i: 6700 , cost :  0.482501 , training accuracy :  0.801347\n",
      "i: 6800 , cost :  0.442605 , training accuracy :  0.817059\n",
      "i: 6900 , cost :  0.407061 , training accuracy :  0.84624\n",
      "i: 7000 , cost :  0.462288 , training accuracy :  0.806958\n",
      "i: 7100 , cost :  0.531403 , training accuracy :  0.810326\n",
      "i: 7200 , cost :  0.593684 , training accuracy :  0.795735\n",
      "i: 7300 , cost :  0.526156 , training accuracy :  0.783389\n",
      "i: 7400 , cost :  0.396454 , training accuracy :  0.843996\n",
      "i: 7500 , cost :  2.08895 , training accuracy :  0.735129\n",
      "i: 7600 , cost :  0.391005 , training accuracy :  0.849607\n",
      "i: 7700 , cost :  0.476662 , training accuracy :  0.801347\n",
      "i: 7800 , cost :  0.388387 , training accuracy :  0.837261\n",
      "i: 7900 , cost :  0.388086 , training accuracy :  0.835017\n",
      "i: 8000 , cost :  0.471916 , training accuracy :  0.804714\n",
      "i: 8100 , cost :  0.571085 , training accuracy :  0.801347\n",
      "i: 8200 , cost :  0.393881 , training accuracy :  0.837261\n",
      "i: 8300 , cost :  0.393657 , training accuracy :  0.832772\n",
      "i: 8400 , cost :  0.413279 , training accuracy :  0.848485\n",
      "i: 8500 , cost :  0.38991 , training accuracy :  0.836139\n",
      "i: 8600 , cost :  0.395913 , training accuracy :  0.835017\n",
      "i: 8700 , cost :  0.385214 , training accuracy :  0.837261\n",
      "i: 8800 , cost :  0.421069 , training accuracy :  0.841751\n",
      "i: 8900 , cost :  0.449185 , training accuracy :  0.83165\n",
      "i: 9000 , cost :  0.43632 , training accuracy :  0.824916\n",
      "i: 9100 , cost :  0.661141 , training accuracy :  0.767677\n",
      "i: 9200 , cost :  0.412089 , training accuracy :  0.85073\n",
      "i: 9300 , cost :  0.379865 , training accuracy :  0.84624\n",
      "i: 9400 , cost :  0.483446 , training accuracy :  0.800224\n",
      "i: 9500 , cost :  0.495173 , training accuracy :  0.79349\n",
      "i: 9600 , cost :  0.430939 , training accuracy :  0.835017\n",
      "i: 9700 , cost :  0.442282 , training accuracy :  0.833894\n",
      "i: 9800 , cost :  0.475939 , training accuracy :  0.802469\n",
      "i: 9900 , cost :  0.398373 , training accuracy :  0.848485\n",
      "Final training accuracy :  0.818182\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 10000 ][hidden: 50 ][file: bhavul_tr_acc_0.82_prediction.csv ] ACCURACY :  0.818182\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1310.18 , training accuracy :  0.616162\n",
      "i: 100 , cost :  78.9051 , training accuracy :  0.602694\n",
      "i: 200 , cost :  17.1699 , training accuracy :  0.640853\n",
      "i: 300 , cost :  6.14673 , training accuracy :  0.709315\n",
      "i: 400 , cost :  3.57345 , training accuracy :  0.756453\n",
      "i: 500 , cost :  2.58956 , training accuracy :  0.773288\n",
      "i: 600 , cost :  2.0492 , training accuracy :  0.769921\n",
      "i: 700 , cost :  1.73119 , training accuracy :  0.784512\n",
      "i: 800 , cost :  1.51361 , training accuracy :  0.786756\n",
      "i: 900 , cost :  1.45377 , training accuracy :  0.774411\n",
      "i: 1000 , cost :  1.25971 , training accuracy :  0.787879\n",
      "i: 1100 , cost :  1.08886 , training accuracy :  0.802469\n",
      "i: 1200 , cost :  1.13365 , training accuracy :  0.784512\n",
      "i: 1300 , cost :  0.988835 , training accuracy :  0.808081\n",
      "i: 1400 , cost :  0.893662 , training accuracy :  0.814815\n",
      "i: 1500 , cost :  1.02622 , training accuracy :  0.774411\n",
      "i: 1600 , cost :  1.19788 , training accuracy :  0.789001\n",
      "i: 1700 , cost :  0.832201 , training accuracy :  0.837261\n",
      "i: 1800 , cost :  0.736931 , training accuracy :  0.830527\n",
      "i: 1900 , cost :  0.847159 , training accuracy :  0.782267\n",
      "i: 2000 , cost :  0.688283 , training accuracy :  0.828283\n",
      "i: 2100 , cost :  0.767226 , training accuracy :  0.806958\n",
      "i: 2200 , cost :  1.0113 , training accuracy :  0.803591\n",
      "i: 2300 , cost :  0.809888 , training accuracy :  0.819304\n",
      "i: 2400 , cost :  0.622972 , training accuracy :  0.847363\n",
      "i: 2500 , cost :  0.684463 , training accuracy :  0.801347\n",
      "i: 2600 , cost :  0.546815 , training accuracy :  0.843996\n",
      "i: 2700 , cost :  0.688767 , training accuracy :  0.83165\n",
      "i: 2800 , cost :  0.541598 , training accuracy :  0.845118\n",
      "i: 2900 , cost :  1.92155 , training accuracy :  0.790123\n",
      "i: 3000 , cost :  0.524888 , training accuracy :  0.842873\n",
      "i: 3100 , cost :  0.538711 , training accuracy :  0.83165\n",
      "i: 3200 , cost :  1.99157 , training accuracy :  0.601571\n",
      "i: 3300 , cost :  0.517142 , training accuracy :  0.851852\n",
      "i: 3400 , cost :  0.472319 , training accuracy :  0.855219\n",
      "i: 3500 , cost :  0.497407 , training accuracy :  0.83165\n",
      "i: 3600 , cost :  0.518963 , training accuracy :  0.823793\n",
      "i: 3700 , cost :  0.530089 , training accuracy :  0.839506\n",
      "i: 3800 , cost :  0.449436 , training accuracy :  0.859708\n",
      "i: 3900 , cost :  0.567785 , training accuracy :  0.848485\n",
      "i: 4000 , cost :  0.891572 , training accuracy :  0.832772\n",
      "i: 4100 , cost :  0.452435 , training accuracy :  0.861953\n",
      "i: 4200 , cost :  0.784904 , training accuracy :  0.82716\n",
      "i: 4300 , cost :  0.420286 , training accuracy :  0.860831\n",
      "i: 4400 , cost :  0.492966 , training accuracy :  0.819304\n",
      "i: 4500 , cost :  0.44937 , training accuracy :  0.868687\n",
      "i: 4600 , cost :  0.492808 , training accuracy :  0.845118\n",
      "i: 4700 , cost :  0.459527 , training accuracy :  0.856341\n",
      "i: 4800 , cost :  0.423001 , training accuracy :  0.85073\n",
      "i: 4900 , cost :  0.53832 , training accuracy :  0.863075\n",
      "i: 5000 , cost :  0.39853 , training accuracy :  0.86532\n",
      "i: 5100 , cost :  0.389786 , training accuracy :  0.866442\n",
      "i: 5200 , cost :  0.47001 , training accuracy :  0.85073\n",
      "i: 5300 , cost :  0.49952 , training accuracy :  0.845118\n",
      "i: 5400 , cost :  0.875329 , training accuracy :  0.826038\n",
      "i: 5500 , cost :  0.417379 , training accuracy :  0.873176\n",
      "i: 5600 , cost :  0.371823 , training accuracy :  0.868687\n",
      "i: 5700 , cost :  1.33237 , training accuracy :  0.828283\n",
      "i: 5800 , cost :  0.496829 , training accuracy :  0.873176\n",
      "i: 5900 , cost :  0.372635 , training accuracy :  0.869809\n",
      "i: 6000 , cost :  0.360188 , training accuracy :  0.868687\n",
      "i: 6100 , cost :  0.393614 , training accuracy :  0.854097\n",
      "i: 6200 , cost :  0.382263 , training accuracy :  0.868687\n",
      "i: 6300 , cost :  0.38822 , training accuracy :  0.874299\n",
      "i: 6400 , cost :  0.370956 , training accuracy :  0.870932\n",
      "i: 6500 , cost :  0.39357 , training accuracy :  0.852974\n",
      "i: 6600 , cost :  0.406392 , training accuracy :  0.847363\n",
      "i: 6700 , cost :  0.386677 , training accuracy :  0.855219\n",
      "i: 6800 , cost :  0.416596 , training accuracy :  0.849607\n",
      "i: 6900 , cost :  1.40051 , training accuracy :  0.814815\n",
      "i: 7000 , cost :  0.359626 , training accuracy :  0.875421\n",
      "i: 7100 , cost :  0.925361 , training accuracy :  0.836139\n",
      "i: 7200 , cost :  0.559463 , training accuracy :  0.863075\n",
      "i: 7300 , cost :  0.354246 , training accuracy :  0.877666\n",
      "i: 7400 , cost :  0.334443 , training accuracy :  0.875421\n",
      "i: 7500 , cost :  0.332223 , training accuracy :  0.874299\n",
      "i: 7600 , cost :  0.547483 , training accuracy :  0.845118\n",
      "i: 7700 , cost :  0.358536 , training accuracy :  0.864198\n",
      "i: 7800 , cost :  0.33503 , training accuracy :  0.877666\n",
      "i: 7900 , cost :  0.68395 , training accuracy :  0.82716\n",
      "i: 8000 , cost :  0.34111 , training accuracy :  0.87991\n",
      "i: 8100 , cost :  0.331161 , training accuracy :  0.875421\n",
      "i: 8200 , cost :  2.3435 , training accuracy :  0.782267\n",
      "i: 8300 , cost :  0.88755 , training accuracy :  0.840629\n",
      "i: 8400 , cost :  0.370551 , training accuracy :  0.881033\n",
      "i: 8500 , cost :  0.322878 , training accuracy :  0.874299\n",
      "i: 8600 , cost :  0.350886 , training accuracy :  0.874299\n",
      "i: 8700 , cost :  0.316594 , training accuracy :  0.877666\n",
      "i: 8800 , cost :  3.23148 , training accuracy :  0.754209\n",
      "i: 8900 , cost :  0.356154 , training accuracy :  0.882155\n",
      "i: 9000 , cost :  0.313588 , training accuracy :  0.875421\n",
      "i: 9100 , cost :  0.310929 , training accuracy :  0.877666\n",
      "i: 9200 , cost :  3.10035 , training accuracy :  0.511784\n",
      "i: 9300 , cost :  0.327972 , training accuracy :  0.882155\n",
      "i: 9400 , cost :  0.307858 , training accuracy :  0.876543\n",
      "i: 9500 , cost :  0.363101 , training accuracy :  0.866442\n",
      "i: 9600 , cost :  0.616664 , training accuracy :  0.858586\n",
      "i: 9700 , cost :  0.348127 , training accuracy :  0.881033\n",
      "i: 9800 , cost :  0.307845 , training accuracy :  0.882155\n",
      "i: 9900 , cost :  0.366191 , training accuracy :  0.856341\n",
      "Final training accuracy :  0.878788\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 10000 ][hidden: 100 ][file: bhavul_tr_acc_0.88_prediction.csv ] ACCURACY :  0.878788\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  153.954 , training accuracy :  0.618406\n",
      "i: 100 , cost :  74.0788 , training accuracy :  0.666667\n",
      "i: 200 , cost :  13.9223 , training accuracy :  0.686869\n",
      "i: 300 , cost :  6.03163 , training accuracy :  0.628507\n",
      "i: 400 , cost :  5.02444 , training accuracy :  0.631874\n",
      "i: 500 , cost :  4.1916 , training accuracy :  0.640853\n",
      "i: 600 , cost :  3.41509 , training accuracy :  0.646465\n",
      "i: 700 , cost :  2.68757 , training accuracy :  0.643098\n",
      "i: 800 , cost :  2.00799 , training accuracy :  0.636364\n",
      "i: 900 , cost :  1.45244 , training accuracy :  0.653199\n",
      "i: 1000 , cost :  1.10656 , training accuracy :  0.670034\n",
      "i: 1100 , cost :  0.895514 , training accuracy :  0.673401\n",
      "i: 1200 , cost :  0.778507 , training accuracy :  0.659933\n",
      "i: 1300 , cost :  0.719191 , training accuracy :  0.673401\n",
      "i: 1400 , cost :  0.682591 , training accuracy :  0.676768\n",
      "i: 1500 , cost :  0.652568 , training accuracy :  0.67789\n",
      "i: 1600 , cost :  0.626685 , training accuracy :  0.691358\n",
      "i: 1700 , cost :  0.604604 , training accuracy :  0.695847\n",
      "i: 1800 , cost :  0.586184 , training accuracy :  0.703704\n",
      "i: 1900 , cost :  0.570864 , training accuracy :  0.707071\n",
      "i: 2000 , cost :  0.558576 , training accuracy :  0.718294\n",
      "i: 2100 , cost :  0.548705 , training accuracy :  0.722783\n",
      "i: 2200 , cost :  0.540787 , training accuracy :  0.727273\n",
      "i: 2300 , cost :  0.534401 , training accuracy :  0.738496\n",
      "i: 2400 , cost :  0.529228 , training accuracy :  0.738496\n",
      "i: 2500 , cost :  0.525009 , training accuracy :  0.739618\n",
      "i: 2600 , cost :  0.521557 , training accuracy :  0.739618\n",
      "i: 2700 , cost :  0.518734 , training accuracy :  0.739618\n",
      "i: 2800 , cost :  0.516158 , training accuracy :  0.738496\n",
      "i: 2900 , cost :  0.514129 , training accuracy :  0.744108\n",
      "i: 3000 , cost :  0.512462 , training accuracy :  0.74523\n",
      "i: 3100 , cost :  0.51034 , training accuracy :  0.748597\n",
      "i: 3200 , cost :  0.508931 , training accuracy :  0.756453\n",
      "i: 3300 , cost :  0.507043 , training accuracy :  0.757576\n",
      "i: 3400 , cost :  0.505955 , training accuracy :  0.755331\n",
      "i: 3500 , cost :  0.504966 , training accuracy :  0.756453\n",
      "i: 3600 , cost :  0.503792 , training accuracy :  0.755331\n",
      "i: 3700 , cost :  0.502997 , training accuracy :  0.755331\n",
      "i: 3800 , cost :  0.502268 , training accuracy :  0.756453\n",
      "i: 3900 , cost :  0.501584 , training accuracy :  0.755331\n",
      "i: 4000 , cost :  0.500936 , training accuracy :  0.755331\n",
      "i: 4100 , cost :  0.500342 , training accuracy :  0.756453\n",
      "i: 4200 , cost :  0.499762 , training accuracy :  0.756453\n",
      "i: 4300 , cost :  0.499252 , training accuracy :  0.755331\n",
      "i: 4400 , cost :  0.498755 , training accuracy :  0.754209\n",
      "i: 4500 , cost :  0.498267 , training accuracy :  0.754209\n",
      "i: 4600 , cost :  0.49779 , training accuracy :  0.755331\n",
      "i: 4700 , cost :  0.497319 , training accuracy :  0.756453\n",
      "i: 4800 , cost :  0.497008 , training accuracy :  0.755331\n",
      "i: 4900 , cost :  0.496427 , training accuracy :  0.755331\n",
      "i: 5000 , cost :  0.496148 , training accuracy :  0.749719\n",
      "i: 5100 , cost :  0.496718 , training accuracy :  0.754209\n",
      "i: 5200 , cost :  0.496296 , training accuracy :  0.763187\n",
      "i: 5300 , cost :  0.494879 , training accuracy :  0.755331\n",
      "i: 5400 , cost :  0.494646 , training accuracy :  0.756453\n",
      "i: 5500 , cost :  0.494857 , training accuracy :  0.751964\n",
      "i: 5600 , cost :  0.494177 , training accuracy :  0.754209\n",
      "i: 5700 , cost :  0.494692 , training accuracy :  0.763187\n",
      "i: 5800 , cost :  0.493319 , training accuracy :  0.753086\n",
      "i: 5900 , cost :  0.495643 , training accuracy :  0.747475\n",
      "i: 6000 , cost :  0.49437 , training accuracy :  0.751964\n",
      "i: 6100 , cost :  0.493499 , training accuracy :  0.750842\n",
      "i: 6200 , cost :  0.492385 , training accuracy :  0.755331\n",
      "i: 6300 , cost :  0.492274 , training accuracy :  0.755331\n",
      "i: 6400 , cost :  0.49205 , training accuracy :  0.754209\n",
      "i: 6500 , cost :  0.491656 , training accuracy :  0.755331\n",
      "i: 6600 , cost :  0.491475 , training accuracy :  0.756453\n",
      "i: 6700 , cost :  0.491318 , training accuracy :  0.755331\n",
      "i: 6800 , cost :  0.491151 , training accuracy :  0.755331\n",
      "i: 6900 , cost :  0.491015 , training accuracy :  0.756453\n",
      "i: 7000 , cost :  0.490892 , training accuracy :  0.754209\n",
      "i: 7100 , cost :  0.490782 , training accuracy :  0.756453\n",
      "i: 7200 , cost :  0.490644 , training accuracy :  0.756453\n",
      "i: 7300 , cost :  0.490566 , training accuracy :  0.757576\n",
      "i: 7400 , cost :  0.490422 , training accuracy :  0.756453\n",
      "i: 7500 , cost :  0.490342 , training accuracy :  0.755331\n",
      "i: 7600 , cost :  0.490294 , training accuracy :  0.751964\n",
      "i: 7700 , cost :  0.490385 , training accuracy :  0.760943\n",
      "i: 7800 , cost :  0.490089 , training accuracy :  0.755331\n",
      "i: 7900 , cost :  0.490061 , training accuracy :  0.755331\n",
      "i: 8000 , cost :  0.490123 , training accuracy :  0.753086\n",
      "i: 8100 , cost :  0.489914 , training accuracy :  0.755331\n",
      "i: 8200 , cost :  0.491785 , training accuracy :  0.74523\n",
      "i: 8300 , cost :  0.490488 , training accuracy :  0.747475\n",
      "i: 8400 , cost :  0.490008 , training accuracy :  0.75982\n",
      "i: 8500 , cost :  0.489734 , training accuracy :  0.757576\n",
      "i: 8600 , cost :  0.490226 , training accuracy :  0.760943\n",
      "i: 8700 , cost :  0.489771 , training accuracy :  0.758698\n",
      "i: 8800 , cost :  0.489607 , training accuracy :  0.751964\n",
      "i: 8900 , cost :  0.490454 , training accuracy :  0.763187\n",
      "i: 9000 , cost :  0.489724 , training accuracy :  0.748597\n",
      "i: 9100 , cost :  0.489539 , training accuracy :  0.753086\n",
      "i: 9200 , cost :  0.489541 , training accuracy :  0.757576\n",
      "i: 9300 , cost :  0.48949 , training accuracy :  0.754209\n",
      "i: 9400 , cost :  0.489469 , training accuracy :  0.754209\n",
      "i: 9500 , cost :  0.489448 , training accuracy :  0.753086\n",
      "i: 9600 , cost :  0.489448 , training accuracy :  0.757576\n",
      "i: 9700 , cost :  0.490701 , training accuracy :  0.746352\n",
      "i: 9800 , cost :  0.489699 , training accuracy :  0.75982\n",
      "i: 9900 , cost :  0.489526 , training accuracy :  0.749719\n",
      "i: 10000 , cost :  0.489544 , training accuracy :  0.75982\n",
      "i: 10100 , cost :  0.489497 , training accuracy :  0.748597\n",
      "i: 10200 , cost :  0.49005 , training accuracy :  0.746352\n",
      "i: 10300 , cost :  0.489577 , training accuracy :  0.748597\n",
      "i: 10400 , cost :  0.489393 , training accuracy :  0.758698\n",
      "i: 10500 , cost :  0.49094 , training accuracy :  0.762065\n",
      "i: 10600 , cost :  0.489519 , training accuracy :  0.748597\n",
      "i: 10700 , cost :  0.489321 , training accuracy :  0.750842\n",
      "i: 10800 , cost :  0.489555 , training accuracy :  0.748597\n",
      "i: 10900 , cost :  0.489324 , training accuracy :  0.757576\n",
      "i: 11000 , cost :  0.489326 , training accuracy :  0.757576\n",
      "i: 11100 , cost :  0.489341 , training accuracy :  0.750842\n",
      "i: 11200 , cost :  0.490969 , training accuracy :  0.760943\n",
      "i: 11300 , cost :  0.489621 , training accuracy :  0.747475\n",
      "i: 11400 , cost :  0.489378 , training accuracy :  0.748597\n",
      "i: 11500 , cost :  0.489346 , training accuracy :  0.749719\n",
      "i: 11600 , cost :  0.48937 , training accuracy :  0.758698\n",
      "i: 11700 , cost :  0.489307 , training accuracy :  0.750842\n",
      "i: 11800 , cost :  0.489439 , training accuracy :  0.748597\n",
      "i: 11900 , cost :  0.489558 , training accuracy :  0.748597\n",
      "i: 12000 , cost :  0.490584 , training accuracy :  0.765432\n",
      "i: 12100 , cost :  0.489286 , training accuracy :  0.757576\n",
      "i: 12200 , cost :  0.490369 , training accuracy :  0.765432\n",
      "i: 12300 , cost :  0.489383 , training accuracy :  0.748597\n",
      "i: 12400 , cost :  0.489466 , training accuracy :  0.75982\n",
      "i: 12500 , cost :  0.489352 , training accuracy :  0.748597\n",
      "i: 12600 , cost :  0.48934 , training accuracy :  0.758698\n",
      "i: 12700 , cost :  0.489662 , training accuracy :  0.746352\n",
      "i: 12800 , cost :  0.489394 , training accuracy :  0.748597\n",
      "i: 12900 , cost :  0.489439 , training accuracy :  0.748597\n",
      "i: 13000 , cost :  0.489687 , training accuracy :  0.762065\n",
      "i: 13100 , cost :  0.489501 , training accuracy :  0.760943\n",
      "i: 13200 , cost :  0.489236 , training accuracy :  0.748597\n",
      "i: 13300 , cost :  0.490518 , training accuracy :  0.744108\n",
      "i: 13400 , cost :  0.489267 , training accuracy :  0.758698\n",
      "i: 13500 , cost :  0.489216 , training accuracy :  0.757576\n",
      "i: 13600 , cost :  0.489249 , training accuracy :  0.758698\n",
      "i: 13700 , cost :  0.489448 , training accuracy :  0.760943\n",
      "i: 13800 , cost :  0.489246 , training accuracy :  0.758698\n",
      "i: 13900 , cost :  0.490173 , training accuracy :  0.76431\n",
      "i: 14000 , cost :  0.489193 , training accuracy :  0.756453\n",
      "i: 14100 , cost :  0.489363 , training accuracy :  0.749719\n",
      "i: 14200 , cost :  0.489194 , training accuracy :  0.749719\n",
      "i: 14300 , cost :  0.489262 , training accuracy :  0.758698\n",
      "i: 14400 , cost :  0.489201 , training accuracy :  0.757576\n",
      "i: 14500 , cost :  0.489336 , training accuracy :  0.749719\n",
      "i: 14600 , cost :  0.489231 , training accuracy :  0.750842\n",
      "i: 14700 , cost :  0.489987 , training accuracy :  0.763187\n",
      "i: 14800 , cost :  0.489209 , training accuracy :  0.750842\n",
      "i: 14900 , cost :  0.489164 , training accuracy :  0.753086\n",
      "i: 15000 , cost :  0.489203 , training accuracy :  0.749719\n",
      "i: 15100 , cost :  0.489259 , training accuracy :  0.758698\n",
      "i: 15200 , cost :  0.489248 , training accuracy :  0.750842\n",
      "i: 15300 , cost :  0.489676 , training accuracy :  0.747475\n",
      "i: 15400 , cost :  0.489159 , training accuracy :  0.756453\n",
      "i: 15500 , cost :  0.48937 , training accuracy :  0.760943\n",
      "i: 15600 , cost :  0.489188 , training accuracy :  0.750842\n",
      "i: 15700 , cost :  0.490275 , training accuracy :  0.766554\n",
      "i: 15800 , cost :  0.489146 , training accuracy :  0.755331\n",
      "i: 15900 , cost :  0.489139 , training accuracy :  0.753086\n",
      "i: 16000 , cost :  0.489187 , training accuracy :  0.758698\n",
      "i: 16100 , cost :  0.489253 , training accuracy :  0.749719\n",
      "i: 16200 , cost :  0.489276 , training accuracy :  0.749719\n",
      "i: 16300 , cost :  0.48936 , training accuracy :  0.749719\n",
      "i: 16400 , cost :  0.489474 , training accuracy :  0.748597\n",
      "i: 16500 , cost :  0.489489 , training accuracy :  0.748597\n",
      "i: 16600 , cost :  0.490019 , training accuracy :  0.76431\n",
      "i: 16700 , cost :  0.48912 , training accuracy :  0.751964\n",
      "i: 16800 , cost :  0.48917 , training accuracy :  0.758698\n",
      "i: 16900 , cost :  0.489189 , training accuracy :  0.750842\n",
      "i: 17000 , cost :  0.491539 , training accuracy :  0.760943\n",
      "i: 17100 , cost :  0.490646 , training accuracy :  0.746352\n",
      "i: 17200 , cost :  0.489269 , training accuracy :  0.760943\n",
      "i: 17300 , cost :  0.489153 , training accuracy :  0.750842\n",
      "i: 17400 , cost :  0.489279 , training accuracy :  0.749719\n",
      "i: 17500 , cost :  0.489389 , training accuracy :  0.760943\n",
      "i: 17600 , cost :  0.489305 , training accuracy :  0.760943\n",
      "i: 17700 , cost :  0.489374 , training accuracy :  0.748597\n",
      "i: 17800 , cost :  0.48924 , training accuracy :  0.760943\n",
      "i: 17900 , cost :  0.489175 , training accuracy :  0.758698\n",
      "i: 18000 , cost :  0.48916 , training accuracy :  0.758698\n",
      "i: 18100 , cost :  0.48932 , training accuracy :  0.748597\n",
      "i: 18200 , cost :  0.489366 , training accuracy :  0.760943\n",
      "i: 18300 , cost :  0.489083 , training accuracy :  0.751964\n",
      "i: 18400 , cost :  0.493055 , training accuracy :  0.741863\n",
      "i: 18500 , cost :  0.489079 , training accuracy :  0.751964\n",
      "i: 18600 , cost :  0.489073 , training accuracy :  0.754209\n",
      "i: 18700 , cost :  0.48947 , training accuracy :  0.747475\n",
      "i: 18800 , cost :  0.48924 , training accuracy :  0.760943\n",
      "i: 18900 , cost :  0.489794 , training accuracy :  0.763187\n",
      "i: 19000 , cost :  0.489502 , training accuracy :  0.762065\n",
      "i: 19100 , cost :  0.489071 , training accuracy :  0.749719\n",
      "i: 19200 , cost :  0.489068 , training accuracy :  0.756453\n",
      "i: 19300 , cost :  0.48909 , training accuracy :  0.748597\n",
      "i: 19400 , cost :  0.489287 , training accuracy :  0.760943\n",
      "i: 19500 , cost :  0.489091 , training accuracy :  0.757576\n",
      "i: 19600 , cost :  0.489771 , training accuracy :  0.763187\n",
      "i: 19700 , cost :  0.490667 , training accuracy :  0.746352\n",
      "i: 19800 , cost :  0.489064 , training accuracy :  0.749719\n",
      "i: 19900 , cost :  0.489051 , training accuracy :  0.756453\n",
      "i: 20000 , cost :  0.489949 , training accuracy :  0.74523\n",
      "i: 20100 , cost :  0.48914 , training accuracy :  0.758698\n",
      "i: 20200 , cost :  0.489066 , training accuracy :  0.748597\n",
      "i: 20300 , cost :  0.489456 , training accuracy :  0.762065\n",
      "i: 20400 , cost :  0.489097 , training accuracy :  0.750842\n",
      "i: 20500 , cost :  0.489102 , training accuracy :  0.758698\n",
      "i: 20600 , cost :  0.4908 , training accuracy :  0.763187\n",
      "i: 20700 , cost :  0.491843 , training accuracy :  0.74523\n",
      "i: 20800 , cost :  0.489026 , training accuracy :  0.751964\n",
      "i: 20900 , cost :  0.48931 , training accuracy :  0.762065\n",
      "i: 21000 , cost :  0.489034 , training accuracy :  0.757576\n",
      "i: 21100 , cost :  0.489082 , training accuracy :  0.758698\n",
      "i: 21200 , cost :  0.48906 , training accuracy :  0.750842\n",
      "i: 21300 , cost :  0.48957 , training accuracy :  0.747475\n",
      "i: 21400 , cost :  0.489338 , training accuracy :  0.762065\n",
      "i: 21500 , cost :  0.489056 , training accuracy :  0.750842\n",
      "i: 21600 , cost :  0.489016 , training accuracy :  0.749719\n",
      "i: 21700 , cost :  0.490435 , training accuracy :  0.765432\n",
      "i: 21800 , cost :  0.489622 , training accuracy :  0.746352\n",
      "i: 21900 , cost :  0.489062 , training accuracy :  0.758698\n",
      "i: 22000 , cost :  0.489163 , training accuracy :  0.760943\n",
      "i: 22100 , cost :  0.48905 , training accuracy :  0.750842\n",
      "i: 22200 , cost :  0.489756 , training accuracy :  0.763187\n",
      "i: 22300 , cost :  0.489024 , training accuracy :  0.757576\n",
      "i: 22400 , cost :  0.489024 , training accuracy :  0.757576\n",
      "i: 22500 , cost :  0.490131 , training accuracy :  0.74523\n",
      "i: 22600 , cost :  0.489002 , training accuracy :  0.749719\n",
      "i: 22700 , cost :  0.488979 , training accuracy :  0.751964\n",
      "i: 22800 , cost :  0.490253 , training accuracy :  0.74523\n",
      "i: 22900 , cost :  0.489091 , training accuracy :  0.75982\n",
      "i: 23000 , cost :  0.489102 , training accuracy :  0.760943\n",
      "i: 23100 , cost :  0.488997 , training accuracy :  0.748597\n",
      "i: 23200 , cost :  0.492475 , training accuracy :  0.75982\n",
      "i: 23300 , cost :  0.49054 , training accuracy :  0.746352\n",
      "i: 23400 , cost :  0.491407 , training accuracy :  0.74523\n",
      "i: 23500 , cost :  0.4892 , training accuracy :  0.762065\n",
      "i: 23600 , cost :  0.488989 , training accuracy :  0.748597\n",
      "i: 23700 , cost :  0.488959 , training accuracy :  0.754209\n",
      "i: 23800 , cost :  0.488951 , training accuracy :  0.754209\n",
      "i: 23900 , cost :  0.489123 , training accuracy :  0.749719\n",
      "i: 24000 , cost :  0.488956 , training accuracy :  0.749719\n",
      "i: 24100 , cost :  0.488945 , training accuracy :  0.754209\n",
      "i: 24200 , cost :  0.488943 , training accuracy :  0.754209\n",
      "i: 24300 , cost :  0.488978 , training accuracy :  0.748597\n",
      "i: 24400 , cost :  0.490436 , training accuracy :  0.763187\n",
      "i: 24500 , cost :  0.489018 , training accuracy :  0.750842\n",
      "i: 24600 , cost :  0.489004 , training accuracy :  0.750842\n",
      "i: 24700 , cost :  0.489734 , training accuracy :  0.746352\n",
      "i: 24800 , cost :  0.489674 , training accuracy :  0.746352\n",
      "i: 24900 , cost :  0.48893 , training accuracy :  0.753086\n",
      "i: 25000 , cost :  0.489018 , training accuracy :  0.750842\n",
      "i: 25100 , cost :  0.489468 , training accuracy :  0.747475\n",
      "i: 25200 , cost :  0.489888 , training accuracy :  0.74523\n",
      "i: 25300 , cost :  0.489062 , training accuracy :  0.760943\n",
      "i: 25400 , cost :  0.488949 , training accuracy :  0.748597\n",
      "i: 25500 , cost :  0.489222 , training accuracy :  0.760943\n",
      "i: 25600 , cost :  0.48908 , training accuracy :  0.749719\n",
      "i: 25700 , cost :  0.489171 , training accuracy :  0.760943\n",
      "i: 25800 , cost :  0.48922 , training accuracy :  0.748597\n",
      "i: 25900 , cost :  0.49126 , training accuracy :  0.760943\n",
      "i: 26000 , cost :  0.489761 , training accuracy :  0.76431\n",
      "i: 26100 , cost :  0.488904 , training accuracy :  0.754209\n",
      "i: 26200 , cost :  0.489012 , training accuracy :  0.749719\n",
      "i: 26300 , cost :  0.488898 , training accuracy :  0.754209\n",
      "i: 26400 , cost :  0.489063 , training accuracy :  0.749719\n",
      "i: 26500 , cost :  0.488895 , training accuracy :  0.754209\n",
      "i: 26600 , cost :  0.488903 , training accuracy :  0.750842\n",
      "i: 26700 , cost :  0.488896 , training accuracy :  0.754209\n",
      "i: 26800 , cost :  0.489082 , training accuracy :  0.760943\n",
      "i: 26900 , cost :  0.4891 , training accuracy :  0.749719\n",
      "i: 27000 , cost :  0.488917 , training accuracy :  0.757576\n",
      "i: 27100 , cost :  0.489155 , training accuracy :  0.748597\n",
      "i: 27200 , cost :  0.489231 , training accuracy :  0.762065\n",
      "i: 27300 , cost :  0.489241 , training accuracy :  0.762065\n",
      "i: 27400 , cost :  0.488988 , training accuracy :  0.75982\n",
      "i: 27500 , cost :  0.491648 , training accuracy :  0.74523\n",
      "i: 27600 , cost :  0.490824 , training accuracy :  0.763187\n",
      "i: 27700 , cost :  0.489031 , training accuracy :  0.760943\n",
      "i: 27800 , cost :  0.488876 , training accuracy :  0.751964\n",
      "i: 27900 , cost :  0.48969 , training accuracy :  0.746352\n",
      "i: 28000 , cost :  0.490006 , training accuracy :  0.74523\n",
      "i: 28100 , cost :  0.488886 , training accuracy :  0.757576\n",
      "i: 28200 , cost :  0.488881 , training accuracy :  0.757576\n",
      "i: 28300 , cost :  0.488858 , training accuracy :  0.754209\n",
      "i: 28400 , cost :  0.488859 , training accuracy :  0.751964\n",
      "i: 28500 , cost :  0.489786 , training accuracy :  0.74523\n",
      "i: 28600 , cost :  0.488918 , training accuracy :  0.758698\n",
      "i: 28700 , cost :  0.488852 , training accuracy :  0.753086\n",
      "i: 28800 , cost :  0.491076 , training accuracy :  0.760943\n",
      "i: 28900 , cost :  0.489295 , training accuracy :  0.762065\n",
      "i: 29000 , cost :  0.488844 , training accuracy :  0.754209\n",
      "i: 29100 , cost :  0.488842 , training accuracy :  0.754209\n",
      "i: 29200 , cost :  0.48894 , training accuracy :  0.75982\n",
      "i: 29300 , cost :  0.488946 , training accuracy :  0.75982\n",
      "i: 29400 , cost :  0.489829 , training accuracy :  0.76431\n",
      "i: 29500 , cost :  0.488908 , training accuracy :  0.758698\n",
      "i: 29600 , cost :  0.488898 , training accuracy :  0.758698\n",
      "i: 29700 , cost :  0.492368 , training accuracy :  0.75982\n",
      "i: 29800 , cost :  0.490272 , training accuracy :  0.765432\n",
      "i: 29900 , cost :  0.489438 , training accuracy :  0.763187\n",
      "Final training accuracy :  0.763187\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 30000 ][hidden: 3 ][file: bhavul_tr_acc_0.76_prediction.csv ] ACCURACY :  0.763187\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  130.023 , training accuracy :  0.397306\n",
      "i: 100 , cost :  13.2062 , training accuracy :  0.657688\n",
      "i: 200 , cost :  5.3761 , training accuracy :  0.672278\n",
      "i: 300 , cost :  1.71021 , training accuracy :  0.671156\n",
      "i: 400 , cost :  1.2759 , training accuracy :  0.698092\n",
      "i: 500 , cost :  1.10364 , training accuracy :  0.742985\n",
      "i: 600 , cost :  0.995028 , training accuracy :  0.758698\n",
      "i: 700 , cost :  0.900242 , training accuracy :  0.75982\n",
      "i: 800 , cost :  0.812684 , training accuracy :  0.769921\n",
      "i: 900 , cost :  0.749566 , training accuracy :  0.784512\n",
      "i: 1000 , cost :  0.69641 , training accuracy :  0.782267\n",
      "i: 1100 , cost :  0.641275 , training accuracy :  0.776655\n",
      "i: 1200 , cost :  0.60909 , training accuracy :  0.789001\n",
      "i: 1300 , cost :  0.581883 , training accuracy :  0.792368\n",
      "i: 1400 , cost :  0.55796 , training accuracy :  0.79349\n",
      "i: 1500 , cost :  0.54697 , training accuracy :  0.782267\n",
      "i: 1600 , cost :  0.52332 , training accuracy :  0.79798\n",
      "i: 1700 , cost :  0.513874 , training accuracy :  0.803591\n",
      "i: 1800 , cost :  0.49901 , training accuracy :  0.803591\n",
      "i: 1900 , cost :  0.495743 , training accuracy :  0.803591\n",
      "i: 2000 , cost :  0.477785 , training accuracy :  0.795735\n",
      "i: 2100 , cost :  0.471859 , training accuracy :  0.805836\n",
      "i: 2200 , cost :  0.471997 , training accuracy :  0.786756\n",
      "i: 2300 , cost :  0.473022 , training accuracy :  0.782267\n",
      "i: 2400 , cost :  0.465511 , training accuracy :  0.811448\n",
      "i: 2500 , cost :  0.464144 , training accuracy :  0.809203\n",
      "i: 2600 , cost :  0.449 , training accuracy :  0.803591\n",
      "i: 2700 , cost :  0.450615 , training accuracy :  0.813693\n",
      "i: 2800 , cost :  0.450495 , training accuracy :  0.790123\n",
      "i: 2900 , cost :  0.464053 , training accuracy :  0.805836\n",
      "i: 3000 , cost :  0.444495 , training accuracy :  0.79798\n",
      "i: 3100 , cost :  0.442545 , training accuracy :  0.809203\n",
      "i: 3200 , cost :  0.461013 , training accuracy :  0.806958\n",
      "i: 3300 , cost :  0.439613 , training accuracy :  0.805836\n",
      "i: 3400 , cost :  0.453516 , training accuracy :  0.805836\n",
      "i: 3500 , cost :  0.438244 , training accuracy :  0.805836\n",
      "i: 3600 , cost :  0.437747 , training accuracy :  0.806958\n",
      "i: 3700 , cost :  0.438478 , training accuracy :  0.805836\n",
      "i: 3800 , cost :  0.438584 , training accuracy :  0.810326\n",
      "i: 3900 , cost :  0.444586 , training accuracy :  0.789001\n",
      "i: 4000 , cost :  0.437839 , training accuracy :  0.806958\n",
      "i: 4100 , cost :  0.44568 , training accuracy :  0.813693\n",
      "i: 4200 , cost :  0.441719 , training accuracy :  0.795735\n",
      "i: 4300 , cost :  0.439225 , training accuracy :  0.819304\n",
      "i: 4400 , cost :  0.437377 , training accuracy :  0.806958\n",
      "i: 4500 , cost :  0.450661 , training accuracy :  0.809203\n",
      "i: 4600 , cost :  0.439058 , training accuracy :  0.814815\n",
      "i: 4700 , cost :  0.449962 , training accuracy :  0.809203\n",
      "i: 4800 , cost :  0.437195 , training accuracy :  0.805836\n",
      "i: 4900 , cost :  0.438503 , training accuracy :  0.817059\n",
      "i: 5000 , cost :  0.43882 , training accuracy :  0.79798\n",
      "i: 5100 , cost :  0.437172 , training accuracy :  0.804714\n",
      "i: 5200 , cost :  0.456928 , training accuracy :  0.805836\n",
      "i: 5300 , cost :  0.435952 , training accuracy :  0.805836\n",
      "i: 5400 , cost :  0.446234 , training accuracy :  0.786756\n",
      "i: 5500 , cost :  0.437322 , training accuracy :  0.811448\n",
      "i: 5600 , cost :  0.461585 , training accuracy :  0.784512\n",
      "i: 5700 , cost :  0.438577 , training accuracy :  0.813693\n",
      "i: 5800 , cost :  0.452077 , training accuracy :  0.810326\n",
      "i: 5900 , cost :  0.436017 , training accuracy :  0.806958\n",
      "i: 6000 , cost :  0.438433 , training accuracy :  0.817059\n",
      "i: 6100 , cost :  0.43608 , training accuracy :  0.804714\n",
      "i: 6200 , cost :  0.451049 , training accuracy :  0.785634\n",
      "i: 6300 , cost :  0.440412 , training accuracy :  0.792368\n",
      "i: 6400 , cost :  0.439762 , training accuracy :  0.814815\n",
      "i: 6500 , cost :  0.442266 , training accuracy :  0.81257\n",
      "i: 6600 , cost :  0.437298 , training accuracy :  0.800224\n",
      "i: 6700 , cost :  0.435574 , training accuracy :  0.804714\n",
      "i: 6800 , cost :  0.440351 , training accuracy :  0.813693\n",
      "i: 6900 , cost :  0.435972 , training accuracy :  0.806958\n",
      "i: 7000 , cost :  0.439016 , training accuracy :  0.795735\n",
      "i: 7100 , cost :  0.438679 , training accuracy :  0.814815\n",
      "i: 7200 , cost :  0.439701 , training accuracy :  0.79349\n",
      "i: 7300 , cost :  0.437683 , training accuracy :  0.81257\n",
      "i: 7400 , cost :  0.437724 , training accuracy :  0.799102\n",
      "i: 7500 , cost :  0.443577 , training accuracy :  0.787879\n",
      "i: 7600 , cost :  0.443703 , training accuracy :  0.787879\n",
      "i: 7700 , cost :  0.438899 , training accuracy :  0.794613\n",
      "i: 7800 , cost :  0.447792 , training accuracy :  0.810326\n",
      "i: 7900 , cost :  0.437956 , training accuracy :  0.813693\n",
      "i: 8000 , cost :  0.448796 , training accuracy :  0.785634\n",
      "i: 8100 , cost :  0.436933 , training accuracy :  0.801347\n",
      "i: 8200 , cost :  0.44668 , training accuracy :  0.81257\n",
      "i: 8300 , cost :  0.437372 , training accuracy :  0.815937\n",
      "i: 8400 , cost :  0.443417 , training accuracy :  0.787879\n",
      "i: 8500 , cost :  0.43764 , training accuracy :  0.800224\n",
      "i: 8600 , cost :  0.444639 , training accuracy :  0.813693\n",
      "i: 8700 , cost :  0.435747 , training accuracy :  0.804714\n",
      "i: 8800 , cost :  0.450186 , training accuracy :  0.785634\n",
      "i: 8900 , cost :  0.435313 , training accuracy :  0.805836\n",
      "i: 9000 , cost :  0.445528 , training accuracy :  0.814815\n",
      "i: 9100 , cost :  0.435556 , training accuracy :  0.806958\n",
      "i: 9200 , cost :  0.443437 , training accuracy :  0.789001\n",
      "i: 9300 , cost :  0.435864 , training accuracy :  0.806958\n",
      "i: 9400 , cost :  0.441454 , training accuracy :  0.809203\n",
      "i: 9500 , cost :  0.437675 , training accuracy :  0.795735\n",
      "i: 9600 , cost :  0.438501 , training accuracy :  0.795735\n",
      "i: 9700 , cost :  0.444264 , training accuracy :  0.811448\n",
      "i: 9800 , cost :  0.436478 , training accuracy :  0.815937\n",
      "i: 9900 , cost :  0.449933 , training accuracy :  0.785634\n",
      "i: 10000 , cost :  0.435779 , training accuracy :  0.801347\n",
      "i: 10100 , cost :  0.452036 , training accuracy :  0.806958\n",
      "i: 10200 , cost :  0.435131 , training accuracy :  0.803591\n",
      "i: 10300 , cost :  0.44045 , training accuracy :  0.791246\n",
      "i: 10400 , cost :  0.43847 , training accuracy :  0.818182\n",
      "i: 10500 , cost :  0.437761 , training accuracy :  0.820426\n",
      "i: 10600 , cost :  0.448993 , training accuracy :  0.787879\n",
      "i: 10700 , cost :  0.436014 , training accuracy :  0.800224\n",
      "i: 10800 , cost :  0.452036 , training accuracy :  0.805836\n",
      "i: 10900 , cost :  0.435114 , training accuracy :  0.803591\n",
      "i: 11000 , cost :  0.439717 , training accuracy :  0.792368\n",
      "i: 11100 , cost :  0.452315 , training accuracy :  0.806958\n",
      "i: 11200 , cost :  0.436236 , training accuracy :  0.794613\n",
      "i: 11300 , cost :  0.434427 , training accuracy :  0.801347\n",
      "i: 11400 , cost :  0.434657 , training accuracy :  0.799102\n",
      "i: 11500 , cost :  0.467438 , training accuracy :  0.781145\n",
      "i: 11600 , cost :  0.435744 , training accuracy :  0.79798\n",
      "i: 11700 , cost :  0.442002 , training accuracy :  0.806958\n",
      "i: 11800 , cost :  0.457118 , training accuracy :  0.803591\n",
      "i: 11900 , cost :  0.4432 , training accuracy :  0.806958\n",
      "i: 12000 , cost :  0.445147 , training accuracy :  0.79349\n",
      "i: 12100 , cost :  0.433054 , training accuracy :  0.803591\n",
      "i: 12200 , cost :  0.432911 , training accuracy :  0.802469\n",
      "i: 12300 , cost :  0.434283 , training accuracy :  0.804714\n",
      "i: 12400 , cost :  0.442164 , training accuracy :  0.795735\n",
      "i: 12500 , cost :  0.451695 , training accuracy :  0.802469\n",
      "i: 12600 , cost :  0.435747 , training accuracy :  0.796857\n",
      "i: 12700 , cost :  0.432505 , training accuracy :  0.806958\n",
      "i: 12800 , cost :  0.433002 , training accuracy :  0.806958\n",
      "i: 12900 , cost :  0.436523 , training accuracy :  0.799102\n",
      "i: 13000 , cost :  0.443552 , training accuracy :  0.806958\n",
      "i: 13100 , cost :  0.441116 , training accuracy :  0.796857\n",
      "i: 13200 , cost :  0.432973 , training accuracy :  0.805836\n",
      "i: 13300 , cost :  0.432446 , training accuracy :  0.809203\n",
      "i: 13400 , cost :  0.433895 , training accuracy :  0.801347\n",
      "i: 13500 , cost :  0.439585 , training accuracy :  0.803591\n",
      "i: 13600 , cost :  0.44559 , training accuracy :  0.791246\n",
      "i: 13700 , cost :  0.436462 , training accuracy :  0.803591\n",
      "i: 13800 , cost :  0.432212 , training accuracy :  0.805836\n",
      "i: 13900 , cost :  0.43291 , training accuracy :  0.804714\n",
      "i: 14000 , cost :  0.44015 , training accuracy :  0.802469\n",
      "i: 14100 , cost :  0.447581 , training accuracy :  0.790123\n",
      "i: 14200 , cost :  0.436178 , training accuracy :  0.803591\n",
      "i: 14300 , cost :  0.432161 , training accuracy :  0.804714\n",
      "i: 14400 , cost :  0.432932 , training accuracy :  0.803591\n",
      "i: 14500 , cost :  0.437596 , training accuracy :  0.803591\n",
      "i: 14600 , cost :  0.447871 , training accuracy :  0.789001\n",
      "i: 14700 , cost :  0.43767 , training accuracy :  0.804714\n",
      "i: 14800 , cost :  0.432225 , training accuracy :  0.802469\n",
      "i: 14900 , cost :  0.432778 , training accuracy :  0.802469\n",
      "i: 15000 , cost :  0.436541 , training accuracy :  0.804714\n",
      "i: 15100 , cost :  0.447071 , training accuracy :  0.790123\n",
      "i: 15200 , cost :  0.439695 , training accuracy :  0.803591\n",
      "i: 15300 , cost :  0.432189 , training accuracy :  0.802469\n",
      "i: 15400 , cost :  0.432873 , training accuracy :  0.800224\n",
      "i: 15500 , cost :  0.437324 , training accuracy :  0.805836\n",
      "i: 15600 , cost :  0.4479 , training accuracy :  0.787879\n",
      "i: 15700 , cost :  0.436793 , training accuracy :  0.806958\n",
      "i: 15800 , cost :  0.431982 , training accuracy :  0.804714\n",
      "i: 15900 , cost :  0.433239 , training accuracy :  0.799102\n",
      "i: 16000 , cost :  0.437792 , training accuracy :  0.805836\n",
      "i: 16100 , cost :  0.448647 , training accuracy :  0.787879\n",
      "i: 16200 , cost :  0.433966 , training accuracy :  0.803591\n",
      "i: 16300 , cost :  0.432081 , training accuracy :  0.806958\n",
      "i: 16400 , cost :  0.433444 , training accuracy :  0.800224\n",
      "i: 16500 , cost :  0.441638 , training accuracy :  0.802469\n",
      "i: 16600 , cost :  0.446172 , training accuracy :  0.789001\n",
      "i: 16700 , cost :  0.43347 , training accuracy :  0.803591\n",
      "i: 16800 , cost :  0.431996 , training accuracy :  0.805836\n",
      "i: 16900 , cost :  0.434074 , training accuracy :  0.802469\n",
      "i: 17000 , cost :  0.444068 , training accuracy :  0.809203\n",
      "i: 17100 , cost :  0.443559 , training accuracy :  0.791246\n",
      "i: 17200 , cost :  0.433422 , training accuracy :  0.803591\n",
      "i: 17300 , cost :  0.43224 , training accuracy :  0.804714\n",
      "i: 17400 , cost :  0.437135 , training accuracy :  0.79798\n",
      "i: 17500 , cost :  0.43248 , training accuracy :  0.801347\n",
      "i: 17600 , cost :  0.431854 , training accuracy :  0.801347\n",
      "i: 17700 , cost :  0.441273 , training accuracy :  0.802469\n",
      "i: 17800 , cost :  0.435546 , training accuracy :  0.804714\n",
      "i: 17900 , cost :  0.431799 , training accuracy :  0.804714\n",
      "i: 18000 , cost :  0.433551 , training accuracy :  0.801347\n",
      "i: 18100 , cost :  0.440475 , training accuracy :  0.801347\n",
      "i: 18200 , cost :  0.446067 , training accuracy :  0.789001\n",
      "i: 18300 , cost :  0.432803 , training accuracy :  0.801347\n",
      "i: 18400 , cost :  0.431886 , training accuracy :  0.805836\n",
      "i: 18500 , cost :  0.434065 , training accuracy :  0.803591\n",
      "i: 18600 , cost :  0.446615 , training accuracy :  0.809203\n",
      "i: 18700 , cost :  0.439058 , training accuracy :  0.795735\n",
      "i: 18800 , cost :  0.431829 , training accuracy :  0.805836\n",
      "i: 18900 , cost :  0.432096 , training accuracy :  0.804714\n",
      "i: 19000 , cost :  0.439609 , training accuracy :  0.794613\n",
      "i: 19100 , cost :  0.437503 , training accuracy :  0.805836\n",
      "i: 19200 , cost :  0.438436 , training accuracy :  0.806958\n",
      "i: 19300 , cost :  0.432257 , training accuracy :  0.796857\n",
      "i: 19400 , cost :  0.433982 , training accuracy :  0.804714\n",
      "i: 19500 , cost :  0.431668 , training accuracy :  0.806958\n",
      "i: 19600 , cost :  0.443789 , training accuracy :  0.805836\n",
      "i: 19700 , cost :  0.44193 , training accuracy :  0.794613\n",
      "i: 19800 , cost :  0.442538 , training accuracy :  0.809203\n",
      "i: 19900 , cost :  0.449604 , training accuracy :  0.802469\n",
      "i: 20000 , cost :  0.434047 , training accuracy :  0.802469\n",
      "i: 20100 , cost :  0.431681 , training accuracy :  0.799102\n",
      "i: 20200 , cost :  0.43393 , training accuracy :  0.804714\n",
      "i: 20300 , cost :  0.444873 , training accuracy :  0.787879\n",
      "i: 20400 , cost :  0.439251 , training accuracy :  0.802469\n",
      "i: 20500 , cost :  0.431667 , training accuracy :  0.79798\n",
      "i: 20600 , cost :  0.432453 , training accuracy :  0.79798\n",
      "i: 20700 , cost :  0.439969 , training accuracy :  0.801347\n",
      "i: 20800 , cost :  0.444008 , training accuracy :  0.789001\n",
      "i: 20900 , cost :  0.431537 , training accuracy :  0.804714\n",
      "i: 21000 , cost :  0.437996 , training accuracy :  0.805836\n",
      "i: 21100 , cost :  0.441068 , training accuracy :  0.794613\n",
      "i: 21200 , cost :  0.431869 , training accuracy :  0.806958\n",
      "i: 21300 , cost :  0.432051 , training accuracy :  0.803591\n",
      "i: 21400 , cost :  0.435962 , training accuracy :  0.796857\n",
      "i: 21500 , cost :  0.450756 , training accuracy :  0.802469\n",
      "i: 21600 , cost :  0.433546 , training accuracy :  0.804714\n",
      "i: 21700 , cost :  0.431606 , training accuracy :  0.79798\n",
      "i: 21800 , cost :  0.434106 , training accuracy :  0.804714\n",
      "i: 21900 , cost :  0.446468 , training accuracy :  0.787879\n",
      "i: 22000 , cost :  0.436005 , training accuracy :  0.806958\n",
      "i: 22100 , cost :  0.43137 , training accuracy :  0.805836\n",
      "i: 22200 , cost :  0.432798 , training accuracy :  0.803591\n",
      "i: 22300 , cost :  0.445076 , training accuracy :  0.809203\n",
      "i: 22400 , cost :  0.4389 , training accuracy :  0.794613\n",
      "i: 22500 , cost :  0.431305 , training accuracy :  0.806958\n",
      "i: 22600 , cost :  0.432171 , training accuracy :  0.801347\n",
      "i: 22700 , cost :  0.438182 , training accuracy :  0.795735\n",
      "i: 22800 , cost :  0.445755 , training accuracy :  0.809203\n",
      "i: 22900 , cost :  0.431613 , training accuracy :  0.79798\n",
      "i: 23000 , cost :  0.431898 , training accuracy :  0.799102\n",
      "i: 23100 , cost :  0.43598 , training accuracy :  0.805836\n",
      "i: 23200 , cost :  0.448105 , training accuracy :  0.787879\n",
      "i: 23300 , cost :  0.432922 , training accuracy :  0.803591\n",
      "i: 23400 , cost :  0.431399 , training accuracy :  0.806958\n",
      "i: 23500 , cost :  0.433767 , training accuracy :  0.804714\n",
      "i: 23600 , cost :  0.449954 , training accuracy :  0.806958\n",
      "i: 23700 , cost :  0.434406 , training accuracy :  0.802469\n",
      "i: 23800 , cost :  0.431351 , training accuracy :  0.800224\n",
      "i: 23900 , cost :  0.434945 , training accuracy :  0.806958\n",
      "i: 24000 , cost :  0.446303 , training accuracy :  0.787879\n",
      "i: 24100 , cost :  0.434527 , training accuracy :  0.808081\n",
      "i: 24200 , cost :  0.431204 , training accuracy :  0.805836\n",
      "i: 24300 , cost :  0.434243 , training accuracy :  0.803591\n",
      "i: 24400 , cost :  0.449701 , training accuracy :  0.806958\n",
      "i: 24500 , cost :  0.43177 , training accuracy :  0.802469\n",
      "i: 24600 , cost :  0.432046 , training accuracy :  0.803591\n",
      "i: 24700 , cost :  0.446385 , training accuracy :  0.806958\n",
      "i: 24800 , cost :  0.432127 , training accuracy :  0.802469\n",
      "i: 24900 , cost :  0.431508 , training accuracy :  0.803591\n",
      "i: 25000 , cost :  0.436687 , training accuracy :  0.806958\n",
      "i: 25100 , cost :  0.443945 , training accuracy :  0.789001\n",
      "i: 25200 , cost :  0.431062 , training accuracy :  0.806958\n",
      "i: 25300 , cost :  0.432839 , training accuracy :  0.804714\n",
      "i: 25400 , cost :  0.444395 , training accuracy :  0.790123\n",
      "i: 25500 , cost :  0.432427 , training accuracy :  0.803591\n",
      "i: 25600 , cost :  0.43137 , training accuracy :  0.803591\n",
      "i: 25700 , cost :  0.438346 , training accuracy :  0.795735\n",
      "i: 25800 , cost :  0.439638 , training accuracy :  0.803591\n",
      "i: 25900 , cost :  0.430783 , training accuracy :  0.806958\n",
      "i: 26000 , cost :  0.434257 , training accuracy :  0.800224\n",
      "i: 26100 , cost :  0.447934 , training accuracy :  0.806958\n",
      "i: 26200 , cost :  0.430636 , training accuracy :  0.805836\n",
      "i: 26300 , cost :  0.432898 , training accuracy :  0.804714\n",
      "i: 26400 , cost :  0.449452 , training accuracy :  0.806958\n",
      "i: 26500 , cost :  0.430927 , training accuracy :  0.800224\n",
      "i: 26600 , cost :  0.432285 , training accuracy :  0.802469\n",
      "i: 26700 , cost :  0.447801 , training accuracy :  0.808081\n",
      "i: 26800 , cost :  0.431017 , training accuracy :  0.800224\n",
      "i: 26900 , cost :  0.432314 , training accuracy :  0.803591\n",
      "i: 27000 , cost :  0.447889 , training accuracy :  0.808081\n",
      "i: 27100 , cost :  0.43084 , training accuracy :  0.800224\n",
      "i: 27200 , cost :  0.432509 , training accuracy :  0.804714\n",
      "i: 27300 , cost :  0.448432 , training accuracy :  0.809203\n",
      "i: 27400 , cost :  0.430664 , training accuracy :  0.800224\n",
      "i: 27500 , cost :  0.431842 , training accuracy :  0.804714\n",
      "i: 27600 , cost :  0.449331 , training accuracy :  0.808081\n",
      "i: 27700 , cost :  0.430393 , training accuracy :  0.805836\n",
      "i: 27800 , cost :  0.434818 , training accuracy :  0.800224\n",
      "i: 27900 , cost :  0.439884 , training accuracy :  0.804714\n",
      "i: 28000 , cost :  0.430527 , training accuracy :  0.806958\n",
      "i: 28100 , cost :  0.438349 , training accuracy :  0.794613\n",
      "i: 28200 , cost :  0.435752 , training accuracy :  0.805836\n",
      "i: 28300 , cost :  0.431217 , training accuracy :  0.802469\n",
      "i: 28400 , cost :  0.441766 , training accuracy :  0.791246\n",
      "i: 28500 , cost :  0.431536 , training accuracy :  0.802469\n",
      "i: 28600 , cost :  0.431694 , training accuracy :  0.803591\n",
      "i: 28700 , cost :  0.445086 , training accuracy :  0.790123\n",
      "i: 28800 , cost :  0.430639 , training accuracy :  0.804714\n",
      "i: 28900 , cost :  0.433096 , training accuracy :  0.806958\n",
      "i: 29000 , cost :  0.44608 , training accuracy :  0.791246\n",
      "i: 29100 , cost :  0.430204 , training accuracy :  0.806958\n",
      "i: 29200 , cost :  0.435512 , training accuracy :  0.806958\n",
      "i: 29300 , cost :  0.436514 , training accuracy :  0.795735\n",
      "i: 29400 , cost :  0.430723 , training accuracy :  0.800224\n",
      "i: 29500 , cost :  0.444251 , training accuracy :  0.809203\n",
      "i: 29600 , cost :  0.431185 , training accuracy :  0.801347\n",
      "i: 29700 , cost :  0.43277 , training accuracy :  0.802469\n",
      "i: 29800 , cost :  0.448282 , training accuracy :  0.809203\n",
      "i: 29900 , cost :  0.430207 , training accuracy :  0.806958\n",
      "Final training accuracy :  0.795735\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 30000 ][hidden: 10 ][file: bhavul_tr_acc_0.80_prediction.csv ] ACCURACY :  0.795735\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1249.06 , training accuracy :  0.388328\n",
      "i: 100 , cost :  333.62 , training accuracy :  0.380471\n",
      "i: 200 , cost :  62.0449 , training accuracy :  0.59147\n",
      "i: 300 , cost :  35.386 , training accuracy :  0.648709\n",
      "i: 400 , cost :  27.2713 , training accuracy :  0.647587\n",
      "i: 500 , cost :  22.1937 , training accuracy :  0.648709\n",
      "i: 600 , cost :  16.6486 , training accuracy :  0.649832\n",
      "i: 700 , cost :  11.6612 , training accuracy :  0.645342\n",
      "i: 800 , cost :  9.07692 , training accuracy :  0.640853\n",
      "i: 900 , cost :  7.52673 , training accuracy :  0.641975\n",
      "i: 1000 , cost :  6.25223 , training accuracy :  0.664422\n",
      "i: 1100 , cost :  5.61076 , training accuracy :  0.671156\n",
      "i: 1200 , cost :  5.00958 , training accuracy :  0.682379\n",
      "i: 1300 , cost :  4.52705 , training accuracy :  0.694725\n",
      "i: 1400 , cost :  4.0312 , training accuracy :  0.710438\n",
      "i: 1500 , cost :  3.52447 , training accuracy :  0.727273\n",
      "i: 1600 , cost :  3.06105 , training accuracy :  0.73064\n",
      "i: 1700 , cost :  2.60847 , training accuracy :  0.725028\n",
      "i: 1800 , cost :  2.16971 , training accuracy :  0.741863\n",
      "i: 1900 , cost :  1.81125 , training accuracy :  0.754209\n",
      "i: 2000 , cost :  1.54164 , training accuracy :  0.75982\n",
      "i: 2100 , cost :  1.34221 , training accuracy :  0.767677\n",
      "i: 2200 , cost :  1.1782 , training accuracy :  0.75982\n",
      "i: 2300 , cost :  1.06616 , training accuracy :  0.768799\n",
      "i: 2400 , cost :  0.966457 , training accuracy :  0.769921\n",
      "i: 2500 , cost :  0.886397 , training accuracy :  0.774411\n",
      "i: 2600 , cost :  0.799477 , training accuracy :  0.786756\n",
      "i: 2700 , cost :  0.726506 , training accuracy :  0.791246\n",
      "i: 2800 , cost :  0.676825 , training accuracy :  0.79798\n",
      "i: 2900 , cost :  0.638771 , training accuracy :  0.803591\n",
      "i: 3000 , cost :  0.617149 , training accuracy :  0.81257\n",
      "i: 3100 , cost :  0.578397 , training accuracy :  0.811448\n",
      "i: 3200 , cost :  0.556554 , training accuracy :  0.809203\n",
      "i: 3300 , cost :  0.529297 , training accuracy :  0.801347\n",
      "i: 3400 , cost :  0.505906 , training accuracy :  0.809203\n",
      "i: 3500 , cost :  0.481422 , training accuracy :  0.823793\n",
      "i: 3600 , cost :  0.505686 , training accuracy :  0.792368\n",
      "i: 3700 , cost :  0.467998 , training accuracy :  0.826038\n",
      "i: 3800 , cost :  0.474996 , training accuracy :  0.804714\n",
      "i: 3900 , cost :  0.463008 , training accuracy :  0.810326\n",
      "i: 4000 , cost :  0.447545 , training accuracy :  0.819304\n",
      "i: 4100 , cost :  0.451219 , training accuracy :  0.824916\n",
      "i: 4200 , cost :  0.457213 , training accuracy :  0.818182\n",
      "i: 4300 , cost :  0.445235 , training accuracy :  0.823793\n",
      "i: 4400 , cost :  0.437196 , training accuracy :  0.82716\n",
      "i: 4500 , cost :  0.448029 , training accuracy :  0.811448\n",
      "i: 4600 , cost :  0.443344 , training accuracy :  0.821549\n",
      "i: 4700 , cost :  0.440609 , training accuracy :  0.821549\n",
      "i: 4800 , cost :  0.432297 , training accuracy :  0.824916\n",
      "i: 4900 , cost :  0.430035 , training accuracy :  0.826038\n",
      "i: 5000 , cost :  0.429041 , training accuracy :  0.824916\n",
      "i: 5100 , cost :  0.427822 , training accuracy :  0.826038\n",
      "i: 5200 , cost :  0.429299 , training accuracy :  0.815937\n",
      "i: 5300 , cost :  0.44476 , training accuracy :  0.808081\n",
      "i: 5400 , cost :  0.425287 , training accuracy :  0.821549\n",
      "i: 5500 , cost :  0.430758 , training accuracy :  0.824916\n",
      "i: 5600 , cost :  0.42327 , training accuracy :  0.828283\n",
      "i: 5700 , cost :  0.427386 , training accuracy :  0.817059\n",
      "i: 5800 , cost :  0.437185 , training accuracy :  0.811448\n",
      "i: 5900 , cost :  0.444186 , training accuracy :  0.808081\n",
      "i: 6000 , cost :  0.437961 , training accuracy :  0.810326\n",
      "i: 6100 , cost :  0.423217 , training accuracy :  0.815937\n",
      "i: 6200 , cost :  0.43893 , training accuracy :  0.813693\n",
      "i: 6300 , cost :  0.4363 , training accuracy :  0.805836\n",
      "i: 6400 , cost :  0.420005 , training accuracy :  0.826038\n",
      "i: 6500 , cost :  0.42438 , training accuracy :  0.813693\n",
      "i: 6600 , cost :  0.418352 , training accuracy :  0.824916\n",
      "i: 6700 , cost :  0.417974 , training accuracy :  0.82716\n",
      "i: 6800 , cost :  0.426207 , training accuracy :  0.819304\n",
      "i: 6900 , cost :  0.434026 , training accuracy :  0.819304\n",
      "i: 7000 , cost :  0.445168 , training accuracy :  0.813693\n",
      "i: 7100 , cost :  0.424127 , training accuracy :  0.820426\n",
      "i: 7200 , cost :  0.417978 , training accuracy :  0.818182\n",
      "i: 7300 , cost :  0.454962 , training accuracy :  0.803591\n",
      "i: 7400 , cost :  0.426901 , training accuracy :  0.822671\n",
      "i: 7500 , cost :  0.425863 , training accuracy :  0.821549\n",
      "i: 7600 , cost :  0.425318 , training accuracy :  0.821549\n",
      "i: 7700 , cost :  0.423415 , training accuracy :  0.822671\n",
      "i: 7800 , cost :  0.423004 , training accuracy :  0.821549\n",
      "i: 7900 , cost :  0.424117 , training accuracy :  0.821549\n",
      "i: 8000 , cost :  0.425407 , training accuracy :  0.823793\n",
      "i: 8100 , cost :  0.424447 , training accuracy :  0.822671\n",
      "i: 8200 , cost :  0.423158 , training accuracy :  0.821549\n",
      "i: 8300 , cost :  0.420604 , training accuracy :  0.821549\n",
      "i: 8400 , cost :  0.419916 , training accuracy :  0.822671\n",
      "i: 8500 , cost :  0.419694 , training accuracy :  0.822671\n",
      "i: 8600 , cost :  0.41237 , training accuracy :  0.83165\n",
      "i: 8700 , cost :  0.4243 , training accuracy :  0.822671\n",
      "i: 8800 , cost :  0.41733 , training accuracy :  0.820426\n",
      "i: 8900 , cost :  0.412464 , training accuracy :  0.826038\n",
      "i: 9000 , cost :  0.411436 , training accuracy :  0.83165\n",
      "i: 9100 , cost :  0.412821 , training accuracy :  0.821549\n",
      "i: 9200 , cost :  0.416167 , training accuracy :  0.819304\n",
      "i: 9300 , cost :  0.428454 , training accuracy :  0.809203\n",
      "i: 9400 , cost :  0.410986 , training accuracy :  0.828283\n",
      "i: 9500 , cost :  0.420697 , training accuracy :  0.814815\n",
      "i: 9600 , cost :  0.454631 , training accuracy :  0.802469\n",
      "i: 9700 , cost :  0.412712 , training accuracy :  0.82716\n",
      "i: 9800 , cost :  0.411767 , training accuracy :  0.821549\n",
      "i: 9900 , cost :  0.434774 , training accuracy :  0.817059\n",
      "i: 10000 , cost :  0.427497 , training accuracy :  0.808081\n",
      "i: 10100 , cost :  0.411403 , training accuracy :  0.823793\n",
      "i: 10200 , cost :  0.410655 , training accuracy :  0.824916\n",
      "i: 10300 , cost :  0.416334 , training accuracy :  0.819304\n",
      "i: 10400 , cost :  0.409205 , training accuracy :  0.83165\n",
      "i: 10500 , cost :  0.41099 , training accuracy :  0.824916\n",
      "i: 10600 , cost :  0.421498 , training accuracy :  0.829405\n",
      "i: 10700 , cost :  0.440928 , training accuracy :  0.806958\n",
      "i: 10800 , cost :  0.411639 , training accuracy :  0.821549\n",
      "i: 10900 , cost :  0.408663 , training accuracy :  0.832772\n",
      "i: 11000 , cost :  0.408518 , training accuracy :  0.828283\n",
      "i: 11100 , cost :  0.412573 , training accuracy :  0.820426\n",
      "i: 11200 , cost :  0.421006 , training accuracy :  0.811448\n",
      "i: 11300 , cost :  0.413692 , training accuracy :  0.819304\n",
      "i: 11400 , cost :  0.408877 , training accuracy :  0.829405\n",
      "i: 11500 , cost :  0.407987 , training accuracy :  0.830527\n",
      "i: 11600 , cost :  0.412322 , training accuracy :  0.82716\n",
      "i: 11700 , cost :  0.414333 , training accuracy :  0.82716\n",
      "i: 11800 , cost :  0.433693 , training accuracy :  0.820426\n",
      "i: 11900 , cost :  0.432767 , training accuracy :  0.808081\n",
      "i: 12000 , cost :  0.413649 , training accuracy :  0.821549\n",
      "i: 12100 , cost :  0.408595 , training accuracy :  0.826038\n",
      "i: 12200 , cost :  0.406945 , training accuracy :  0.836139\n",
      "i: 12300 , cost :  0.407124 , training accuracy :  0.832772\n",
      "i: 12400 , cost :  0.408331 , training accuracy :  0.823793\n",
      "i: 12500 , cost :  0.417339 , training accuracy :  0.828283\n",
      "i: 12600 , cost :  0.420803 , training accuracy :  0.829405\n",
      "i: 12700 , cost :  0.428967 , training accuracy :  0.815937\n",
      "i: 12800 , cost :  0.428019 , training accuracy :  0.815937\n",
      "i: 12900 , cost :  0.417824 , training accuracy :  0.828283\n",
      "i: 13000 , cost :  0.408514 , training accuracy :  0.828283\n",
      "i: 13100 , cost :  0.40691 , training accuracy :  0.83165\n",
      "i: 13200 , cost :  0.406278 , training accuracy :  0.83165\n",
      "i: 13300 , cost :  0.407498 , training accuracy :  0.823793\n",
      "i: 13400 , cost :  0.414132 , training accuracy :  0.817059\n",
      "i: 13500 , cost :  0.424965 , training accuracy :  0.810326\n",
      "i: 13600 , cost :  0.4291 , training accuracy :  0.809203\n",
      "i: 13700 , cost :  0.415285 , training accuracy :  0.814815\n",
      "i: 13800 , cost :  0.407287 , training accuracy :  0.826038\n",
      "i: 13900 , cost :  0.405903 , training accuracy :  0.83165\n",
      "i: 14000 , cost :  0.406008 , training accuracy :  0.83165\n",
      "i: 14100 , cost :  0.406843 , training accuracy :  0.822671\n",
      "i: 14200 , cost :  0.407571 , training accuracy :  0.824916\n",
      "i: 14300 , cost :  0.413452 , training accuracy :  0.82716\n",
      "i: 14400 , cost :  0.424257 , training accuracy :  0.819304\n",
      "i: 14500 , cost :  0.428901 , training accuracy :  0.818182\n",
      "i: 14600 , cost :  0.422606 , training accuracy :  0.820426\n",
      "i: 14700 , cost :  0.408028 , training accuracy :  0.828283\n",
      "i: 14800 , cost :  0.423155 , training accuracy :  0.819304\n",
      "i: 14900 , cost :  0.423543 , training accuracy :  0.811448\n",
      "i: 15000 , cost :  0.435842 , training accuracy :  0.813693\n",
      "i: 15100 , cost :  0.411177 , training accuracy :  0.829405\n",
      "i: 15200 , cost :  0.407552 , training accuracy :  0.82716\n",
      "i: 15300 , cost :  0.405843 , training accuracy :  0.82716\n",
      "i: 15400 , cost :  0.405952 , training accuracy :  0.823793\n",
      "i: 15500 , cost :  0.408796 , training accuracy :  0.818182\n",
      "i: 15600 , cost :  0.409324 , training accuracy :  0.829405\n",
      "i: 15700 , cost :  0.411585 , training accuracy :  0.829405\n",
      "i: 15800 , cost :  0.430147 , training accuracy :  0.817059\n",
      "i: 15900 , cost :  0.430129 , training accuracy :  0.818182\n",
      "i: 16000 , cost :  0.427837 , training accuracy :  0.815937\n",
      "i: 16100 , cost :  0.407685 , training accuracy :  0.828283\n",
      "i: 16200 , cost :  0.406776 , training accuracy :  0.821549\n",
      "i: 16300 , cost :  0.411997 , training accuracy :  0.815937\n",
      "i: 16400 , cost :  0.41357 , training accuracy :  0.815937\n",
      "i: 16500 , cost :  0.429013 , training accuracy :  0.818182\n",
      "i: 16600 , cost :  0.42757 , training accuracy :  0.817059\n",
      "i: 16700 , cost :  0.408494 , training accuracy :  0.82716\n",
      "i: 16800 , cost :  0.410299 , training accuracy :  0.830527\n",
      "i: 16900 , cost :  0.407325 , training accuracy :  0.828283\n",
      "i: 17000 , cost :  0.40493 , training accuracy :  0.83165\n",
      "i: 17100 , cost :  0.406571 , training accuracy :  0.828283\n",
      "i: 17200 , cost :  0.41799 , training accuracy :  0.829405\n",
      "i: 17300 , cost :  0.409147 , training accuracy :  0.829405\n",
      "i: 17400 , cost :  0.416151 , training accuracy :  0.81257\n",
      "i: 17500 , cost :  0.410508 , training accuracy :  0.818182\n",
      "i: 17600 , cost :  0.407654 , training accuracy :  0.820426\n",
      "i: 17700 , cost :  0.404283 , training accuracy :  0.83165\n",
      "i: 17800 , cost :  0.41835 , training accuracy :  0.814815\n",
      "i: 17900 , cost :  0.406779 , training accuracy :  0.829405\n",
      "i: 18000 , cost :  0.411458 , training accuracy :  0.818182\n",
      "i: 18100 , cost :  0.411365 , training accuracy :  0.817059\n",
      "i: 18200 , cost :  0.444337 , training accuracy :  0.811448\n",
      "i: 18300 , cost :  0.442066 , training accuracy :  0.79798\n",
      "i: 18400 , cost :  0.424069 , training accuracy :  0.809203\n",
      "i: 18500 , cost :  0.404062 , training accuracy :  0.832772\n",
      "i: 18600 , cost :  0.437034 , training accuracy :  0.818182\n",
      "i: 18700 , cost :  0.434873 , training accuracy :  0.801347\n",
      "i: 18800 , cost :  0.408859 , training accuracy :  0.820426\n",
      "i: 18900 , cost :  0.410959 , training accuracy :  0.820426\n",
      "i: 19000 , cost :  0.418653 , training accuracy :  0.828283\n",
      "i: 19100 , cost :  0.414916 , training accuracy :  0.82716\n",
      "i: 19200 , cost :  0.406106 , training accuracy :  0.832772\n",
      "i: 19300 , cost :  0.415853 , training accuracy :  0.81257\n",
      "i: 19400 , cost :  0.422229 , training accuracy :  0.81257\n",
      "i: 19500 , cost :  0.419595 , training accuracy :  0.808081\n",
      "i: 19600 , cost :  0.416523 , training accuracy :  0.817059\n",
      "i: 19700 , cost :  0.410442 , training accuracy :  0.822671\n",
      "i: 19800 , cost :  0.408107 , training accuracy :  0.819304\n",
      "i: 19900 , cost :  0.406119 , training accuracy :  0.820426\n",
      "i: 20000 , cost :  0.415748 , training accuracy :  0.826038\n",
      "i: 20100 , cost :  0.406131 , training accuracy :  0.82716\n",
      "i: 20200 , cost :  0.405184 , training accuracy :  0.822671\n",
      "i: 20300 , cost :  0.407516 , training accuracy :  0.820426\n",
      "i: 20400 , cost :  0.403089 , training accuracy :  0.833894\n",
      "i: 20500 , cost :  0.417905 , training accuracy :  0.83165\n",
      "i: 20600 , cost :  0.412244 , training accuracy :  0.814815\n",
      "i: 20700 , cost :  0.419354 , training accuracy :  0.82716\n",
      "i: 20800 , cost :  0.405617 , training accuracy :  0.828283\n",
      "i: 20900 , cost :  0.41083 , training accuracy :  0.82716\n",
      "i: 21000 , cost :  0.410638 , training accuracy :  0.829405\n",
      "i: 21100 , cost :  0.440286 , training accuracy :  0.814815\n",
      "i: 21200 , cost :  0.410281 , training accuracy :  0.819304\n",
      "i: 21300 , cost :  0.410327 , training accuracy :  0.820426\n",
      "i: 21400 , cost :  0.40981 , training accuracy :  0.828283\n",
      "i: 21500 , cost :  0.405279 , training accuracy :  0.828283\n",
      "i: 21600 , cost :  0.414266 , training accuracy :  0.829405\n",
      "i: 21700 , cost :  0.41553 , training accuracy :  0.811448\n",
      "i: 21800 , cost :  0.409724 , training accuracy :  0.819304\n",
      "i: 21900 , cost :  0.426227 , training accuracy :  0.815937\n",
      "i: 22000 , cost :  0.4049 , training accuracy :  0.82716\n",
      "i: 22100 , cost :  0.408598 , training accuracy :  0.83165\n",
      "i: 22200 , cost :  0.417599 , training accuracy :  0.828283\n",
      "i: 22300 , cost :  0.413554 , training accuracy :  0.828283\n",
      "i: 22400 , cost :  0.402891 , training accuracy :  0.832772\n",
      "i: 22500 , cost :  0.402718 , training accuracy :  0.833894\n",
      "i: 22600 , cost :  0.403085 , training accuracy :  0.82716\n",
      "i: 22700 , cost :  0.405756 , training accuracy :  0.814815\n",
      "i: 22800 , cost :  0.404175 , training accuracy :  0.818182\n",
      "i: 22900 , cost :  0.406591 , training accuracy :  0.817059\n",
      "i: 23000 , cost :  0.422191 , training accuracy :  0.806958\n",
      "i: 23100 , cost :  0.419538 , training accuracy :  0.809203\n",
      "i: 23200 , cost :  0.420794 , training accuracy :  0.808081\n",
      "i: 23300 , cost :  0.408477 , training accuracy :  0.818182\n",
      "i: 23400 , cost :  0.406178 , training accuracy :  0.817059\n",
      "i: 23500 , cost :  0.412333 , training accuracy :  0.828283\n",
      "i: 23600 , cost :  0.433914 , training accuracy :  0.815937\n",
      "i: 23700 , cost :  0.410973 , training accuracy :  0.829405\n",
      "i: 23800 , cost :  0.408591 , training accuracy :  0.830527\n",
      "i: 23900 , cost :  0.437076 , training accuracy :  0.799102\n",
      "i: 24000 , cost :  0.411455 , training accuracy :  0.815937\n",
      "i: 24100 , cost :  0.427866 , training accuracy :  0.808081\n",
      "i: 24200 , cost :  0.425158 , training accuracy :  0.818182\n",
      "i: 24300 , cost :  0.412508 , training accuracy :  0.828283\n",
      "i: 24400 , cost :  0.402218 , training accuracy :  0.836139\n",
      "i: 24500 , cost :  0.40225 , training accuracy :  0.83165\n",
      "i: 24600 , cost :  0.404591 , training accuracy :  0.817059\n",
      "i: 24700 , cost :  0.416897 , training accuracy :  0.808081\n",
      "i: 24800 , cost :  0.412808 , training accuracy :  0.815937\n",
      "i: 24900 , cost :  0.420796 , training accuracy :  0.806958\n",
      "i: 25000 , cost :  0.413843 , training accuracy :  0.810326\n",
      "i: 25100 , cost :  0.405951 , training accuracy :  0.815937\n",
      "i: 25200 , cost :  0.402055 , training accuracy :  0.832772\n",
      "i: 25300 , cost :  0.401988 , training accuracy :  0.836139\n",
      "i: 25400 , cost :  0.402349 , training accuracy :  0.835017\n",
      "i: 25500 , cost :  0.402095 , training accuracy :  0.832772\n",
      "i: 25600 , cost :  0.40548 , training accuracy :  0.828283\n",
      "i: 25700 , cost :  0.409067 , training accuracy :  0.828283\n",
      "i: 25800 , cost :  0.42115 , training accuracy :  0.818182\n",
      "i: 25900 , cost :  0.417256 , training accuracy :  0.828283\n",
      "i: 26000 , cost :  0.404163 , training accuracy :  0.829405\n",
      "i: 26100 , cost :  0.412166 , training accuracy :  0.814815\n",
      "i: 26200 , cost :  0.401802 , training accuracy :  0.832772\n",
      "i: 26300 , cost :  0.401848 , training accuracy :  0.837261\n",
      "i: 26400 , cost :  0.40696 , training accuracy :  0.830527\n",
      "i: 26500 , cost :  0.419244 , training accuracy :  0.822671\n",
      "i: 26600 , cost :  0.413719 , training accuracy :  0.83165\n",
      "i: 26700 , cost :  0.401877 , training accuracy :  0.833894\n",
      "i: 26800 , cost :  0.403298 , training accuracy :  0.819304\n",
      "i: 26900 , cost :  0.401991 , training accuracy :  0.829405\n",
      "i: 27000 , cost :  0.406528 , training accuracy :  0.818182\n",
      "i: 27100 , cost :  0.420436 , training accuracy :  0.806958\n",
      "i: 27200 , cost :  0.415239 , training accuracy :  0.809203\n",
      "i: 27300 , cost :  0.425021 , training accuracy :  0.809203\n",
      "i: 27400 , cost :  0.409929 , training accuracy :  0.813693\n",
      "i: 27500 , cost :  0.401599 , training accuracy :  0.835017\n",
      "i: 27600 , cost :  0.402821 , training accuracy :  0.830527\n",
      "i: 27700 , cost :  0.401544 , training accuracy :  0.832772\n",
      "i: 27800 , cost :  0.405418 , training accuracy :  0.828283\n",
      "i: 27900 , cost :  0.407849 , training accuracy :  0.83165\n",
      "i: 28000 , cost :  0.41163 , training accuracy :  0.829405\n",
      "i: 28100 , cost :  0.428166 , training accuracy :  0.819304\n",
      "i: 28200 , cost :  0.429251 , training accuracy :  0.818182\n",
      "i: 28300 , cost :  0.420056 , training accuracy :  0.819304\n",
      "i: 28400 , cost :  0.415419 , training accuracy :  0.829405\n",
      "i: 28500 , cost :  0.402833 , training accuracy :  0.826038\n",
      "i: 28600 , cost :  0.402045 , training accuracy :  0.826038\n",
      "i: 28700 , cost :  0.402168 , training accuracy :  0.822671\n",
      "i: 28800 , cost :  0.403377 , training accuracy :  0.815937\n",
      "i: 28900 , cost :  0.403629 , training accuracy :  0.817059\n",
      "i: 29000 , cost :  0.412903 , training accuracy :  0.81257\n",
      "i: 29100 , cost :  0.424145 , training accuracy :  0.810326\n",
      "i: 29200 , cost :  0.425348 , training accuracy :  0.810326\n",
      "i: 29300 , cost :  0.409559 , training accuracy :  0.814815\n",
      "i: 29400 , cost :  0.401539 , training accuracy :  0.833894\n",
      "i: 29500 , cost :  0.403197 , training accuracy :  0.82716\n",
      "i: 29600 , cost :  0.414438 , training accuracy :  0.832772\n",
      "i: 29700 , cost :  0.427962 , training accuracy :  0.819304\n",
      "i: 29800 , cost :  0.406889 , training accuracy :  0.83165\n",
      "i: 29900 , cost :  0.410497 , training accuracy :  0.830527\n",
      "Final training accuracy :  0.83165\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 30000 ][hidden: 15 ][file: bhavul_tr_acc_0.83_prediction.csv ] ACCURACY :  0.83165\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1288.7 , training accuracy :  0.363636\n",
      "i: 100 , cost :  153.921 , training accuracy :  0.618406\n",
      "i: 200 , cost :  21.7979 , training accuracy :  0.594837\n",
      "i: 300 , cost :  5.95709 , training accuracy :  0.680135\n",
      "i: 400 , cost :  3.92262 , training accuracy :  0.705948\n",
      "i: 500 , cost :  2.73898 , training accuracy :  0.731762\n",
      "i: 600 , cost :  1.9804 , training accuracy :  0.750842\n",
      "i: 700 , cost :  1.49169 , training accuracy :  0.767677\n",
      "i: 800 , cost :  1.31144 , training accuracy :  0.773288\n",
      "i: 900 , cost :  1.17564 , training accuracy :  0.782267\n",
      "i: 1000 , cost :  1.05856 , training accuracy :  0.784512\n",
      "i: 1100 , cost :  1.07535 , training accuracy :  0.756453\n",
      "i: 1200 , cost :  1.29786 , training accuracy :  0.756453\n",
      "i: 1300 , cost :  0.932692 , training accuracy :  0.789001\n",
      "i: 1400 , cost :  1.01205 , training accuracy :  0.776655\n",
      "i: 1500 , cost :  0.841909 , training accuracy :  0.802469\n",
      "i: 1600 , cost :  1.2348 , training accuracy :  0.754209\n",
      "i: 1700 , cost :  0.9341 , training accuracy :  0.782267\n",
      "i: 1800 , cost :  0.786368 , training accuracy :  0.802469\n",
      "i: 1900 , cost :  0.839975 , training accuracy :  0.767677\n",
      "i: 2000 , cost :  0.881923 , training accuracy :  0.776655\n",
      "i: 2100 , cost :  0.737116 , training accuracy :  0.81257\n",
      "i: 2200 , cost :  0.806464 , training accuracy :  0.762065\n",
      "i: 2300 , cost :  0.856138 , training accuracy :  0.780022\n",
      "i: 2400 , cost :  0.700161 , training accuracy :  0.813693\n",
      "i: 2500 , cost :  0.737237 , training accuracy :  0.782267\n",
      "i: 2600 , cost :  0.854358 , training accuracy :  0.772166\n",
      "i: 2700 , cost :  0.718451 , training accuracy :  0.785634\n",
      "i: 2800 , cost :  0.658588 , training accuracy :  0.806958\n",
      "i: 2900 , cost :  0.772414 , training accuracy :  0.781145\n",
      "i: 3000 , cost :  0.717684 , training accuracy :  0.772166\n",
      "i: 3100 , cost :  0.633166 , training accuracy :  0.809203\n",
      "i: 3200 , cost :  0.789299 , training accuracy :  0.776655\n",
      "i: 3300 , cost :  0.615574 , training accuracy :  0.809203\n",
      "i: 3400 , cost :  0.712961 , training accuracy :  0.75982\n",
      "i: 3500 , cost :  0.622038 , training accuracy :  0.796857\n",
      "i: 3600 , cost :  0.673439 , training accuracy :  0.810326\n",
      "i: 3700 , cost :  0.774878 , training accuracy :  0.777778\n",
      "i: 3800 , cost :  0.733413 , training accuracy :  0.7789\n",
      "i: 3900 , cost :  0.68149 , training accuracy :  0.789001\n",
      "i: 4000 , cost :  0.672056 , training accuracy :  0.79349\n",
      "i: 4100 , cost :  0.688455 , training accuracy :  0.789001\n",
      "i: 4200 , cost :  0.732022 , training accuracy :  0.780022\n",
      "i: 4300 , cost :  0.752261 , training accuracy :  0.782267\n",
      "i: 4400 , cost :  0.646015 , training accuracy :  0.808081\n",
      "i: 4500 , cost :  0.567243 , training accuracy :  0.835017\n",
      "i: 4600 , cost :  0.574224 , training accuracy :  0.803591\n",
      "i: 4700 , cost :  0.640588 , training accuracy :  0.769921\n",
      "i: 4800 , cost :  0.643705 , training accuracy :  0.767677\n",
      "i: 4900 , cost :  0.553377 , training accuracy :  0.820426\n",
      "i: 5000 , cost :  0.608363 , training accuracy :  0.811448\n",
      "i: 5100 , cost :  0.730179 , training accuracy :  0.777778\n",
      "i: 5200 , cost :  0.547889 , training accuracy :  0.842873\n",
      "i: 5300 , cost :  0.587947 , training accuracy :  0.79349\n",
      "i: 5400 , cost :  0.61932 , training accuracy :  0.774411\n",
      "i: 5500 , cost :  0.52361 , training accuracy :  0.840629\n",
      "i: 5600 , cost :  0.678743 , training accuracy :  0.781145\n",
      "i: 5700 , cost :  0.562919 , training accuracy :  0.829405\n",
      "i: 5800 , cost :  0.554211 , training accuracy :  0.799102\n",
      "i: 5900 , cost :  0.602721 , training accuracy :  0.775533\n",
      "i: 6000 , cost :  0.50929 , training accuracy :  0.84624\n",
      "i: 6100 , cost :  0.663575 , training accuracy :  0.787879\n",
      "i: 6200 , cost :  0.548078 , training accuracy :  0.830527\n",
      "i: 6300 , cost :  0.547046 , training accuracy :  0.802469\n",
      "i: 6400 , cost :  0.587818 , training accuracy :  0.784512\n",
      "i: 6500 , cost :  0.495036 , training accuracy :  0.843996\n",
      "i: 6600 , cost :  0.633798 , training accuracy :  0.792368\n",
      "i: 6700 , cost :  0.546003 , training accuracy :  0.822671\n",
      "i: 6800 , cost :  0.51293 , training accuracy :  0.81257\n",
      "i: 6900 , cost :  0.584933 , training accuracy :  0.7789\n",
      "i: 7000 , cost :  0.494404 , training accuracy :  0.828283\n",
      "i: 7100 , cost :  0.556038 , training accuracy :  0.811448\n",
      "i: 7200 , cost :  0.650824 , training accuracy :  0.791246\n",
      "i: 7300 , cost :  0.477058 , training accuracy :  0.838384\n",
      "i: 7400 , cost :  0.545872 , training accuracy :  0.800224\n",
      "i: 7500 , cost :  0.556789 , training accuracy :  0.79349\n",
      "i: 7600 , cost :  0.470699 , training accuracy :  0.842873\n",
      "i: 7700 , cost :  0.580599 , training accuracy :  0.804714\n",
      "i: 7800 , cost :  0.589893 , training accuracy :  0.809203\n",
      "i: 7900 , cost :  0.471676 , training accuracy :  0.833894\n",
      "i: 8000 , cost :  0.515223 , training accuracy :  0.808081\n",
      "i: 8100 , cost :  0.554807 , training accuracy :  0.790123\n",
      "i: 8200 , cost :  0.538544 , training accuracy :  0.792368\n",
      "i: 8300 , cost :  0.471331 , training accuracy :  0.829405\n",
      "i: 8400 , cost :  0.468546 , training accuracy :  0.841751\n",
      "i: 8500 , cost :  0.578033 , training accuracy :  0.799102\n",
      "i: 8600 , cost :  0.576205 , training accuracy :  0.81257\n",
      "i: 8700 , cost :  0.447581 , training accuracy :  0.847363\n",
      "i: 8800 , cost :  0.496628 , training accuracy :  0.811448\n",
      "i: 8900 , cost :  0.530192 , training accuracy :  0.789001\n",
      "i: 9000 , cost :  0.449885 , training accuracy :  0.838384\n",
      "i: 9100 , cost :  0.448825 , training accuracy :  0.847363\n",
      "i: 9200 , cost :  0.525042 , training accuracy :  0.813693\n",
      "i: 9300 , cost :  0.593136 , training accuracy :  0.801347\n",
      "i: 9400 , cost :  0.60488 , training accuracy :  0.801347\n",
      "i: 9500 , cost :  0.434754 , training accuracy :  0.843996\n",
      "i: 9600 , cost :  0.429262 , training accuracy :  0.854097\n",
      "i: 9700 , cost :  0.521255 , training accuracy :  0.782267\n",
      "i: 9800 , cost :  0.42904 , training accuracy :  0.843996\n",
      "i: 9900 , cost :  0.450433 , training accuracy :  0.840629\n",
      "i: 10000 , cost :  0.436971 , training accuracy :  0.832772\n",
      "i: 10100 , cost :  0.464565 , training accuracy :  0.826038\n",
      "i: 10200 , cost :  0.534488 , training accuracy :  0.767677\n",
      "i: 10300 , cost :  0.444242 , training accuracy :  0.837261\n",
      "i: 10400 , cost :  0.47452 , training accuracy :  0.805836\n",
      "i: 10500 , cost :  0.41262 , training accuracy :  0.859708\n",
      "i: 10600 , cost :  0.451008 , training accuracy :  0.839506\n",
      "i: 10700 , cost :  0.427245 , training accuracy :  0.854097\n",
      "i: 10800 , cost :  0.405795 , training accuracy :  0.859708\n",
      "i: 10900 , cost :  0.410925 , training accuracy :  0.849607\n",
      "i: 11000 , cost :  0.403414 , training accuracy :  0.860831\n",
      "i: 11100 , cost :  0.40443 , training accuracy :  0.854097\n",
      "i: 11200 , cost :  0.470357 , training accuracy :  0.829405\n",
      "i: 11300 , cost :  0.492764 , training accuracy :  0.787879\n",
      "i: 11400 , cost :  0.403167 , training accuracy :  0.861953\n",
      "i: 11500 , cost :  0.473249 , training accuracy :  0.83165\n",
      "i: 11600 , cost :  0.47835 , training accuracy :  0.79349\n",
      "i: 11700 , cost :  0.400109 , training accuracy :  0.852974\n",
      "i: 11800 , cost :  0.539619 , training accuracy :  0.810326\n",
      "i: 11900 , cost :  0.41322 , training accuracy :  0.838384\n",
      "i: 12000 , cost :  0.438219 , training accuracy :  0.821549\n",
      "i: 12100 , cost :  0.561894 , training accuracy :  0.811448\n",
      "i: 12200 , cost :  0.396231 , training accuracy :  0.861953\n",
      "i: 12300 , cost :  0.469091 , training accuracy :  0.795735\n",
      "i: 12400 , cost :  0.394635 , training accuracy :  0.858586\n",
      "i: 12500 , cost :  0.475584 , training accuracy :  0.833894\n",
      "i: 12600 , cost :  0.466359 , training accuracy :  0.800224\n",
      "i: 12700 , cost :  0.389604 , training accuracy :  0.855219\n",
      "i: 12800 , cost :  0.542799 , training accuracy :  0.814815\n",
      "i: 12900 , cost :  0.382495 , training accuracy :  0.866442\n",
      "i: 13000 , cost :  0.464322 , training accuracy :  0.803591\n",
      "i: 13100 , cost :  0.476919 , training accuracy :  0.836139\n",
      "i: 13200 , cost :  0.389758 , training accuracy :  0.856341\n",
      "i: 13300 , cost :  0.478883 , training accuracy :  0.786756\n",
      "i: 13400 , cost :  0.376602 , training accuracy :  0.863075\n",
      "i: 13500 , cost :  0.517678 , training accuracy :  0.820426\n",
      "i: 13600 , cost :  0.414949 , training accuracy :  0.839506\n",
      "i: 13700 , cost :  0.506118 , training accuracy :  0.820426\n",
      "i: 13800 , cost :  0.434134 , training accuracy :  0.842873\n",
      "i: 13900 , cost :  0.411898 , training accuracy :  0.840629\n",
      "i: 14000 , cost :  0.459249 , training accuracy :  0.802469\n",
      "i: 14100 , cost :  0.373028 , training accuracy :  0.868687\n",
      "i: 14200 , cost :  0.429883 , training accuracy :  0.839506\n",
      "i: 14300 , cost :  0.370521 , training accuracy :  0.860831\n",
      "i: 14400 , cost :  0.450212 , training accuracy :  0.805836\n",
      "i: 14500 , cost :  0.474929 , training accuracy :  0.837261\n",
      "i: 14600 , cost :  0.368569 , training accuracy :  0.870932\n",
      "i: 14700 , cost :  0.420094 , training accuracy :  0.826038\n",
      "i: 14800 , cost :  0.55407 , training accuracy :  0.817059\n",
      "i: 14900 , cost :  0.37818 , training accuracy :  0.848485\n",
      "i: 15000 , cost :  0.36904 , training accuracy :  0.854097\n",
      "i: 15100 , cost :  0.453809 , training accuracy :  0.838384\n",
      "i: 15200 , cost :  0.466438 , training accuracy :  0.792368\n",
      "i: 15300 , cost :  0.365652 , training accuracy :  0.867565\n",
      "i: 15400 , cost :  0.405769 , training accuracy :  0.842873\n",
      "i: 15500 , cost :  0.459964 , training accuracy :  0.792368\n",
      "i: 15600 , cost :  0.390842 , training accuracy :  0.845118\n",
      "i: 15700 , cost :  0.368147 , training accuracy :  0.866442\n",
      "i: 15800 , cost :  0.426169 , training accuracy :  0.815937\n",
      "i: 15900 , cost :  0.487791 , training accuracy :  0.841751\n",
      "i: 16000 , cost :  0.356914 , training accuracy :  0.858586\n",
      "i: 16100 , cost :  0.400081 , training accuracy :  0.835017\n",
      "i: 16200 , cost :  0.525957 , training accuracy :  0.830527\n",
      "i: 16300 , cost :  0.354374 , training accuracy :  0.867565\n",
      "i: 16400 , cost :  0.411909 , training accuracy :  0.823793\n",
      "i: 16500 , cost :  0.518812 , training accuracy :  0.83165\n",
      "i: 16600 , cost :  0.352679 , training accuracy :  0.86532\n",
      "i: 16700 , cost :  0.398557 , training accuracy :  0.836139\n",
      "i: 16800 , cost :  0.525541 , training accuracy :  0.829405\n",
      "i: 16900 , cost :  0.350835 , training accuracy :  0.870932\n",
      "i: 17000 , cost :  0.42246 , training accuracy :  0.818182\n",
      "i: 17100 , cost :  0.401774 , training accuracy :  0.847363\n",
      "i: 17200 , cost :  0.373948 , training accuracy :  0.854097\n",
      "i: 17300 , cost :  0.44977 , training accuracy :  0.795735\n",
      "i: 17400 , cost :  0.354922 , training accuracy :  0.872054\n",
      "i: 17500 , cost :  0.404219 , training accuracy :  0.847363\n",
      "i: 17600 , cost :  0.445186 , training accuracy :  0.79798\n",
      "i: 17700 , cost :  0.35114 , training accuracy :  0.875421\n",
      "i: 17800 , cost :  0.39965 , training accuracy :  0.847363\n",
      "i: 17900 , cost :  0.441221 , training accuracy :  0.799102\n",
      "i: 18000 , cost :  0.345617 , training accuracy :  0.86532\n",
      "i: 18100 , cost :  0.402242 , training accuracy :  0.849607\n",
      "i: 18200 , cost :  0.356309 , training accuracy :  0.855219\n",
      "i: 18300 , cost :  0.551294 , training accuracy :  0.842873\n",
      "i: 18400 , cost :  0.346399 , training accuracy :  0.873176\n",
      "i: 18500 , cost :  0.376443 , training accuracy :  0.848485\n",
      "i: 18600 , cost :  0.390868 , training accuracy :  0.851852\n",
      "i: 18700 , cost :  0.35407 , training accuracy :  0.867565\n",
      "i: 18800 , cost :  0.340683 , training accuracy :  0.868687\n",
      "i: 18900 , cost :  0.34194 , training accuracy :  0.870932\n",
      "i: 19000 , cost :  0.371606 , training accuracy :  0.849607\n",
      "i: 19100 , cost :  0.362146 , training accuracy :  0.848485\n",
      "i: 19200 , cost :  0.567428 , training accuracy :  0.820426\n",
      "i: 19300 , cost :  0.341622 , training accuracy :  0.869809\n",
      "i: 19400 , cost :  0.364457 , training accuracy :  0.848485\n",
      "i: 19500 , cost :  0.384935 , training accuracy :  0.851852\n",
      "i: 19600 , cost :  0.379554 , training accuracy :  0.843996\n",
      "i: 19700 , cost :  0.351359 , training accuracy :  0.874299\n",
      "i: 19800 , cost :  0.35438 , training accuracy :  0.856341\n",
      "i: 19900 , cost :  0.373554 , training accuracy :  0.848485\n",
      "i: 20000 , cost :  0.507755 , training accuracy :  0.841751\n",
      "i: 20100 , cost :  0.428964 , training accuracy :  0.803591\n",
      "i: 20200 , cost :  0.399599 , training accuracy :  0.829405\n",
      "i: 20300 , cost :  0.380865 , training accuracy :  0.839506\n",
      "i: 20400 , cost :  0.379836 , training accuracy :  0.839506\n",
      "i: 20500 , cost :  0.52594 , training accuracy :  0.829405\n",
      "i: 20600 , cost :  0.381496 , training accuracy :  0.85073\n",
      "i: 20700 , cost :  0.340306 , training accuracy :  0.877666\n",
      "i: 20800 , cost :  0.390173 , training accuracy :  0.833894\n",
      "i: 20900 , cost :  0.423897 , training accuracy :  0.852974\n",
      "i: 21000 , cost :  1.47694 , training accuracy :  0.755331\n",
      "i: 21100 , cost :  0.559803 , training accuracy :  0.85073\n",
      "i: 21200 , cost :  0.350275 , training accuracy :  0.874299\n",
      "i: 21300 , cost :  0.334829 , training accuracy :  0.877666\n",
      "i: 21400 , cost :  0.33177 , training accuracy :  0.878788\n",
      "i: 21500 , cost :  0.338593 , training accuracy :  0.866442\n",
      "i: 21600 , cost :  0.359849 , training accuracy :  0.847363\n",
      "i: 21700 , cost :  0.332556 , training accuracy :  0.874299\n",
      "i: 21800 , cost :  0.340072 , training accuracy :  0.867565\n",
      "i: 21900 , cost :  0.358633 , training accuracy :  0.851852\n",
      "i: 22000 , cost :  0.56656 , training accuracy :  0.818182\n",
      "i: 22100 , cost :  0.346254 , training accuracy :  0.858586\n",
      "i: 22200 , cost :  0.337521 , training accuracy :  0.867565\n",
      "i: 22300 , cost :  0.490987 , training accuracy :  0.835017\n",
      "i: 22400 , cost :  0.397492 , training accuracy :  0.83165\n",
      "i: 22500 , cost :  0.340058 , training accuracy :  0.882155\n",
      "i: 22600 , cost :  0.794557 , training accuracy :  0.751964\n",
      "i: 22700 , cost :  0.35248 , training accuracy :  0.875421\n",
      "i: 22800 , cost :  0.330532 , training accuracy :  0.877666\n",
      "i: 22900 , cost :  0.329106 , training accuracy :  0.878788\n",
      "i: 23000 , cost :  0.334363 , training accuracy :  0.86532\n",
      "i: 23100 , cost :  0.362904 , training accuracy :  0.848485\n",
      "i: 23200 , cost :  0.435088 , training accuracy :  0.796857\n",
      "i: 23300 , cost :  0.355315 , training accuracy :  0.860831\n",
      "i: 23400 , cost :  0.331816 , training accuracy :  0.87991\n",
      "i: 23500 , cost :  0.353316 , training accuracy :  0.867565\n",
      "i: 23600 , cost :  0.332343 , training accuracy :  0.869809\n",
      "i: 23700 , cost :  0.546422 , training accuracy :  0.821549\n",
      "i: 23800 , cost :  0.476063 , training accuracy :  0.843996\n",
      "i: 23900 , cost :  0.359769 , training accuracy :  0.854097\n",
      "i: 24000 , cost :  0.413053 , training accuracy :  0.852974\n",
      "i: 24100 , cost :  0.484337 , training accuracy :  0.839506\n",
      "i: 24200 , cost :  0.410962 , training accuracy :  0.823793\n",
      "i: 24300 , cost :  0.500494 , training accuracy :  0.840629\n",
      "i: 24400 , cost :  0.486527 , training accuracy :  0.84624\n",
      "i: 24500 , cost :  0.328964 , training accuracy :  0.876543\n",
      "i: 24600 , cost :  0.336164 , training accuracy :  0.87991\n",
      "i: 24700 , cost :  0.473115 , training accuracy :  0.848485\n",
      "i: 24800 , cost :  0.444632 , training accuracy :  0.849607\n",
      "i: 24900 , cost :  0.503052 , training accuracy :  0.835017\n",
      "i: 25000 , cost :  0.331094 , training accuracy :  0.877666\n",
      "i: 25100 , cost :  0.328767 , training accuracy :  0.874299\n",
      "i: 25200 , cost :  0.574423 , training accuracy :  0.830527\n",
      "i: 25300 , cost :  0.331685 , training accuracy :  0.874299\n",
      "i: 25400 , cost :  0.328994 , training accuracy :  0.868687\n",
      "i: 25500 , cost :  0.375386 , training accuracy :  0.858586\n",
      "i: 25600 , cost :  0.400163 , training accuracy :  0.820426\n",
      "i: 25700 , cost :  0.339303 , training accuracy :  0.872054\n",
      "i: 25800 , cost :  0.332032 , training accuracy :  0.882155\n",
      "i: 25900 , cost :  0.396134 , training accuracy :  0.830527\n",
      "i: 26000 , cost :  0.42661 , training accuracy :  0.852974\n",
      "i: 26100 , cost :  0.329201 , training accuracy :  0.881033\n",
      "i: 26200 , cost :  0.399704 , training accuracy :  0.82716\n",
      "i: 26300 , cost :  0.376875 , training accuracy :  0.859708\n",
      "i: 26400 , cost :  0.349302 , training accuracy :  0.863075\n",
      "i: 26500 , cost :  0.42309 , training accuracy :  0.806958\n",
      "i: 26600 , cost :  0.350241 , training accuracy :  0.861953\n",
      "i: 26700 , cost :  0.372832 , training accuracy :  0.858586\n",
      "i: 26800 , cost :  0.412236 , training accuracy :  0.81257\n",
      "i: 26900 , cost :  0.327836 , training accuracy :  0.870932\n",
      "i: 27000 , cost :  0.410163 , training accuracy :  0.857464\n",
      "i: 27100 , cost :  0.408639 , training accuracy :  0.818182\n",
      "i: 27200 , cost :  0.327711 , training accuracy :  0.882155\n",
      "i: 27300 , cost :  0.426825 , training accuracy :  0.851852\n",
      "i: 27400 , cost :  0.430006 , training accuracy :  0.803591\n",
      "i: 27500 , cost :  0.325726 , training accuracy :  0.876543\n",
      "i: 27600 , cost :  0.418641 , training accuracy :  0.85073\n",
      "i: 27700 , cost :  0.395089 , training accuracy :  0.835017\n",
      "i: 27800 , cost :  0.32643 , training accuracy :  0.881033\n",
      "i: 27900 , cost :  0.371986 , training accuracy :  0.861953\n",
      "i: 28000 , cost :  0.397486 , training accuracy :  0.83165\n",
      "i: 28100 , cost :  0.325269 , training accuracy :  0.873176\n",
      "i: 28200 , cost :  0.43795 , training accuracy :  0.851852\n",
      "i: 28300 , cost :  0.429182 , training accuracy :  0.800224\n",
      "i: 28400 , cost :  0.332543 , training accuracy :  0.8844\n",
      "i: 28500 , cost :  0.343011 , training accuracy :  0.869809\n",
      "i: 28600 , cost :  0.331089 , training accuracy :  0.869809\n",
      "i: 28700 , cost :  0.546994 , training accuracy :  0.820426\n",
      "i: 28800 , cost :  0.36763 , training accuracy :  0.847363\n",
      "i: 28900 , cost :  0.544284 , training accuracy :  0.824916\n",
      "i: 29000 , cost :  0.33939 , training accuracy :  0.863075\n",
      "i: 29100 , cost :  0.38252 , training accuracy :  0.840629\n",
      "i: 29200 , cost :  0.523884 , training accuracy :  0.82716\n",
      "i: 29300 , cost :  0.414998 , training accuracy :  0.815937\n",
      "i: 29400 , cost :  0.326467 , training accuracy :  0.872054\n",
      "i: 29500 , cost :  0.436253 , training accuracy :  0.85073\n",
      "i: 29600 , cost :  0.420248 , training accuracy :  0.81257\n",
      "i: 29700 , cost :  0.324528 , training accuracy :  0.876543\n",
      "i: 29800 , cost :  0.457534 , training accuracy :  0.84624\n",
      "i: 29900 , cost :  0.370291 , training accuracy :  0.847363\n",
      "Final training accuracy :  0.876543\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 30000 ][hidden: 50 ][file: bhavul_tr_acc_0.88_prediction.csv ] ACCURACY :  0.876543\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  546.116 , training accuracy :  0.62963\n",
      "i: 100 , cost :  27.6822 , training accuracy :  0.617284\n",
      "i: 200 , cost :  10.0888 , training accuracy :  0.630752\n",
      "i: 300 , cost :  6.29591 , training accuracy :  0.676768\n",
      "i: 400 , cost :  4.0278 , training accuracy :  0.700337\n",
      "i: 500 , cost :  3.32806 , training accuracy :  0.75982\n",
      "i: 600 , cost :  2.92714 , training accuracy :  0.773288\n",
      "i: 700 , cost :  2.34481 , training accuracy :  0.760943\n",
      "i: 800 , cost :  2.27021 , training accuracy :  0.781145\n",
      "i: 900 , cost :  2.05326 , training accuracy :  0.7789\n",
      "i: 1000 , cost :  1.80291 , training accuracy :  0.762065\n",
      "i: 1100 , cost :  1.5863 , training accuracy :  0.766554\n",
      "i: 1200 , cost :  2.27291 , training accuracy :  0.691358\n",
      "i: 1300 , cost :  1.25206 , training accuracy :  0.795735\n",
      "i: 1400 , cost :  1.25267 , training accuracy :  0.776655\n",
      "i: 1500 , cost :  3.79565 , training accuracy :  0.727273\n",
      "i: 1600 , cost :  1.13097 , training accuracy :  0.801347\n",
      "i: 1700 , cost :  1.06732 , training accuracy :  0.813693\n",
      "i: 1800 , cost :  0.972267 , training accuracy :  0.808081\n",
      "i: 1900 , cost :  1.21058 , training accuracy :  0.79349\n",
      "i: 2000 , cost :  1.70961 , training accuracy :  0.772166\n",
      "i: 2100 , cost :  0.924223 , training accuracy :  0.815937\n",
      "i: 2200 , cost :  0.877491 , training accuracy :  0.819304\n",
      "i: 2300 , cost :  0.825465 , training accuracy :  0.815937\n",
      "i: 2400 , cost :  7.54412 , training accuracy :  0.429854\n",
      "i: 2500 , cost :  0.82117 , training accuracy :  0.826038\n",
      "i: 2600 , cost :  0.76986 , training accuracy :  0.820426\n",
      "i: 2700 , cost :  1.0899 , training accuracy :  0.787879\n",
      "i: 2800 , cost :  0.748082 , training accuracy :  0.829405\n",
      "i: 2900 , cost :  0.810706 , training accuracy :  0.79349\n",
      "i: 3000 , cost :  1.20283 , training accuracy :  0.740741\n",
      "i: 3100 , cost :  0.709336 , training accuracy :  0.839506\n",
      "i: 3200 , cost :  0.676458 , training accuracy :  0.836139\n",
      "i: 3300 , cost :  0.705461 , training accuracy :  0.828283\n",
      "i: 3400 , cost :  0.653686 , training accuracy :  0.835017\n",
      "i: 3500 , cost :  1.767 , training accuracy :  0.792368\n",
      "i: 3600 , cost :  1.65035 , training accuracy :  0.676768\n",
      "i: 3700 , cost :  1.59473 , training accuracy :  0.670034\n",
      "i: 3800 , cost :  1.2954 , training accuracy :  0.721661\n",
      "i: 3900 , cost :  0.682471 , training accuracy :  0.845118\n",
      "i: 4000 , cost :  0.614775 , training accuracy :  0.84624\n",
      "i: 4100 , cost :  0.58566 , training accuracy :  0.836139\n",
      "i: 4200 , cost :  1.3823 , training accuracy :  0.782267\n",
      "i: 4300 , cost :  0.596591 , training accuracy :  0.839506\n",
      "i: 4400 , cost :  0.569112 , training accuracy :  0.841751\n",
      "i: 4500 , cost :  1.09015 , training accuracy :  0.818182\n",
      "i: 4600 , cost :  1.06809 , training accuracy :  0.810326\n",
      "i: 4700 , cost :  0.769875 , training accuracy :  0.847363\n",
      "i: 4800 , cost :  0.604377 , training accuracy :  0.852974\n",
      "i: 4900 , cost :  0.545299 , training accuracy :  0.849607\n",
      "i: 5000 , cost :  0.521817 , training accuracy :  0.843996\n",
      "i: 5100 , cost :  0.566061 , training accuracy :  0.826038\n",
      "i: 5200 , cost :  1.67488 , training accuracy :  0.657688\n",
      "i: 5300 , cost :  1.53283 , training accuracy :  0.6633\n",
      "i: 5400 , cost :  1.06793 , training accuracy :  0.741863\n",
      "i: 5500 , cost :  0.563499 , training accuracy :  0.860831\n",
      "i: 5600 , cost :  0.510539 , training accuracy :  0.852974\n",
      "i: 5700 , cost :  0.487985 , training accuracy :  0.851852\n",
      "i: 5800 , cost :  0.735512 , training accuracy :  0.786756\n",
      "i: 5900 , cost :  0.539883 , training accuracy :  0.859708\n",
      "i: 6000 , cost :  0.47516 , training accuracy :  0.856341\n",
      "i: 6100 , cost :  1.77463 , training accuracy :  0.5578\n",
      "i: 6200 , cost :  2.04292 , training accuracy :  0.578002\n",
      "i: 6300 , cost :  1.29555 , training accuracy :  0.69248\n",
      "i: 6400 , cost :  0.965018 , training accuracy :  0.769921\n",
      "i: 6500 , cost :  0.500057 , training accuracy :  0.86532\n",
      "i: 6600 , cost :  0.459647 , training accuracy :  0.857464\n",
      "i: 6700 , cost :  0.614385 , training accuracy :  0.838384\n",
      "i: 6800 , cost :  0.868855 , training accuracy :  0.828283\n",
      "i: 6900 , cost :  0.484228 , training accuracy :  0.866442\n",
      "i: 7000 , cost :  0.440533 , training accuracy :  0.860831\n",
      "i: 7100 , cost :  0.945378 , training accuracy :  0.836139\n",
      "i: 7200 , cost :  0.861913 , training accuracy :  0.832772\n",
      "i: 7300 , cost :  0.562733 , training accuracy :  0.870932\n",
      "i: 7400 , cost :  0.565212 , training accuracy :  0.870932\n",
      "i: 7500 , cost :  0.45673 , training accuracy :  0.868687\n",
      "i: 7600 , cost :  0.422999 , training accuracy :  0.866442\n",
      "i: 7700 , cost :  0.562671 , training accuracy :  0.799102\n",
      "i: 7800 , cost :  1.06076 , training accuracy :  0.815937\n",
      "i: 7900 , cost :  0.610808 , training accuracy :  0.867565\n",
      "i: 8000 , cost :  0.474163 , training accuracy :  0.867565\n",
      "i: 8100 , cost :  0.413208 , training accuracy :  0.877666\n",
      "i: 8200 , cost :  0.39322 , training accuracy :  0.881033\n",
      "i: 8300 , cost :  0.416507 , training accuracy :  0.870932\n",
      "i: 8400 , cost :  1.88554 , training accuracy :  0.537598\n",
      "i: 8500 , cost :  1.93139 , training accuracy :  0.618406\n",
      "i: 8600 , cost :  1.9272 , training accuracy :  0.561167\n",
      "i: 8700 , cost :  0.423493 , training accuracy :  0.872054\n",
      "i: 8800 , cost :  0.380405 , training accuracy :  0.881033\n",
      "i: 8900 , cost :  0.427164 , training accuracy :  0.863075\n",
      "i: 9000 , cost :  0.489403 , training accuracy :  0.84624\n",
      "i: 9100 , cost :  0.497792 , training accuracy :  0.869809\n",
      "i: 9200 , cost :  0.4697 , training accuracy :  0.875421\n",
      "i: 9300 , cost :  0.384192 , training accuracy :  0.87991\n",
      "i: 9400 , cost :  0.359787 , training accuracy :  0.883277\n",
      "i: 9500 , cost :  4.82398 , training accuracy :  0.452301\n",
      "i: 9600 , cost :  1.56316 , training accuracy :  0.65881\n",
      "i: 9700 , cost :  0.848001 , training accuracy :  0.795735\n",
      "i: 9800 , cost :  0.383082 , training accuracy :  0.87991\n",
      "i: 9900 , cost :  0.348749 , training accuracy :  0.885522\n",
      "i: 10000 , cost :  0.344023 , training accuracy :  0.878788\n",
      "i: 10100 , cost :  0.457099 , training accuracy :  0.838384\n",
      "i: 10200 , cost :  0.566467 , training accuracy :  0.868687\n",
      "i: 10300 , cost :  0.472707 , training accuracy :  0.870932\n",
      "i: 10400 , cost :  0.356821 , training accuracy :  0.882155\n",
      "i: 10500 , cost :  0.32941 , training accuracy :  0.883277\n",
      "i: 10600 , cost :  0.364836 , training accuracy :  0.873176\n",
      "i: 10700 , cost :  0.838462 , training accuracy :  0.840629\n",
      "i: 10800 , cost :  0.419359 , training accuracy :  0.882155\n",
      "i: 10900 , cost :  0.340315 , training accuracy :  0.882155\n",
      "i: 11000 , cost :  0.318843 , training accuracy :  0.886644\n",
      "i: 11100 , cost :  0.324788 , training accuracy :  0.886644\n",
      "i: 11200 , cost :  0.378072 , training accuracy :  0.870932\n",
      "i: 11300 , cost :  0.39039 , training accuracy :  0.868687\n",
      "i: 11400 , cost :  0.403772 , training accuracy :  0.864198\n",
      "i: 11500 , cost :  2.25862 , training accuracy :  0.787879\n",
      "i: 11600 , cost :  1.04662 , training accuracy :  0.838384\n",
      "i: 11700 , cost :  0.406585 , training accuracy :  0.882155\n",
      "i: 11800 , cost :  0.323642 , training accuracy :  0.890011\n",
      "i: 11900 , cost :  0.305037 , training accuracy :  0.886644\n",
      "i: 12000 , cost :  1.54159 , training accuracy :  0.806958\n",
      "i: 12100 , cost :  0.433917 , training accuracy :  0.8844\n",
      "i: 12200 , cost :  0.329416 , training accuracy :  0.888889\n",
      "i: 12300 , cost :  0.30457 , training accuracy :  0.890011\n",
      "i: 12400 , cost :  1.56006 , training accuracy :  0.808081\n",
      "i: 12500 , cost :  0.760662 , training accuracy :  0.839506\n",
      "i: 12600 , cost :  0.373546 , training accuracy :  0.8844\n",
      "i: 12700 , cost :  0.312083 , training accuracy :  0.892256\n",
      "i: 12800 , cost :  0.297905 , training accuracy :  0.892256\n",
      "i: 12900 , cost :  0.346716 , training accuracy :  0.870932\n",
      "i: 13000 , cost :  0.343463 , training accuracy :  0.875421\n",
      "i: 13100 , cost :  0.389041 , training accuracy :  0.861953\n",
      "i: 13200 , cost :  0.362089 , training accuracy :  0.870932\n",
      "i: 13300 , cost :  0.801755 , training accuracy :  0.849607\n",
      "i: 13400 , cost :  0.442497 , training accuracy :  0.886644\n",
      "i: 13500 , cost :  0.329493 , training accuracy :  0.888889\n",
      "i: 13600 , cost :  0.294277 , training accuracy :  0.894501\n",
      "i: 13700 , cost :  2.7055 , training accuracy :  0.547699\n",
      "i: 13800 , cost :  1.0101 , training accuracy :  0.747475\n",
      "i: 13900 , cost :  0.35227 , training accuracy :  0.890011\n",
      "i: 14000 , cost :  0.299724 , training accuracy :  0.896745\n",
      "i: 14100 , cost :  0.287568 , training accuracy :  0.896745\n",
      "i: 14200 , cost :  2.49665 , training accuracy :  0.531987\n",
      "i: 14300 , cost :  0.742913 , training accuracy :  0.808081\n",
      "i: 14400 , cost :  0.32046 , training accuracy :  0.891134\n",
      "i: 14500 , cost :  0.290172 , training accuracy :  0.896745\n",
      "i: 14600 , cost :  0.417471 , training accuracy :  0.852974\n",
      "i: 14700 , cost :  2.20738 , training accuracy :  0.792368\n",
      "i: 14800 , cost :  0.306604 , training accuracy :  0.892256\n",
      "i: 14900 , cost :  0.284978 , training accuracy :  0.897868\n",
      "i: 15000 , cost :  1.19729 , training accuracy :  0.637486\n",
      "i: 15100 , cost :  0.298541 , training accuracy :  0.894501\n",
      "i: 15200 , cost :  0.281774 , training accuracy :  0.897868\n",
      "i: 15300 , cost :  0.358474 , training accuracy :  0.8844\n",
      "i: 15400 , cost :  0.284776 , training accuracy :  0.895623\n",
      "i: 15500 , cost :  1.11208 , training accuracy :  0.719416\n",
      "i: 15600 , cost :  0.817677 , training accuracy :  0.763187\n",
      "i: 15700 , cost :  0.317638 , training accuracy :  0.894501\n",
      "i: 15800 , cost :  0.284629 , training accuracy :  0.895623\n",
      "i: 15900 , cost :  0.29421 , training accuracy :  0.878788\n",
      "i: 16000 , cost :  0.467077 , training accuracy :  0.872054\n",
      "i: 16100 , cost :  0.689322 , training accuracy :  0.859708\n",
      "i: 16200 , cost :  0.318986 , training accuracy :  0.895623\n",
      "i: 16300 , cost :  0.283923 , training accuracy :  0.896745\n",
      "i: 16400 , cost :  0.321643 , training accuracy :  0.887767\n",
      "i: 16500 , cost :  1.62999 , training accuracy :  0.792368\n",
      "i: 16600 , cost :  0.292341 , training accuracy :  0.893378\n",
      "i: 16700 , cost :  0.27697 , training accuracy :  0.89899\n",
      "i: 16800 , cost :  0.301249 , training accuracy :  0.892256\n",
      "i: 16900 , cost :  0.681083 , training accuracy :  0.851852\n",
      "i: 17000 , cost :  0.392892 , training accuracy :  0.891134\n",
      "i: 17100 , cost :  0.284508 , training accuracy :  0.893378\n",
      "i: 17200 , cost :  0.354261 , training accuracy :  0.866442\n",
      "i: 17300 , cost :  1.49371 , training accuracy :  0.823793\n",
      "i: 17400 , cost :  0.361438 , training accuracy :  0.888889\n",
      "i: 17500 , cost :  0.292812 , training accuracy :  0.888889\n",
      "i: 17600 , cost :  0.276238 , training accuracy :  0.895623\n",
      "i: 17700 , cost :  2.3541 , training accuracy :  0.552189\n",
      "i: 17800 , cost :  0.427958 , training accuracy :  0.877666\n",
      "i: 17900 , cost :  0.304243 , training accuracy :  0.892256\n",
      "i: 18000 , cost :  0.279736 , training accuracy :  0.892256\n",
      "i: 18100 , cost :  0.307267 , training accuracy :  0.885522\n",
      "i: 18200 , cost :  0.334634 , training accuracy :  0.875421\n",
      "i: 18300 , cost :  0.343064 , training accuracy :  0.875421\n",
      "i: 18400 , cost :  0.386482 , training accuracy :  0.864198\n",
      "i: 18500 , cost :  0.86283 , training accuracy :  0.811448\n",
      "i: 18600 , cost :  0.386425 , training accuracy :  0.8844\n",
      "i: 18700 , cost :  0.28339 , training accuracy :  0.890011\n",
      "i: 18800 , cost :  0.634259 , training accuracy :  0.851852\n",
      "i: 18900 , cost :  0.27524 , training accuracy :  0.893378\n",
      "i: 19000 , cost :  0.398448 , training accuracy :  0.876543\n",
      "i: 19100 , cost :  0.77222 , training accuracy :  0.849607\n",
      "i: 19200 , cost :  0.322027 , training accuracy :  0.894501\n",
      "i: 19300 , cost :  0.279162 , training accuracy :  0.896745\n",
      "i: 19400 , cost :  0.312859 , training accuracy :  0.881033\n",
      "i: 19500 , cost :  0.27535 , training accuracy :  0.896745\n",
      "i: 19600 , cost :  1.25891 , training accuracy :  0.829405\n",
      "i: 19700 , cost :  0.390602 , training accuracy :  0.886644\n",
      "i: 19800 , cost :  0.32324 , training accuracy :  0.895623\n",
      "i: 19900 , cost :  0.27799 , training accuracy :  0.895623\n",
      "i: 20000 , cost :  0.32826 , training accuracy :  0.875421\n",
      "i: 20100 , cost :  0.320113 , training accuracy :  0.877666\n",
      "i: 20200 , cost :  0.3414 , training accuracy :  0.877666\n",
      "i: 20300 , cost :  0.331291 , training accuracy :  0.876543\n",
      "i: 20400 , cost :  2.97033 , training accuracy :  0.756453\n",
      "i: 20500 , cost :  0.29887 , training accuracy :  0.896745\n",
      "i: 20600 , cost :  0.839728 , training accuracy :  0.828283\n",
      "i: 20700 , cost :  0.338149 , training accuracy :  0.894501\n",
      "i: 20800 , cost :  0.270202 , training accuracy :  0.892256\n",
      "i: 20900 , cost :  0.459754 , training accuracy :  0.842873\n",
      "i: 21000 , cost :  0.311918 , training accuracy :  0.893378\n",
      "i: 21100 , cost :  0.268394 , training accuracy :  0.896745\n",
      "i: 21200 , cost :  0.323187 , training accuracy :  0.885522\n",
      "i: 21300 , cost :  1.70068 , training accuracy :  0.61055\n",
      "i: 21400 , cost :  0.428811 , training accuracy :  0.888889\n",
      "i: 21500 , cost :  0.427979 , training accuracy :  0.894501\n",
      "i: 21600 , cost :  0.295046 , training accuracy :  0.896745\n",
      "i: 21700 , cost :  0.267835 , training accuracy :  0.896745\n",
      "i: 21800 , cost :  2.33944 , training accuracy :  0.804714\n",
      "i: 21900 , cost :  0.282852 , training accuracy :  0.894501\n",
      "i: 22000 , cost :  0.265541 , training accuracy :  0.897868\n",
      "i: 22100 , cost :  0.994771 , training accuracy :  0.717172\n",
      "i: 22200 , cost :  0.7991 , training accuracy :  0.781145\n",
      "i: 22300 , cost :  0.330466 , training accuracy :  0.895623\n",
      "i: 22400 , cost :  0.276562 , training accuracy :  0.894501\n",
      "i: 22500 , cost :  0.263639 , training accuracy :  0.896745\n",
      "i: 22600 , cost :  0.309195 , training accuracy :  0.877666\n",
      "i: 22700 , cost :  0.267893 , training accuracy :  0.896745\n",
      "i: 22800 , cost :  2.77373 , training accuracy :  0.527497\n",
      "i: 22900 , cost :  0.428749 , training accuracy :  0.890011\n",
      "i: 23000 , cost :  0.363118 , training accuracy :  0.896745\n",
      "i: 23100 , cost :  0.282995 , training accuracy :  0.895623\n",
      "i: 23200 , cost :  0.263116 , training accuracy :  0.897868\n",
      "i: 23300 , cost :  0.515041 , training accuracy :  0.832772\n",
      "i: 23400 , cost :  0.308705 , training accuracy :  0.896745\n",
      "i: 23500 , cost :  0.263234 , training accuracy :  0.89899\n",
      "i: 23600 , cost :  2.64477 , training accuracy :  0.800224\n",
      "i: 23700 , cost :  1.02444 , training accuracy :  0.84624\n",
      "i: 23800 , cost :  0.372085 , training accuracy :  0.893378\n",
      "i: 23900 , cost :  0.287378 , training accuracy :  0.896745\n",
      "i: 24000 , cost :  0.263886 , training accuracy :  0.897868\n",
      "i: 24100 , cost :  0.440516 , training accuracy :  0.819304\n",
      "i: 24200 , cost :  0.293321 , training accuracy :  0.886644\n",
      "i: 24300 , cost :  0.264793 , training accuracy :  0.901235\n",
      "i: 24400 , cost :  0.274737 , training accuracy :  0.902357\n",
      "i: 24500 , cost :  1.29646 , training accuracy :  0.840629\n",
      "i: 24600 , cost :  1.05386 , training accuracy :  0.84624\n",
      "i: 24700 , cost :  0.356494 , training accuracy :  0.895623\n",
      "i: 24800 , cost :  0.280672 , training accuracy :  0.895623\n",
      "i: 24900 , cost :  0.260173 , training accuracy :  0.901235\n",
      "i: 25000 , cost :  2.22015 , training accuracy :  0.782267\n",
      "i: 25100 , cost :  1.07201 , training accuracy :  0.710438\n",
      "i: 25200 , cost :  0.340714 , training accuracy :  0.894501\n",
      "i: 25300 , cost :  0.2766 , training accuracy :  0.900112\n",
      "i: 25400 , cost :  0.259536 , training accuracy :  0.89899\n",
      "i: 25500 , cost :  0.257275 , training accuracy :  0.900112\n",
      "i: 25600 , cost :  0.282194 , training accuracy :  0.895623\n",
      "i: 25700 , cost :  0.268396 , training accuracy :  0.902357\n",
      "i: 25800 , cost :  0.425709 , training accuracy :  0.887767\n",
      "i: 25900 , cost :  0.396866 , training accuracy :  0.891134\n",
      "i: 26000 , cost :  0.294189 , training accuracy :  0.895623\n",
      "i: 26100 , cost :  0.261592 , training accuracy :  0.900112\n",
      "i: 26200 , cost :  0.585006 , training accuracy :  0.845118\n",
      "i: 26300 , cost :  1.13509 , training accuracy :  0.847363\n",
      "i: 26400 , cost :  0.368647 , training accuracy :  0.896745\n",
      "i: 26500 , cost :  0.286254 , training accuracy :  0.896745\n",
      "i: 26600 , cost :  0.260362 , training accuracy :  0.901235\n",
      "i: 26700 , cost :  0.288049 , training accuracy :  0.894501\n",
      "i: 26800 , cost :  0.256498 , training accuracy :  0.904602\n",
      "i: 26900 , cost :  2.30586 , training accuracy :  0.79349\n",
      "i: 27000 , cost :  0.659319 , training accuracy :  0.86532\n",
      "i: 27100 , cost :  0.310024 , training accuracy :  0.897868\n",
      "i: 27200 , cost :  0.264707 , training accuracy :  0.901235\n",
      "i: 27300 , cost :  0.253649 , training accuracy :  0.904602\n",
      "i: 27400 , cost :  0.352874 , training accuracy :  0.8844\n",
      "i: 27500 , cost :  0.266995 , training accuracy :  0.903479\n",
      "i: 27600 , cost :  0.274878 , training accuracy :  0.897868\n",
      "i: 27700 , cost :  0.301897 , training accuracy :  0.883277\n",
      "i: 27800 , cost :  0.35973 , training accuracy :  0.87991\n",
      "i: 27900 , cost :  0.257075 , training accuracy :  0.903479\n",
      "i: 28000 , cost :  0.540405 , training accuracy :  0.837261\n",
      "i: 28100 , cost :  0.559405 , training accuracy :  0.868687\n",
      "i: 28200 , cost :  0.386893 , training accuracy :  0.893378\n",
      "i: 28300 , cost :  0.286239 , training accuracy :  0.895623\n",
      "i: 28400 , cost :  0.257047 , training accuracy :  0.907969\n",
      "i: 28500 , cost :  0.263227 , training accuracy :  0.900112\n",
      "i: 28600 , cost :  0.284844 , training accuracy :  0.895623\n",
      "i: 28700 , cost :  0.293241 , training accuracy :  0.891134\n",
      "i: 28800 , cost :  0.300103 , training accuracy :  0.883277\n",
      "i: 28900 , cost :  2.38989 , training accuracy :  0.804714\n",
      "i: 29000 , cost :  0.262287 , training accuracy :  0.904602\n",
      "i: 29100 , cost :  4.12222 , training accuracy :  0.767677\n",
      "i: 29200 , cost :  0.284022 , training accuracy :  0.901235\n",
      "i: 29300 , cost :  0.250108 , training accuracy :  0.904602\n",
      "i: 29400 , cost :  2.18857 , training accuracy :  0.813693\n",
      "i: 29500 , cost :  0.301831 , training accuracy :  0.897868\n",
      "i: 29600 , cost :  0.251804 , training accuracy :  0.905724\n",
      "i: 29700 , cost :  2.75875 , training accuracy :  0.786756\n",
      "i: 29800 , cost :  0.26381 , training accuracy :  0.905724\n",
      "i: 29900 , cost :  0.254531 , training accuracy :  0.906846\n",
      "Final training accuracy :  0.634119\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.002 ][epoch: 30000 ][hidden: 100 ][file: bhavul_tr_acc_0.63_prediction.csv ] ACCURACY :  0.634119\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  159.797 , training accuracy :  0.618406\n",
      "i: 100 , cost :  30.4009 , training accuracy :  0.65881\n",
      "i: 200 , cost :  0.66009 , training accuracy :  0.659933\n",
      "i: 300 , cost :  0.650186 , training accuracy :  0.653199\n",
      "i: 400 , cost :  0.647737 , training accuracy :  0.657688\n",
      "i: 500 , cost :  0.645861 , training accuracy :  0.65881\n",
      "i: 600 , cost :  0.644551 , training accuracy :  0.656566\n",
      "i: 700 , cost :  0.643706 , training accuracy :  0.65881\n",
      "i: 800 , cost :  0.643201 , training accuracy :  0.661055\n",
      "i: 900 , cost :  0.642918 , training accuracy :  0.665544\n",
      "Final training accuracy :  0.664422\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 1000 ][hidden: 3 ][file: bhavul_tr_acc_0.66_prediction.csv ] ACCURACY :  0.664422\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  235.161 , training accuracy :  0.615039\n",
      "i: 100 , cost :  6.24484 , training accuracy :  0.613917\n",
      "i: 200 , cost :  1.26253 , training accuracy :  0.62514\n",
      "i: 300 , cost :  0.65904 , training accuracy :  0.738496\n",
      "i: 400 , cost :  0.550094 , training accuracy :  0.786756\n",
      "i: 500 , cost :  0.506148 , training accuracy :  0.794613\n",
      "i: 600 , cost :  0.481276 , training accuracy :  0.796857\n",
      "i: 700 , cost :  0.49027 , training accuracy :  0.776655\n",
      "i: 800 , cost :  0.459725 , training accuracy :  0.805836\n",
      "i: 900 , cost :  0.452007 , training accuracy :  0.79798\n",
      "Final training accuracy :  0.794613\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 1000 ][hidden: 10 ][file: bhavul_tr_acc_0.79_prediction.csv ] ACCURACY :  0.794613\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  732.785 , training accuracy :  0.383838\n",
      "i: 100 , cost :  31.3533 , training accuracy :  0.618406\n",
      "i: 200 , cost :  10.0743 , training accuracy :  0.636364\n",
      "i: 300 , cost :  4.18181 , training accuracy :  0.619529\n",
      "i: 400 , cost :  1.98351 , training accuracy :  0.648709\n",
      "i: 500 , cost :  1.47276 , training accuracy :  0.705948\n",
      "i: 600 , cost :  1.27569 , training accuracy :  0.720539\n",
      "i: 700 , cost :  1.14704 , training accuracy :  0.729517\n",
      "i: 800 , cost :  1.00878 , training accuracy :  0.736251\n",
      "i: 900 , cost :  0.87308 , training accuracy :  0.738496\n",
      "Final training accuracy :  0.750842\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 1000 ][hidden: 15 ][file: bhavul_tr_acc_0.75_prediction.csv ] ACCURACY :  0.750842\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  161.124 , training accuracy :  0.535354\n",
      "i: 100 , cost :  3.73507 , training accuracy :  0.699214\n",
      "i: 200 , cost :  1.70494 , training accuracy :  0.774411\n",
      "i: 300 , cost :  1.1029 , training accuracy :  0.7789\n",
      "i: 400 , cost :  6.29845 , training accuracy :  0.420875\n",
      "i: 500 , cost :  1.20548 , training accuracy :  0.79798\n",
      "i: 600 , cost :  0.812187 , training accuracy :  0.800224\n",
      "i: 700 , cost :  6.28309 , training accuracy :  0.411897\n",
      "i: 800 , cost :  0.958603 , training accuracy :  0.800224\n",
      "i: 900 , cost :  0.633287 , training accuracy :  0.800224\n",
      "Final training accuracy :  0.721661\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 1000 ][hidden: 50 ][file: bhavul_tr_acc_0.72_prediction.csv ] ACCURACY :  0.721661\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  916.05 , training accuracy :  0.365881\n",
      "i: 100 , cost :  7.58109 , training accuracy :  0.695847\n",
      "i: 200 , cost :  4.36704 , training accuracy :  0.747475\n",
      "i: 300 , cost :  2.71895 , training accuracy :  0.772166\n",
      "i: 400 , cost :  4.66112 , training accuracy :  0.754209\n",
      "i: 500 , cost :  1.88584 , training accuracy :  0.791246\n",
      "i: 600 , cost :  4.49472 , training accuracy :  0.631874\n",
      "i: 700 , cost :  1.48281 , training accuracy :  0.802469\n",
      "i: 800 , cost :  1.49654 , training accuracy :  0.795735\n",
      "i: 900 , cost :  3.55415 , training accuracy :  0.766554\n",
      "Final training accuracy :  0.836139\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 1000 ][hidden: 100 ][file: bhavul_tr_acc_0.84_prediction.csv ] ACCURACY :  0.836139\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  16.0418 , training accuracy :  0.616162\n",
      "i: 100 , cost :  0.731274 , training accuracy :  0.372615\n",
      "i: 200 , cost :  0.687555 , training accuracy :  0.616162\n",
      "i: 300 , cost :  0.670757 , training accuracy :  0.616162\n",
      "i: 400 , cost :  0.663822 , training accuracy :  0.624018\n",
      "i: 500 , cost :  0.6605 , training accuracy :  0.62963\n",
      "i: 600 , cost :  0.657888 , training accuracy :  0.630752\n",
      "i: 700 , cost :  0.654602 , training accuracy :  0.62963\n",
      "i: 800 , cost :  0.651894 , training accuracy :  0.636364\n",
      "i: 900 , cost :  0.65129 , training accuracy :  0.637486\n",
      "i: 1000 , cost :  0.650882 , training accuracy :  0.637486\n",
      "i: 1100 , cost :  0.650422 , training accuracy :  0.636364\n",
      "i: 1200 , cost :  0.649936 , training accuracy :  0.638608\n",
      "i: 1300 , cost :  0.649461 , training accuracy :  0.638608\n",
      "i: 1400 , cost :  0.648929 , training accuracy :  0.638608\n",
      "i: 1500 , cost :  0.648437 , training accuracy :  0.638608\n",
      "i: 1600 , cost :  0.647871 , training accuracy :  0.636364\n",
      "i: 1700 , cost :  0.647175 , training accuracy :  0.637486\n",
      "i: 1800 , cost :  0.646214 , training accuracy :  0.639731\n",
      "i: 1900 , cost :  0.645037 , training accuracy :  0.635241\n",
      "i: 2000 , cost :  0.643659 , training accuracy :  0.632997\n",
      "i: 2100 , cost :  0.642314 , training accuracy :  0.638608\n",
      "i: 2200 , cost :  0.640739 , training accuracy :  0.640853\n",
      "i: 2300 , cost :  0.638219 , training accuracy :  0.640853\n",
      "i: 2400 , cost :  0.635174 , training accuracy :  0.641975\n",
      "i: 2500 , cost :  0.630526 , training accuracy :  0.649832\n",
      "i: 2600 , cost :  0.624883 , training accuracy :  0.655443\n",
      "i: 2700 , cost :  0.620933 , training accuracy :  0.666667\n",
      "i: 2800 , cost :  0.617348 , training accuracy :  0.67789\n",
      "i: 2900 , cost :  0.614773 , training accuracy :  0.67789\n",
      "i: 3000 , cost :  0.611561 , training accuracy :  0.681257\n",
      "i: 3100 , cost :  0.609224 , training accuracy :  0.682379\n",
      "i: 3200 , cost :  0.607201 , training accuracy :  0.684624\n",
      "i: 3300 , cost :  0.604665 , training accuracy :  0.682379\n",
      "i: 3400 , cost :  0.601929 , training accuracy :  0.682379\n",
      "i: 3500 , cost :  0.597632 , training accuracy :  0.69248\n",
      "i: 3600 , cost :  0.590326 , training accuracy :  0.703704\n",
      "i: 3700 , cost :  0.569495 , training accuracy :  0.723906\n",
      "i: 3800 , cost :  0.505435 , training accuracy :  0.783389\n",
      "i: 3900 , cost :  0.48314 , training accuracy :  0.79349\n",
      "i: 4000 , cost :  0.476066 , training accuracy :  0.795735\n",
      "i: 4100 , cost :  0.469429 , training accuracy :  0.796857\n",
      "i: 4200 , cost :  0.464104 , training accuracy :  0.805836\n",
      "i: 4300 , cost :  0.459539 , training accuracy :  0.810326\n",
      "i: 4400 , cost :  0.456033 , training accuracy :  0.808081\n",
      "i: 4500 , cost :  0.453191 , training accuracy :  0.806958\n",
      "i: 4600 , cost :  0.450516 , training accuracy :  0.805836\n",
      "i: 4700 , cost :  0.448216 , training accuracy :  0.808081\n",
      "i: 4800 , cost :  0.446366 , training accuracy :  0.808081\n",
      "i: 4900 , cost :  0.444973 , training accuracy :  0.806958\n",
      "Final training accuracy :  0.810326\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 5000 ][hidden: 3 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.810326\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  206.56 , training accuracy :  0.356902\n",
      "i: 100 , cost :  9.64299 , training accuracy :  0.640853\n",
      "i: 200 , cost :  4.45144 , training accuracy :  0.691358\n",
      "i: 300 , cost :  1.63883 , training accuracy :  0.753086\n",
      "i: 400 , cost :  1.13457 , training accuracy :  0.776655\n",
      "i: 500 , cost :  0.941892 , training accuracy :  0.771044\n",
      "i: 600 , cost :  0.840365 , training accuracy :  0.777778\n",
      "i: 700 , cost :  0.819496 , training accuracy :  0.772166\n",
      "i: 800 , cost :  0.750505 , training accuracy :  0.787879\n",
      "i: 900 , cost :  0.737149 , training accuracy :  0.776655\n",
      "i: 1000 , cost :  0.688758 , training accuracy :  0.781145\n",
      "i: 1100 , cost :  0.525613 , training accuracy :  0.796857\n",
      "i: 1200 , cost :  0.562651 , training accuracy :  0.773288\n",
      "i: 1300 , cost :  0.513193 , training accuracy :  0.795735\n",
      "i: 1400 , cost :  0.523171 , training accuracy :  0.781145\n",
      "i: 1500 , cost :  0.495192 , training accuracy :  0.783389\n",
      "i: 1600 , cost :  0.470321 , training accuracy :  0.804714\n",
      "i: 1700 , cost :  0.491453 , training accuracy :  0.802469\n",
      "i: 1800 , cost :  0.481611 , training accuracy :  0.811448\n",
      "i: 1900 , cost :  0.501367 , training accuracy :  0.803591\n",
      "i: 2000 , cost :  0.503321 , training accuracy :  0.801347\n",
      "i: 2100 , cost :  0.472333 , training accuracy :  0.810326\n",
      "i: 2200 , cost :  0.452919 , training accuracy :  0.804714\n",
      "i: 2300 , cost :  0.467297 , training accuracy :  0.791246\n",
      "i: 2400 , cost :  0.482761 , training accuracy :  0.781145\n",
      "i: 2500 , cost :  0.475451 , training accuracy :  0.783389\n",
      "i: 2600 , cost :  0.447844 , training accuracy :  0.79349\n",
      "i: 2700 , cost :  0.448693 , training accuracy :  0.818182\n",
      "i: 2800 , cost :  0.464979 , training accuracy :  0.806958\n",
      "i: 2900 , cost :  0.460434 , training accuracy :  0.810326\n",
      "i: 3000 , cost :  0.427795 , training accuracy :  0.799102\n",
      "i: 3100 , cost :  0.459331 , training accuracy :  0.783389\n",
      "i: 3200 , cost :  0.455567 , training accuracy :  0.786756\n",
      "i: 3300 , cost :  0.424593 , training accuracy :  0.808081\n",
      "i: 3400 , cost :  0.461405 , training accuracy :  0.809203\n",
      "i: 3500 , cost :  0.452257 , training accuracy :  0.811448\n",
      "i: 3600 , cost :  0.428629 , training accuracy :  0.794613\n",
      "i: 3700 , cost :  0.46026 , training accuracy :  0.786756\n",
      "i: 3800 , cost :  0.428868 , training accuracy :  0.789001\n",
      "i: 3900 , cost :  0.444656 , training accuracy :  0.811448\n",
      "i: 4000 , cost :  0.468363 , training accuracy :  0.805836\n",
      "i: 4100 , cost :  0.422262 , training accuracy :  0.805836\n",
      "i: 4200 , cost :  0.443096 , training accuracy :  0.799102\n",
      "i: 4300 , cost :  0.459872 , training accuracy :  0.785634\n",
      "i: 4400 , cost :  0.457335 , training accuracy :  0.787879\n",
      "i: 4500 , cost :  0.45797 , training accuracy :  0.787879\n",
      "i: 4600 , cost :  0.459463 , training accuracy :  0.787879\n",
      "i: 4700 , cost :  0.449705 , training accuracy :  0.79349\n",
      "i: 4800 , cost :  0.432492 , training accuracy :  0.794613\n",
      "i: 4900 , cost :  0.421417 , training accuracy :  0.799102\n",
      "Final training accuracy :  0.815937\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 5000 ][hidden: 10 ][file: bhavul_tr_acc_0.82_prediction.csv ] ACCURACY :  0.815937\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  287.301 , training accuracy :  0.383838\n",
      "i: 100 , cost :  26.4901 , training accuracy :  0.631874\n",
      "i: 200 , cost :  2.61327 , training accuracy :  0.655443\n",
      "i: 300 , cost :  1.03055 , training accuracy :  0.746352\n",
      "i: 400 , cost :  0.8206 , training accuracy :  0.775533\n",
      "i: 500 , cost :  0.676854 , training accuracy :  0.783389\n",
      "i: 600 , cost :  0.571121 , training accuracy :  0.791246\n",
      "i: 700 , cost :  0.547 , training accuracy :  0.784512\n",
      "i: 800 , cost :  0.540187 , training accuracy :  0.7789\n",
      "i: 900 , cost :  0.494479 , training accuracy :  0.799102\n",
      "i: 1000 , cost :  0.4756 , training accuracy :  0.804714\n",
      "i: 1100 , cost :  0.460599 , training accuracy :  0.814815\n",
      "i: 1200 , cost :  0.499236 , training accuracy :  0.782267\n",
      "i: 1300 , cost :  0.469677 , training accuracy :  0.799102\n",
      "i: 1400 , cost :  0.444612 , training accuracy :  0.810326\n",
      "i: 1500 , cost :  0.453404 , training accuracy :  0.809203\n",
      "i: 1600 , cost :  0.456082 , training accuracy :  0.801347\n",
      "i: 1700 , cost :  0.454178 , training accuracy :  0.802469\n",
      "i: 1800 , cost :  0.449095 , training accuracy :  0.805836\n",
      "i: 1900 , cost :  0.441608 , training accuracy :  0.81257\n",
      "i: 2000 , cost :  0.43783 , training accuracy :  0.81257\n",
      "i: 2100 , cost :  0.443873 , training accuracy :  0.808081\n",
      "i: 2200 , cost :  0.445145 , training accuracy :  0.801347\n",
      "i: 2300 , cost :  0.441158 , training accuracy :  0.803591\n",
      "i: 2400 , cost :  0.434175 , training accuracy :  0.81257\n",
      "i: 2500 , cost :  0.430355 , training accuracy :  0.813693\n",
      "i: 2600 , cost :  0.427441 , training accuracy :  0.815937\n",
      "i: 2700 , cost :  0.428007 , training accuracy :  0.809203\n",
      "i: 2800 , cost :  0.42761 , training accuracy :  0.810326\n",
      "i: 2900 , cost :  0.42488 , training accuracy :  0.81257\n",
      "i: 3000 , cost :  0.421214 , training accuracy :  0.81257\n",
      "i: 3100 , cost :  0.419462 , training accuracy :  0.81257\n",
      "i: 3200 , cost :  0.419747 , training accuracy :  0.81257\n",
      "i: 3300 , cost :  0.420659 , training accuracy :  0.814815\n",
      "i: 3400 , cost :  0.419781 , training accuracy :  0.81257\n",
      "i: 3500 , cost :  0.423356 , training accuracy :  0.808081\n",
      "i: 3600 , cost :  0.422825 , training accuracy :  0.806958\n",
      "i: 3700 , cost :  0.405059 , training accuracy :  0.826038\n",
      "i: 3800 , cost :  0.41361 , training accuracy :  0.813693\n",
      "i: 3900 , cost :  0.398536 , training accuracy :  0.822671\n",
      "i: 4000 , cost :  0.419315 , training accuracy :  0.808081\n",
      "i: 4100 , cost :  0.412377 , training accuracy :  0.813693\n",
      "i: 4200 , cost :  0.406761 , training accuracy :  0.814815\n",
      "i: 4300 , cost :  0.398347 , training accuracy :  0.823793\n",
      "i: 4400 , cost :  0.399075 , training accuracy :  0.822671\n",
      "i: 4500 , cost :  0.39497 , training accuracy :  0.828283\n",
      "i: 4600 , cost :  0.397801 , training accuracy :  0.823793\n",
      "i: 4700 , cost :  0.394717 , training accuracy :  0.828283\n",
      "i: 4800 , cost :  0.396651 , training accuracy :  0.821549\n",
      "i: 4900 , cost :  0.415557 , training accuracy :  0.81257\n",
      "Final training accuracy :  0.81257\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 5000 ][hidden: 15 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.81257\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  426.375 , training accuracy :  0.613917\n",
      "i: 100 , cost :  5.88236 , training accuracy :  0.647587\n",
      "i: 200 , cost :  2.51468 , training accuracy :  0.725028\n",
      "i: 300 , cost :  1.87008 , training accuracy :  0.76431\n",
      "i: 400 , cost :  1.60803 , training accuracy :  0.760943\n",
      "i: 500 , cost :  1.54904 , training accuracy :  0.75982\n",
      "i: 600 , cost :  1.20847 , training accuracy :  0.791246\n",
      "i: 700 , cost :  0.998355 , training accuracy :  0.784512\n",
      "i: 800 , cost :  2.1887 , training accuracy :  0.758698\n",
      "i: 900 , cost :  0.965231 , training accuracy :  0.763187\n",
      "i: 1000 , cost :  1.2213 , training accuracy :  0.777778\n",
      "i: 1100 , cost :  3.65584 , training accuracy :  0.699214\n",
      "i: 1200 , cost :  0.758068 , training accuracy :  0.820426\n",
      "i: 1300 , cost :  0.645554 , training accuracy :  0.81257\n",
      "i: 1400 , cost :  0.927952 , training accuracy :  0.805836\n",
      "i: 1500 , cost :  0.737155 , training accuracy :  0.820426\n",
      "i: 1600 , cost :  0.672569 , training accuracy :  0.800224\n",
      "i: 1700 , cost :  0.799017 , training accuracy :  0.800224\n",
      "i: 1800 , cost :  0.620695 , training accuracy :  0.837261\n",
      "i: 1900 , cost :  3.02382 , training accuracy :  0.713805\n",
      "i: 2000 , cost :  0.641267 , training accuracy :  0.840629\n",
      "i: 2100 , cost :  0.63754 , training accuracy :  0.842873\n",
      "i: 2200 , cost :  0.842309 , training accuracy :  0.7789\n",
      "i: 2300 , cost :  0.665845 , training accuracy :  0.842873\n",
      "i: 2400 , cost :  1.42985 , training accuracy :  0.728395\n",
      "i: 2500 , cost :  0.819004 , training accuracy :  0.833894\n",
      "i: 2600 , cost :  0.949635 , training accuracy :  0.754209\n",
      "i: 2700 , cost :  0.522287 , training accuracy :  0.845118\n",
      "i: 2800 , cost :  1.66351 , training accuracy :  0.762065\n",
      "i: 2900 , cost :  1.39704 , training accuracy :  0.777778\n",
      "i: 3000 , cost :  1.77215 , training accuracy :  0.757576\n",
      "i: 3100 , cost :  0.559725 , training accuracy :  0.857464\n",
      "i: 3200 , cost :  0.526081 , training accuracy :  0.85073\n",
      "i: 3300 , cost :  3.98629 , training accuracy :  0.687991\n",
      "i: 3400 , cost :  0.606353 , training accuracy :  0.856341\n",
      "i: 3500 , cost :  0.862313 , training accuracy :  0.820426\n",
      "i: 3600 , cost :  0.460344 , training accuracy :  0.842873\n",
      "i: 3700 , cost :  0.777044 , training accuracy :  0.843996\n",
      "i: 3800 , cost :  0.634321 , training accuracy :  0.84624\n",
      "i: 3900 , cost :  0.502635 , training accuracy :  0.829405\n",
      "i: 4000 , cost :  0.966713 , training accuracy :  0.805836\n",
      "i: 4100 , cost :  0.661889 , training accuracy :  0.815937\n",
      "i: 4200 , cost :  0.566971 , training accuracy :  0.841751\n",
      "i: 4300 , cost :  0.475267 , training accuracy :  0.847363\n",
      "i: 4400 , cost :  0.793289 , training accuracy :  0.787879\n",
      "i: 4500 , cost :  2.9573 , training accuracy :  0.512907\n",
      "i: 4600 , cost :  0.80283 , training accuracy :  0.787879\n",
      "i: 4700 , cost :  0.457224 , training accuracy :  0.848485\n",
      "i: 4800 , cost :  1.76711 , training accuracy :  0.781145\n",
      "i: 4900 , cost :  1.04247 , training accuracy :  0.810326\n",
      "Final training accuracy :  0.799102\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 5000 ][hidden: 50 ][file: bhavul_tr_acc_0.80_prediction.csv ] ACCURACY :  0.799102\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1275.37 , training accuracy :  0.622896\n",
      "i: 100 , cost :  12.7564 , training accuracy :  0.639731\n",
      "i: 200 , cost :  3.18308 , training accuracy :  0.732884\n",
      "i: 300 , cost :  2.71301 , training accuracy :  0.735129\n",
      "i: 400 , cost :  3.73315 , training accuracy :  0.627385\n",
      "i: 500 , cost :  1.70162 , training accuracy :  0.782267\n",
      "i: 600 , cost :  3.94653 , training accuracy :  0.742985\n",
      "i: 700 , cost :  1.43961 , training accuracy :  0.804714\n",
      "i: 800 , cost :  4.28835 , training accuracy :  0.592593\n",
      "i: 900 , cost :  1.44584 , training accuracy :  0.822671\n",
      "i: 1000 , cost :  5.80342 , training accuracy :  0.73064\n",
      "i: 1100 , cost :  2.02202 , training accuracy :  0.794613\n",
      "i: 1200 , cost :  1.02207 , training accuracy :  0.832772\n",
      "i: 1300 , cost :  5.70709 , training accuracy :  0.732884\n",
      "i: 1400 , cost :  1.00383 , training accuracy :  0.841751\n",
      "i: 1500 , cost :  1.34584 , training accuracy :  0.822671\n",
      "i: 1600 , cost :  1.11183 , training accuracy :  0.85073\n",
      "i: 1700 , cost :  7.50273 , training accuracy :  0.685746\n",
      "i: 1800 , cost :  1.52252 , training accuracy :  0.823793\n",
      "i: 1900 , cost :  0.814238 , training accuracy :  0.85073\n",
      "i: 2000 , cost :  1.59729 , training accuracy :  0.787879\n",
      "i: 2100 , cost :  1.03771 , training accuracy :  0.849607\n",
      "i: 2200 , cost :  1.13615 , training accuracy :  0.842873\n",
      "i: 2300 , cost :  1.27102 , training accuracy :  0.824916\n",
      "i: 2400 , cost :  1.22145 , training accuracy :  0.808081\n",
      "i: 2500 , cost :  1.36243 , training accuracy :  0.814815\n",
      "i: 2600 , cost :  0.758651 , training accuracy :  0.840629\n",
      "i: 2700 , cost :  2.29118 , training accuracy :  0.710438\n",
      "i: 2800 , cost :  0.730306 , training accuracy :  0.863075\n",
      "i: 2900 , cost :  1.92667 , training accuracy :  0.823793\n",
      "i: 3000 , cost :  0.67819 , training accuracy :  0.861953\n",
      "i: 3100 , cost :  2.09662 , training accuracy :  0.808081\n",
      "i: 3200 , cost :  0.684887 , training accuracy :  0.868687\n",
      "i: 3300 , cost :  2.05882 , training accuracy :  0.748597\n",
      "i: 3400 , cost :  0.664075 , training accuracy :  0.86532\n",
      "i: 3500 , cost :  2.74838 , training accuracy :  0.809203\n",
      "i: 3600 , cost :  0.932595 , training accuracy :  0.857464\n",
      "i: 3700 , cost :  0.733538 , training accuracy :  0.856341\n",
      "i: 3800 , cost :  0.861833 , training accuracy :  0.86532\n",
      "i: 3900 , cost :  1.61751 , training accuracy :  0.808081\n",
      "i: 4000 , cost :  0.721856 , training accuracy :  0.870932\n",
      "i: 4100 , cost :  0.765349 , training accuracy :  0.82716\n",
      "i: 4200 , cost :  2.54969 , training accuracy :  0.71156\n",
      "i: 4300 , cost :  1.52314 , training accuracy :  0.818182\n",
      "i: 4400 , cost :  0.903303 , training accuracy :  0.863075\n",
      "i: 4500 , cost :  0.611293 , training accuracy :  0.860831\n",
      "i: 4600 , cost :  0.940255 , training accuracy :  0.854097\n",
      "i: 4700 , cost :  2.32855 , training accuracy :  0.802469\n",
      "i: 4800 , cost :  1.88634 , training accuracy :  0.821549\n",
      "i: 4900 , cost :  0.619572 , training accuracy :  0.874299\n",
      "Final training accuracy :  0.837261\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 5000 ][hidden: 100 ][file: bhavul_tr_acc_0.84_prediction.csv ] ACCURACY :  0.837261\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  488.251 , training accuracy :  0.616162\n",
      "i: 100 , cost :  68.5868 , training accuracy :  0.636364\n",
      "i: 200 , cost :  17.3092 , training accuracy :  0.571268\n",
      "i: 300 , cost :  0.742429 , training accuracy :  0.760943\n",
      "i: 400 , cost :  0.519695 , training accuracy :  0.787879\n",
      "i: 500 , cost :  0.489311 , training accuracy :  0.796857\n",
      "i: 600 , cost :  0.478472 , training accuracy :  0.795735\n",
      "i: 700 , cost :  0.474244 , training accuracy :  0.799102\n",
      "i: 800 , cost :  0.471095 , training accuracy :  0.800224\n",
      "i: 900 , cost :  0.468178 , training accuracy :  0.800224\n",
      "i: 1000 , cost :  0.465426 , training accuracy :  0.800224\n",
      "i: 1100 , cost :  0.462824 , training accuracy :  0.79798\n",
      "i: 1200 , cost :  0.460377 , training accuracy :  0.800224\n",
      "i: 1300 , cost :  0.458077 , training accuracy :  0.800224\n",
      "i: 1400 , cost :  0.455927 , training accuracy :  0.800224\n",
      "i: 1500 , cost :  0.453928 , training accuracy :  0.799102\n",
      "i: 1600 , cost :  0.452085 , training accuracy :  0.79798\n",
      "i: 1700 , cost :  0.453519 , training accuracy :  0.79349\n",
      "i: 1800 , cost :  0.449735 , training accuracy :  0.796857\n",
      "i: 1900 , cost :  0.449262 , training accuracy :  0.79349\n",
      "i: 2000 , cost :  0.455776 , training accuracy :  0.805836\n",
      "i: 2100 , cost :  0.44611 , training accuracy :  0.803591\n",
      "i: 2200 , cost :  0.450052 , training accuracy :  0.79349\n",
      "i: 2300 , cost :  0.450651 , training accuracy :  0.787879\n",
      "i: 2400 , cost :  0.451666 , training accuracy :  0.787879\n",
      "i: 2500 , cost :  0.449774 , training accuracy :  0.789001\n",
      "i: 2600 , cost :  0.445646 , training accuracy :  0.79798\n",
      "i: 2700 , cost :  0.447642 , training accuracy :  0.79349\n",
      "i: 2800 , cost :  0.456628 , training accuracy :  0.782267\n",
      "i: 2900 , cost :  0.455208 , training accuracy :  0.805836\n",
      "i: 3000 , cost :  0.458423 , training accuracy :  0.805836\n",
      "i: 3100 , cost :  0.441473 , training accuracy :  0.801347\n",
      "i: 3200 , cost :  0.442024 , training accuracy :  0.801347\n",
      "i: 3300 , cost :  0.472079 , training accuracy :  0.802469\n",
      "i: 3400 , cost :  0.451488 , training accuracy :  0.811448\n",
      "i: 3500 , cost :  0.442125 , training accuracy :  0.792368\n",
      "i: 3600 , cost :  0.4403 , training accuracy :  0.801347\n",
      "i: 3700 , cost :  0.44025 , training accuracy :  0.802469\n",
      "i: 3800 , cost :  0.450696 , training accuracy :  0.785634\n",
      "i: 3900 , cost :  0.443757 , training accuracy :  0.794613\n",
      "i: 4000 , cost :  0.440232 , training accuracy :  0.800224\n",
      "i: 4100 , cost :  0.449734 , training accuracy :  0.808081\n",
      "i: 4200 , cost :  0.458166 , training accuracy :  0.783389\n",
      "i: 4300 , cost :  0.443671 , training accuracy :  0.814815\n",
      "i: 4400 , cost :  0.446999 , training accuracy :  0.786756\n",
      "i: 4500 , cost :  0.443807 , training accuracy :  0.79349\n",
      "i: 4600 , cost :  0.469966 , training accuracy :  0.805836\n",
      "i: 4700 , cost :  0.441087 , training accuracy :  0.799102\n",
      "i: 4800 , cost :  0.453828 , training accuracy :  0.784512\n",
      "i: 4900 , cost :  0.452095 , training accuracy :  0.783389\n",
      "i: 5000 , cost :  0.440888 , training accuracy :  0.800224\n",
      "i: 5100 , cost :  0.446662 , training accuracy :  0.804714\n",
      "i: 5200 , cost :  0.449011 , training accuracy :  0.783389\n",
      "i: 5300 , cost :  0.443447 , training accuracy :  0.794613\n",
      "i: 5400 , cost :  0.44073 , training accuracy :  0.801347\n",
      "i: 5500 , cost :  0.443835 , training accuracy :  0.792368\n",
      "i: 5600 , cost :  0.470996 , training accuracy :  0.804714\n",
      "i: 5700 , cost :  0.441374 , training accuracy :  0.800224\n",
      "i: 5800 , cost :  0.452741 , training accuracy :  0.782267\n",
      "i: 5900 , cost :  0.452972 , training accuracy :  0.784512\n",
      "i: 6000 , cost :  0.446845 , training accuracy :  0.806958\n",
      "i: 6100 , cost :  0.451536 , training accuracy :  0.809203\n",
      "i: 6200 , cost :  0.457984 , training accuracy :  0.805836\n",
      "i: 6300 , cost :  0.439824 , training accuracy :  0.800224\n",
      "i: 6400 , cost :  0.447326 , training accuracy :  0.800224\n",
      "i: 6500 , cost :  0.459803 , training accuracy :  0.784512\n",
      "i: 6600 , cost :  0.457966 , training accuracy :  0.782267\n",
      "i: 6700 , cost :  0.441455 , training accuracy :  0.808081\n",
      "i: 6800 , cost :  0.454526 , training accuracy :  0.783389\n",
      "i: 6900 , cost :  0.482596 , training accuracy :  0.801347\n",
      "i: 7000 , cost :  0.45595 , training accuracy :  0.804714\n",
      "i: 7100 , cost :  0.456581 , training accuracy :  0.803591\n",
      "i: 7200 , cost :  0.481654 , training accuracy :  0.803591\n",
      "i: 7300 , cost :  0.454902 , training accuracy :  0.805836\n",
      "i: 7400 , cost :  0.446934 , training accuracy :  0.786756\n",
      "i: 7500 , cost :  0.439862 , training accuracy :  0.805836\n",
      "i: 7600 , cost :  0.440466 , training accuracy :  0.801347\n",
      "i: 7700 , cost :  0.443559 , training accuracy :  0.79798\n",
      "i: 7800 , cost :  0.460551 , training accuracy :  0.806958\n",
      "i: 7900 , cost :  0.442544 , training accuracy :  0.810326\n",
      "i: 8000 , cost :  0.443484 , training accuracy :  0.809203\n",
      "i: 8100 , cost :  0.446576 , training accuracy :  0.785634\n",
      "i: 8200 , cost :  0.455394 , training accuracy :  0.783389\n",
      "i: 8300 , cost :  0.444382 , training accuracy :  0.791246\n",
      "i: 8400 , cost :  0.452675 , training accuracy :  0.81257\n",
      "i: 8500 , cost :  0.461811 , training accuracy :  0.782267\n",
      "i: 8600 , cost :  0.440896 , training accuracy :  0.803591\n",
      "i: 8700 , cost :  0.481088 , training accuracy :  0.801347\n",
      "i: 8800 , cost :  0.467311 , training accuracy :  0.7789\n",
      "i: 8900 , cost :  0.439656 , training accuracy :  0.799102\n",
      "i: 9000 , cost :  0.440267 , training accuracy :  0.801347\n",
      "i: 9100 , cost :  0.464013 , training accuracy :  0.803591\n",
      "i: 9200 , cost :  0.441484 , training accuracy :  0.796857\n",
      "i: 9300 , cost :  0.442587 , training accuracy :  0.809203\n",
      "i: 9400 , cost :  0.440707 , training accuracy :  0.801347\n",
      "i: 9500 , cost :  0.440149 , training accuracy :  0.801347\n",
      "i: 9600 , cost :  0.451003 , training accuracy :  0.782267\n",
      "i: 9700 , cost :  0.451277 , training accuracy :  0.782267\n",
      "i: 9800 , cost :  0.439632 , training accuracy :  0.799102\n",
      "i: 9900 , cost :  0.440849 , training accuracy :  0.803591\n",
      "Final training accuracy :  0.809203\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 10000 ][hidden: 3 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.809203\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  28.9793 , training accuracy :  0.694725\n",
      "i: 100 , cost :  5.1592 , training accuracy :  0.654321\n",
      "i: 200 , cost :  1.0283 , training accuracy :  0.652076\n",
      "i: 300 , cost :  0.74447 , training accuracy :  0.769921\n",
      "i: 400 , cost :  0.648035 , training accuracy :  0.787879\n",
      "i: 500 , cost :  0.617203 , training accuracy :  0.784512\n",
      "i: 600 , cost :  0.603237 , training accuracy :  0.777778\n",
      "i: 700 , cost :  0.550308 , training accuracy :  0.790123\n",
      "i: 800 , cost :  0.556282 , training accuracy :  0.781145\n",
      "i: 900 , cost :  0.546209 , training accuracy :  0.783389\n",
      "i: 1000 , cost :  0.499924 , training accuracy :  0.794613\n",
      "i: 1100 , cost :  0.485153 , training accuracy :  0.790123\n",
      "i: 1200 , cost :  0.495061 , training accuracy :  0.774411\n",
      "i: 1300 , cost :  0.498509 , training accuracy :  0.773288\n",
      "i: 1400 , cost :  0.466437 , training accuracy :  0.795735\n",
      "i: 1500 , cost :  0.483255 , training accuracy :  0.79798\n",
      "i: 1600 , cost :  0.487752 , training accuracy :  0.792368\n",
      "i: 1700 , cost :  0.458804 , training accuracy :  0.795735\n",
      "i: 1800 , cost :  0.483003 , training accuracy :  0.777778\n",
      "i: 1900 , cost :  0.452596 , training accuracy :  0.799102\n",
      "i: 2000 , cost :  0.480079 , training accuracy :  0.79798\n",
      "i: 2100 , cost :  0.44855 , training accuracy :  0.79798\n",
      "i: 2200 , cost :  0.47393 , training accuracy :  0.782267\n",
      "i: 2300 , cost :  0.44595 , training accuracy :  0.799102\n",
      "i: 2400 , cost :  0.457617 , training accuracy :  0.805836\n",
      "i: 2500 , cost :  0.467318 , training accuracy :  0.781145\n",
      "i: 2600 , cost :  0.445444 , training accuracy :  0.804714\n",
      "i: 2700 , cost :  0.438529 , training accuracy :  0.802469\n",
      "i: 2800 , cost :  0.459063 , training accuracy :  0.782267\n",
      "i: 2900 , cost :  0.465951 , training accuracy :  0.79798\n",
      "i: 3000 , cost :  0.441886 , training accuracy :  0.79798\n",
      "i: 3100 , cost :  0.431991 , training accuracy :  0.806958\n",
      "i: 3200 , cost :  0.44986 , training accuracy :  0.806958\n",
      "i: 3300 , cost :  0.457105 , training accuracy :  0.789001\n",
      "i: 3400 , cost :  0.435137 , training accuracy :  0.808081\n",
      "i: 3500 , cost :  0.431814 , training accuracy :  0.810326\n",
      "i: 3600 , cost :  0.456281 , training accuracy :  0.787879\n",
      "i: 3700 , cost :  0.435643 , training accuracy :  0.806958\n",
      "i: 3800 , cost :  0.432631 , training accuracy :  0.809203\n",
      "i: 3900 , cost :  0.455263 , training accuracy :  0.786756\n",
      "i: 4000 , cost :  0.436688 , training accuracy :  0.808081\n",
      "i: 4100 , cost :  0.424861 , training accuracy :  0.806958\n",
      "i: 4200 , cost :  0.429403 , training accuracy :  0.802469\n",
      "i: 4300 , cost :  0.446176 , training accuracy :  0.801347\n",
      "i: 4400 , cost :  0.451865 , training accuracy :  0.787879\n",
      "i: 4500 , cost :  0.458934 , training accuracy :  0.802469\n",
      "i: 4600 , cost :  0.446412 , training accuracy :  0.79349\n",
      "i: 4700 , cost :  0.445276 , training accuracy :  0.806958\n",
      "i: 4800 , cost :  0.434793 , training accuracy :  0.79798\n",
      "i: 4900 , cost :  0.429729 , training accuracy :  0.806958\n",
      "i: 5000 , cost :  0.424277 , training accuracy :  0.81257\n",
      "i: 5100 , cost :  0.422209 , training accuracy :  0.815937\n",
      "i: 5200 , cost :  0.421801 , training accuracy :  0.81257\n",
      "i: 5300 , cost :  0.424358 , training accuracy :  0.806958\n",
      "i: 5400 , cost :  0.431776 , training accuracy :  0.813693\n",
      "i: 5500 , cost :  0.437911 , training accuracy :  0.792368\n",
      "i: 5600 , cost :  0.452245 , training accuracy :  0.803591\n",
      "i: 5700 , cost :  0.449408 , training accuracy :  0.787879\n",
      "i: 5800 , cost :  0.455823 , training accuracy :  0.800224\n",
      "i: 5900 , cost :  0.442546 , training accuracy :  0.792368\n",
      "i: 6000 , cost :  0.43765 , training accuracy :  0.81257\n",
      "i: 6100 , cost :  0.428123 , training accuracy :  0.801347\n",
      "i: 6200 , cost :  0.42282 , training accuracy :  0.811448\n",
      "i: 6300 , cost :  0.419791 , training accuracy :  0.810326\n",
      "i: 6400 , cost :  0.423069 , training accuracy :  0.804714\n",
      "i: 6500 , cost :  0.435433 , training accuracy :  0.818182\n",
      "i: 6600 , cost :  0.443991 , training accuracy :  0.792368\n",
      "i: 6700 , cost :  0.456109 , training accuracy :  0.79798\n",
      "i: 6800 , cost :  0.443661 , training accuracy :  0.795735\n",
      "i: 6900 , cost :  0.43988 , training accuracy :  0.805836\n",
      "i: 7000 , cost :  0.427684 , training accuracy :  0.79798\n",
      "i: 7100 , cost :  0.422143 , training accuracy :  0.810326\n",
      "i: 7200 , cost :  0.418768 , training accuracy :  0.810326\n",
      "i: 7300 , cost :  0.420514 , training accuracy :  0.811448\n",
      "i: 7400 , cost :  0.430529 , training accuracy :  0.817059\n",
      "i: 7500 , cost :  0.439789 , training accuracy :  0.795735\n",
      "i: 7600 , cost :  0.45497 , training accuracy :  0.79798\n",
      "i: 7700 , cost :  0.444463 , training accuracy :  0.795735\n",
      "i: 7800 , cost :  0.441231 , training accuracy :  0.809203\n",
      "i: 7900 , cost :  0.428622 , training accuracy :  0.799102\n",
      "i: 8000 , cost :  0.421914 , training accuracy :  0.810326\n",
      "i: 8100 , cost :  0.417638 , training accuracy :  0.81257\n",
      "i: 8200 , cost :  0.419064 , training accuracy :  0.81257\n",
      "i: 8300 , cost :  0.428874 , training accuracy :  0.817059\n",
      "i: 8400 , cost :  0.438843 , training accuracy :  0.795735\n",
      "i: 8500 , cost :  0.453029 , training accuracy :  0.800224\n",
      "i: 8600 , cost :  0.44288 , training accuracy :  0.79798\n",
      "i: 8700 , cost :  0.440709 , training accuracy :  0.806958\n",
      "i: 8800 , cost :  0.428269 , training accuracy :  0.79798\n",
      "i: 8900 , cost :  0.421852 , training accuracy :  0.814815\n",
      "i: 9000 , cost :  0.416817 , training accuracy :  0.81257\n",
      "i: 9100 , cost :  0.41677 , training accuracy :  0.811448\n",
      "i: 9200 , cost :  0.423817 , training accuracy :  0.818182\n",
      "i: 9300 , cost :  0.433949 , training accuracy :  0.796857\n",
      "i: 9400 , cost :  0.450523 , training accuracy :  0.804714\n",
      "i: 9500 , cost :  0.443251 , training accuracy :  0.800224\n",
      "i: 9600 , cost :  0.442796 , training accuracy :  0.804714\n",
      "i: 9700 , cost :  0.429091 , training accuracy :  0.802469\n",
      "i: 9800 , cost :  0.423493 , training accuracy :  0.814815\n",
      "i: 9900 , cost :  0.417367 , training accuracy :  0.815937\n",
      "Final training accuracy :  0.815937\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 10000 ][hidden: 10 ][file: bhavul_tr_acc_0.82_prediction.csv ] ACCURACY :  0.815937\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  27.6317 , training accuracy :  0.6633\n",
      "i: 100 , cost :  2.44413 , training accuracy :  0.609428\n",
      "i: 200 , cost :  1.00548 , training accuracy :  0.710438\n",
      "i: 300 , cost :  0.641812 , training accuracy :  0.736251\n",
      "i: 400 , cost :  0.555093 , training accuracy :  0.75982\n",
      "i: 500 , cost :  0.521713 , training accuracy :  0.781145\n",
      "i: 600 , cost :  0.537388 , training accuracy :  0.757576\n",
      "i: 700 , cost :  0.51803 , training accuracy :  0.774411\n",
      "i: 800 , cost :  0.495521 , training accuracy :  0.784512\n",
      "i: 900 , cost :  0.544577 , training accuracy :  0.769921\n",
      "i: 1000 , cost :  0.464684 , training accuracy :  0.800224\n",
      "i: 1100 , cost :  0.477378 , training accuracy :  0.775533\n",
      "i: 1200 , cost :  0.442743 , training accuracy :  0.806958\n",
      "i: 1300 , cost :  0.437579 , training accuracy :  0.808081\n",
      "i: 1400 , cost :  0.441028 , training accuracy :  0.794613\n",
      "i: 1500 , cost :  0.432152 , training accuracy :  0.799102\n",
      "i: 1600 , cost :  0.43165 , training accuracy :  0.802469\n",
      "i: 1700 , cost :  0.43707 , training accuracy :  0.792368\n",
      "i: 1800 , cost :  0.442407 , training accuracy :  0.801347\n",
      "i: 1900 , cost :  0.453939 , training accuracy :  0.800224\n",
      "i: 2000 , cost :  0.440626 , training accuracy :  0.79798\n",
      "i: 2100 , cost :  0.431412 , training accuracy :  0.801347\n",
      "i: 2200 , cost :  0.468403 , training accuracy :  0.791246\n",
      "i: 2300 , cost :  0.459132 , training accuracy :  0.792368\n",
      "i: 2400 , cost :  0.484374 , training accuracy :  0.777778\n",
      "i: 2500 , cost :  0.468906 , training accuracy :  0.791246\n",
      "i: 2600 , cost :  0.417238 , training accuracy :  0.815937\n",
      "i: 2700 , cost :  0.417707 , training accuracy :  0.814815\n",
      "i: 2800 , cost :  0.444572 , training accuracy :  0.805836\n",
      "i: 2900 , cost :  0.413642 , training accuracy :  0.820426\n",
      "i: 3000 , cost :  0.414547 , training accuracy :  0.822671\n",
      "i: 3100 , cost :  0.423333 , training accuracy :  0.81257\n",
      "i: 3200 , cost :  0.432658 , training accuracy :  0.814815\n",
      "i: 3300 , cost :  0.415056 , training accuracy :  0.818182\n",
      "i: 3400 , cost :  0.422654 , training accuracy :  0.813693\n",
      "i: 3500 , cost :  0.407758 , training accuracy :  0.826038\n",
      "i: 3600 , cost :  0.423161 , training accuracy :  0.819304\n",
      "i: 3700 , cost :  0.491809 , training accuracy :  0.789001\n",
      "i: 3800 , cost :  0.406079 , training accuracy :  0.826038\n",
      "i: 3900 , cost :  0.514428 , training accuracy :  0.776655\n",
      "i: 4000 , cost :  0.411938 , training accuracy :  0.822671\n",
      "i: 4100 , cost :  0.411896 , training accuracy :  0.817059\n",
      "i: 4200 , cost :  0.402021 , training accuracy :  0.833894\n",
      "i: 4300 , cost :  0.404897 , training accuracy :  0.830527\n",
      "i: 4400 , cost :  0.409007 , training accuracy :  0.829405\n",
      "i: 4500 , cost :  0.404123 , training accuracy :  0.835017\n",
      "i: 4600 , cost :  0.460089 , training accuracy :  0.799102\n",
      "i: 4700 , cost :  0.396951 , training accuracy :  0.836139\n",
      "i: 4800 , cost :  0.438663 , training accuracy :  0.808081\n",
      "i: 4900 , cost :  0.437603 , training accuracy :  0.806958\n",
      "i: 5000 , cost :  0.456175 , training accuracy :  0.801347\n",
      "i: 5100 , cost :  0.451045 , training accuracy :  0.803591\n",
      "i: 5200 , cost :  0.40104 , training accuracy :  0.835017\n",
      "i: 5300 , cost :  0.393692 , training accuracy :  0.841751\n",
      "i: 5400 , cost :  0.39494 , training accuracy :  0.829405\n",
      "i: 5500 , cost :  0.460366 , training accuracy :  0.79798\n",
      "i: 5600 , cost :  0.408664 , training accuracy :  0.815937\n",
      "i: 5700 , cost :  0.395762 , training accuracy :  0.829405\n",
      "i: 5800 , cost :  0.391749 , training accuracy :  0.837261\n",
      "i: 5900 , cost :  0.432321 , training accuracy :  0.811448\n",
      "i: 6000 , cost :  0.438582 , training accuracy :  0.809203\n",
      "i: 6100 , cost :  0.391291 , training accuracy :  0.83165\n",
      "i: 6200 , cost :  0.416414 , training accuracy :  0.811448\n",
      "i: 6300 , cost :  0.389649 , training accuracy :  0.830527\n",
      "i: 6400 , cost :  0.433583 , training accuracy :  0.813693\n",
      "i: 6500 , cost :  0.403096 , training accuracy :  0.830527\n",
      "i: 6600 , cost :  0.392158 , training accuracy :  0.830527\n",
      "i: 6700 , cost :  0.430756 , training accuracy :  0.81257\n",
      "i: 6800 , cost :  0.389746 , training accuracy :  0.826038\n",
      "i: 6900 , cost :  0.451736 , training accuracy :  0.806958\n",
      "i: 7000 , cost :  0.397579 , training accuracy :  0.835017\n",
      "i: 7100 , cost :  0.390117 , training accuracy :  0.832772\n",
      "i: 7200 , cost :  0.424554 , training accuracy :  0.81257\n",
      "i: 7300 , cost :  0.386272 , training accuracy :  0.837261\n",
      "i: 7400 , cost :  0.462543 , training accuracy :  0.803591\n",
      "i: 7500 , cost :  0.384839 , training accuracy :  0.840629\n",
      "i: 7600 , cost :  0.434658 , training accuracy :  0.809203\n",
      "i: 7700 , cost :  0.480628 , training accuracy :  0.794613\n",
      "i: 7800 , cost :  0.415333 , training accuracy :  0.819304\n",
      "i: 7900 , cost :  0.4384 , training accuracy :  0.809203\n",
      "i: 8000 , cost :  0.385011 , training accuracy :  0.835017\n",
      "i: 8100 , cost :  0.476156 , training accuracy :  0.79798\n",
      "i: 8200 , cost :  0.383045 , training accuracy :  0.841751\n",
      "i: 8300 , cost :  0.433865 , training accuracy :  0.809203\n",
      "i: 8400 , cost :  0.382068 , training accuracy :  0.841751\n",
      "i: 8500 , cost :  0.43545 , training accuracy :  0.81257\n",
      "i: 8600 , cost :  0.39688 , training accuracy :  0.829405\n",
      "i: 8700 , cost :  0.418551 , training accuracy :  0.814815\n",
      "i: 8800 , cost :  0.395958 , training accuracy :  0.829405\n",
      "i: 8900 , cost :  0.388824 , training accuracy :  0.832772\n",
      "i: 9000 , cost :  0.382699 , training accuracy :  0.835017\n",
      "i: 9100 , cost :  0.42931 , training accuracy :  0.813693\n",
      "i: 9200 , cost :  0.381968 , training accuracy :  0.838384\n",
      "i: 9300 , cost :  0.427741 , training accuracy :  0.814815\n",
      "i: 9400 , cost :  0.383871 , training accuracy :  0.83165\n",
      "i: 9500 , cost :  0.407581 , training accuracy :  0.822671\n",
      "i: 9600 , cost :  0.381591 , training accuracy :  0.836139\n",
      "i: 9700 , cost :  0.421964 , training accuracy :  0.815937\n",
      "i: 9800 , cost :  0.477438 , training accuracy :  0.792368\n",
      "i: 9900 , cost :  0.379011 , training accuracy :  0.842873\n",
      "Final training accuracy :  0.83165\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 10000 ][hidden: 15 ][file: bhavul_tr_acc_0.83_prediction.csv ] ACCURACY :  0.83165\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  206.42 , training accuracy :  0.640853\n",
      "i: 100 , cost :  5.65746 , training accuracy :  0.657688\n",
      "i: 200 , cost :  2.71101 , training accuracy :  0.732884\n",
      "i: 300 , cost :  1.63044 , training accuracy :  0.756453\n",
      "i: 400 , cost :  1.34227 , training accuracy :  0.762065\n",
      "i: 500 , cost :  1.19103 , training accuracy :  0.757576\n",
      "i: 600 , cost :  1.07942 , training accuracy :  0.771044\n",
      "i: 700 , cost :  0.849452 , training accuracy :  0.792368\n",
      "i: 800 , cost :  3.3353 , training accuracy :  0.445567\n",
      "i: 900 , cost :  3.79804 , training accuracy :  0.454545\n",
      "i: 1000 , cost :  0.695283 , training accuracy :  0.802469\n",
      "i: 1100 , cost :  0.767328 , training accuracy :  0.800224\n",
      "i: 1200 , cost :  1.00723 , training accuracy :  0.732884\n",
      "i: 1300 , cost :  0.929052 , training accuracy :  0.795735\n",
      "i: 1400 , cost :  1.25179 , training accuracy :  0.6633\n",
      "i: 1500 , cost :  0.516848 , training accuracy :  0.810326\n",
      "i: 1600 , cost :  1.70195 , training accuracy :  0.744108\n",
      "i: 1700 , cost :  3.58777 , training accuracy :  0.708193\n",
      "i: 1800 , cost :  2.2545 , training accuracy :  0.727273\n",
      "i: 1900 , cost :  0.552505 , training accuracy :  0.819304\n",
      "i: 2000 , cost :  0.465687 , training accuracy :  0.821549\n",
      "i: 2100 , cost :  1.69722 , training accuracy :  0.767677\n",
      "i: 2200 , cost :  0.556971 , training accuracy :  0.826038\n",
      "i: 2300 , cost :  0.457696 , training accuracy :  0.829405\n",
      "i: 2400 , cost :  2.80365 , training accuracy :  0.700337\n",
      "i: 2500 , cost :  0.453818 , training accuracy :  0.832772\n",
      "i: 2600 , cost :  0.957735 , training accuracy :  0.79798\n",
      "i: 2700 , cost :  0.627596 , training accuracy :  0.823793\n",
      "i: 2800 , cost :  0.456592 , training accuracy :  0.839506\n",
      "i: 2900 , cost :  2.28562 , training accuracy :  0.728395\n",
      "i: 3000 , cost :  0.577085 , training accuracy :  0.838384\n",
      "i: 3100 , cost :  0.418695 , training accuracy :  0.841751\n",
      "i: 3200 , cost :  0.824005 , training accuracy :  0.810326\n",
      "i: 3300 , cost :  0.472577 , training accuracy :  0.830527\n",
      "i: 3400 , cost :  0.494303 , training accuracy :  0.829405\n",
      "i: 3500 , cost :  0.449751 , training accuracy :  0.83165\n",
      "i: 3600 , cost :  0.409023 , training accuracy :  0.836139\n",
      "i: 3700 , cost :  1.39652 , training accuracy :  0.768799\n",
      "i: 3800 , cost :  0.780033 , training accuracy :  0.817059\n",
      "i: 3900 , cost :  0.414942 , training accuracy :  0.843996\n",
      "i: 4000 , cost :  4.40162 , training accuracy :  0.4422\n",
      "i: 4100 , cost :  0.611715 , training accuracy :  0.795735\n",
      "i: 4200 , cost :  0.971999 , training accuracy :  0.690236\n",
      "i: 4300 , cost :  0.648902 , training accuracy :  0.82716\n",
      "i: 4400 , cost :  0.460992 , training accuracy :  0.843996\n",
      "i: 4500 , cost :  0.544292 , training accuracy :  0.808081\n",
      "i: 4600 , cost :  0.540584 , training accuracy :  0.837261\n",
      "i: 4700 , cost :  0.39031 , training accuracy :  0.849607\n",
      "i: 4800 , cost :  0.822888 , training accuracy :  0.806958\n",
      "i: 4900 , cost :  1.0171 , training accuracy :  0.800224\n",
      "i: 5000 , cost :  0.833454 , training accuracy :  0.804714\n",
      "i: 5100 , cost :  0.471635 , training accuracy :  0.830527\n",
      "i: 5200 , cost :  0.38579 , training accuracy :  0.85073\n",
      "i: 5300 , cost :  1.19495 , training accuracy :  0.801347\n",
      "i: 5400 , cost :  0.569644 , training accuracy :  0.803591\n",
      "i: 5500 , cost :  0.379352 , training accuracy :  0.85073\n",
      "i: 5600 , cost :  0.449218 , training accuracy :  0.838384\n",
      "i: 5700 , cost :  2.09296 , training accuracy :  0.740741\n",
      "i: 5800 , cost :  0.482261 , training accuracy :  0.848485\n",
      "i: 5900 , cost :  0.381083 , training accuracy :  0.85073\n",
      "i: 6000 , cost :  0.44274 , training accuracy :  0.829405\n",
      "i: 6100 , cost :  0.519932 , training accuracy :  0.769921\n",
      "i: 6200 , cost :  1.73452 , training accuracy :  0.731762\n",
      "i: 6300 , cost :  2.92876 , training accuracy :  0.483726\n",
      "i: 6400 , cost :  1.03163 , training accuracy :  0.728395\n",
      "i: 6500 , cost :  1.33406 , training accuracy :  0.6633\n",
      "i: 6600 , cost :  1.35449 , training accuracy :  0.655443\n",
      "i: 6700 , cost :  1.21549 , training accuracy :  0.685746\n",
      "i: 6800 , cost :  1.22909 , training accuracy :  0.67789\n",
      "i: 6900 , cost :  1.2625 , training accuracy :  0.674523\n",
      "i: 7000 , cost :  1.23069 , training accuracy :  0.676768\n",
      "i: 7100 , cost :  1.09273 , training accuracy :  0.707071\n",
      "i: 7200 , cost :  0.988813 , training accuracy :  0.729517\n",
      "i: 7300 , cost :  0.953079 , training accuracy :  0.732884\n",
      "i: 7400 , cost :  0.901586 , training accuracy :  0.735129\n",
      "i: 7500 , cost :  0.817137 , training accuracy :  0.755331\n",
      "i: 7600 , cost :  0.444942 , training accuracy :  0.849607\n",
      "i: 7700 , cost :  0.383398 , training accuracy :  0.85073\n",
      "i: 7800 , cost :  0.365039 , training accuracy :  0.848485\n",
      "i: 7900 , cost :  0.857194 , training accuracy :  0.808081\n",
      "i: 8000 , cost :  0.579975 , training accuracy :  0.794613\n",
      "i: 8100 , cost :  0.368225 , training accuracy :  0.852974\n",
      "i: 8200 , cost :  1.54731 , training accuracy :  0.593715\n",
      "i: 8300 , cost :  0.411266 , training accuracy :  0.856341\n",
      "i: 8400 , cost :  0.360055 , training accuracy :  0.852974\n",
      "i: 8500 , cost :  0.920335 , training accuracy :  0.79798\n",
      "i: 8600 , cost :  0.52492 , training accuracy :  0.809203\n",
      "i: 8700 , cost :  1.42133 , training accuracy :  0.763187\n",
      "i: 8800 , cost :  1.21898 , training accuracy :  0.603816\n",
      "i: 8900 , cost :  0.380738 , training accuracy :  0.857464\n",
      "i: 9000 , cost :  1.73788 , training accuracy :  0.748597\n",
      "i: 9100 , cost :  1.06321 , training accuracy :  0.805836\n",
      "i: 9200 , cost :  0.898028 , training accuracy :  0.818182\n",
      "i: 9300 , cost :  0.567683 , training accuracy :  0.851852\n",
      "i: 9400 , cost :  0.708778 , training accuracy :  0.838384\n",
      "i: 9500 , cost :  0.693659 , training accuracy :  0.838384\n",
      "i: 9600 , cost :  0.696656 , training accuracy :  0.839506\n",
      "i: 9700 , cost :  0.680237 , training accuracy :  0.839506\n",
      "i: 9800 , cost :  0.645942 , training accuracy :  0.842873\n",
      "i: 9900 , cost :  0.609617 , training accuracy :  0.849607\n",
      "Final training accuracy :  0.848485\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 10000 ][hidden: 50 ][file: bhavul_tr_acc_0.85_prediction.csv ] ACCURACY :  0.848485\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  2502.57 , training accuracy :  0.379349\n",
      "i: 100 , cost :  35.5732 , training accuracy :  0.661055\n",
      "i: 200 , cost :  6.83999 , training accuracy :  0.712682\n",
      "i: 300 , cost :  3.77642 , training accuracy :  0.751964\n",
      "i: 400 , cost :  3.25931 , training accuracy :  0.741863\n",
      "i: 500 , cost :  2.87437 , training accuracy :  0.785634\n",
      "i: 600 , cost :  2.25551 , training accuracy :  0.777778\n",
      "i: 700 , cost :  6.85712 , training accuracy :  0.57688\n",
      "i: 800 , cost :  1.93408 , training accuracy :  0.785634\n",
      "i: 900 , cost :  2.32098 , training accuracy :  0.783389\n",
      "i: 1000 , cost :  1.69897 , training accuracy :  0.808081\n",
      "i: 1100 , cost :  6.83096 , training accuracy :  0.484848\n",
      "i: 1200 , cost :  1.58216 , training accuracy :  0.818182\n",
      "i: 1300 , cost :  3.43407 , training accuracy :  0.784512\n",
      "i: 1400 , cost :  1.29949 , training accuracy :  0.810326\n",
      "i: 1500 , cost :  8.42035 , training accuracy :  0.480359\n",
      "i: 1600 , cost :  1.25101 , training accuracy :  0.809203\n",
      "i: 1700 , cost :  4.25159 , training accuracy :  0.777778\n",
      "i: 1800 , cost :  1.23494 , training accuracy :  0.794613\n",
      "i: 1900 , cost :  1.22308 , training accuracy :  0.813693\n",
      "i: 2000 , cost :  1.93406 , training accuracy :  0.814815\n",
      "i: 2100 , cost :  17.3474 , training accuracy :  0.393939\n",
      "i: 2200 , cost :  2.61911 , training accuracy :  0.79349\n",
      "i: 2300 , cost :  1.28932 , training accuracy :  0.823793\n",
      "i: 2400 , cost :  6.43138 , training accuracy :  0.758698\n",
      "i: 2500 , cost :  0.941297 , training accuracy :  0.845118\n",
      "i: 2600 , cost :  1.72975 , training accuracy :  0.824916\n",
      "i: 2700 , cost :  0.838955 , training accuracy :  0.837261\n",
      "i: 2800 , cost :  1.05509 , training accuracy :  0.843996\n",
      "i: 2900 , cost :  1.31205 , training accuracy :  0.781145\n",
      "i: 3000 , cost :  2.16418 , training accuracy :  0.801347\n",
      "i: 3100 , cost :  0.957192 , training accuracy :  0.847363\n",
      "i: 3200 , cost :  8.49167 , training accuracy :  0.472503\n",
      "i: 3300 , cost :  0.884195 , training accuracy :  0.843996\n",
      "i: 3400 , cost :  1.77068 , training accuracy :  0.81257\n",
      "i: 3500 , cost :  0.962712 , training accuracy :  0.841751\n",
      "i: 3600 , cost :  0.922185 , training accuracy :  0.849607\n",
      "i: 3700 , cost :  2.04839 , training accuracy :  0.763187\n",
      "i: 3800 , cost :  0.817892 , training accuracy :  0.852974\n",
      "i: 3900 , cost :  1.25499 , training accuracy :  0.836139\n",
      "i: 4000 , cost :  0.931657 , training accuracy :  0.856341\n",
      "i: 4100 , cost :  15.6435 , training accuracy :  0.426487\n",
      "i: 4200 , cost :  4.06796 , training accuracy :  0.681257\n",
      "i: 4300 , cost :  0.927904 , training accuracy :  0.863075\n",
      "i: 4400 , cost :  10.5652 , training accuracy :  0.4422\n",
      "i: 4500 , cost :  1.42851 , training accuracy :  0.85073\n",
      "i: 4600 , cost :  0.65014 , training accuracy :  0.848485\n",
      "i: 4700 , cost :  1.55641 , training accuracy :  0.839506\n",
      "i: 4800 , cost :  0.855136 , training accuracy :  0.84624\n",
      "i: 4900 , cost :  0.741001 , training accuracy :  0.859708\n",
      "i: 5000 , cost :  3.18568 , training accuracy :  0.814815\n",
      "i: 5100 , cost :  1.06577 , training accuracy :  0.852974\n",
      "i: 5200 , cost :  1.2287 , training accuracy :  0.824916\n",
      "i: 5300 , cost :  5.14822 , training accuracy :  0.786756\n",
      "i: 5400 , cost :  0.772949 , training accuracy :  0.863075\n",
      "i: 5500 , cost :  3.35352 , training accuracy :  0.603816\n",
      "i: 5600 , cost :  0.935235 , training accuracy :  0.856341\n",
      "i: 5700 , cost :  0.995102 , training accuracy :  0.774411\n",
      "i: 5800 , cost :  0.779854 , training accuracy :  0.869809\n",
      "i: 5900 , cost :  3.24768 , training accuracy :  0.799102\n",
      "i: 6000 , cost :  0.753172 , training accuracy :  0.863075\n",
      "i: 6100 , cost :  3.50172 , training accuracy :  0.789001\n",
      "i: 6200 , cost :  0.677147 , training accuracy :  0.866442\n",
      "i: 6300 , cost :  7.32346 , training accuracy :  0.448934\n",
      "i: 6400 , cost :  1.0345 , training accuracy :  0.837261\n",
      "i: 6500 , cost :  3.32215 , training accuracy :  0.801347\n",
      "i: 6600 , cost :  0.751646 , training accuracy :  0.872054\n",
      "i: 6700 , cost :  10.8388 , training accuracy :  0.737374\n",
      "i: 6800 , cost :  1.27181 , training accuracy :  0.822671\n",
      "i: 6900 , cost :  0.704443 , training accuracy :  0.872054\n",
      "i: 7000 , cost :  1.30906 , training accuracy :  0.847363\n",
      "i: 7100 , cost :  0.796299 , training accuracy :  0.855219\n",
      "i: 7200 , cost :  0.740256 , training accuracy :  0.867565\n",
      "i: 7300 , cost :  3.19998 , training accuracy :  0.638608\n",
      "i: 7400 , cost :  0.686141 , training accuracy :  0.877666\n",
      "i: 7500 , cost :  3.08886 , training accuracy :  0.806958\n",
      "i: 7600 , cost :  5.44277 , training accuracy :  0.757576\n",
      "i: 7700 , cost :  6.29031 , training accuracy :  0.527497\n",
      "i: 7800 , cost :  0.640856 , training accuracy :  0.885522\n",
      "i: 7900 , cost :  3.49337 , training accuracy :  0.809203\n",
      "i: 8000 , cost :  0.565033 , training accuracy :  0.888889\n",
      "i: 8100 , cost :  1.80052 , training accuracy :  0.837261\n",
      "i: 8200 , cost :  0.686509 , training accuracy :  0.882155\n",
      "i: 8300 , cost :  2.50941 , training accuracy :  0.716049\n",
      "i: 8400 , cost :  0.550386 , training accuracy :  0.888889\n",
      "i: 8500 , cost :  4.18563 , training accuracy :  0.618406\n",
      "i: 8600 , cost :  0.564311 , training accuracy :  0.878788\n",
      "i: 8700 , cost :  2.29406 , training accuracy :  0.821549\n",
      "i: 8800 , cost :  0.740831 , training accuracy :  0.858586\n",
      "i: 8900 , cost :  0.951099 , training accuracy :  0.875421\n",
      "i: 9000 , cost :  0.910873 , training accuracy :  0.849607\n",
      "i: 9100 , cost :  1.00589 , training accuracy :  0.85073\n",
      "i: 9200 , cost :  0.617367 , training accuracy :  0.854097\n",
      "i: 9300 , cost :  1.08693 , training accuracy :  0.841751\n",
      "i: 9400 , cost :  0.715516 , training accuracy :  0.886644\n",
      "i: 9500 , cost :  2.02805 , training accuracy :  0.824916\n",
      "i: 9600 , cost :  0.842662 , training accuracy :  0.869809\n",
      "i: 9700 , cost :  0.677417 , training accuracy :  0.840629\n",
      "i: 9800 , cost :  1.16959 , training accuracy :  0.832772\n",
      "i: 9900 , cost :  0.577059 , training accuracy :  0.870932\n",
      "Final training accuracy :  0.785634\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 10000 ][hidden: 100 ][file: bhavul_tr_acc_0.79_prediction.csv ] ACCURACY :  0.785634\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  50.08 , training accuracy :  0.351291\n",
      "i: 100 , cost :  4.64281 , training accuracy :  0.578002\n",
      "i: 200 , cost :  1.72115 , training accuracy :  0.61055\n",
      "i: 300 , cost :  0.769214 , training accuracy :  0.656566\n",
      "i: 400 , cost :  0.519241 , training accuracy :  0.716049\n",
      "i: 500 , cost :  0.501336 , training accuracy :  0.773288\n",
      "i: 600 , cost :  0.492349 , training accuracy :  0.777778\n",
      "i: 700 , cost :  0.486254 , training accuracy :  0.782267\n",
      "i: 800 , cost :  0.481897 , training accuracy :  0.780022\n",
      "i: 900 , cost :  0.478768 , training accuracy :  0.780022\n",
      "i: 1000 , cost :  0.476077 , training accuracy :  0.781145\n",
      "i: 1100 , cost :  0.474025 , training accuracy :  0.781145\n",
      "i: 1200 , cost :  0.4726 , training accuracy :  0.780022\n",
      "i: 1300 , cost :  0.474292 , training accuracy :  0.792368\n",
      "i: 1400 , cost :  0.469736 , training accuracy :  0.782267\n",
      "i: 1500 , cost :  0.483733 , training accuracy :  0.784512\n",
      "i: 1600 , cost :  0.471673 , training accuracy :  0.774411\n",
      "i: 1700 , cost :  0.468742 , training accuracy :  0.777778\n",
      "i: 1800 , cost :  0.468909 , training accuracy :  0.7789\n",
      "i: 1900 , cost :  0.463674 , training accuracy :  0.784512\n",
      "i: 2000 , cost :  0.467926 , training accuracy :  0.801347\n",
      "i: 2100 , cost :  0.462457 , training accuracy :  0.786756\n",
      "i: 2200 , cost :  0.461716 , training accuracy :  0.785634\n",
      "i: 2300 , cost :  0.462098 , training accuracy :  0.791246\n",
      "i: 2400 , cost :  0.460935 , training accuracy :  0.784512\n",
      "i: 2500 , cost :  0.467582 , training accuracy :  0.774411\n",
      "i: 2600 , cost :  0.463318 , training accuracy :  0.777778\n",
      "i: 2700 , cost :  0.464068 , training accuracy :  0.776655\n",
      "i: 2800 , cost :  0.459231 , training accuracy :  0.784512\n",
      "i: 2900 , cost :  0.459174 , training accuracy :  0.785634\n",
      "i: 3000 , cost :  0.459115 , training accuracy :  0.782267\n",
      "i: 3100 , cost :  0.459971 , training accuracy :  0.783389\n",
      "i: 3200 , cost :  0.458633 , training accuracy :  0.783389\n",
      "i: 3300 , cost :  0.458829 , training accuracy :  0.783389\n",
      "i: 3400 , cost :  0.459052 , training accuracy :  0.789001\n",
      "i: 3500 , cost :  0.462864 , training accuracy :  0.776655\n",
      "i: 3600 , cost :  0.457511 , training accuracy :  0.789001\n",
      "i: 3700 , cost :  0.457449 , training accuracy :  0.790123\n",
      "i: 3800 , cost :  0.460066 , training accuracy :  0.780022\n",
      "i: 3900 , cost :  0.457264 , training accuracy :  0.790123\n",
      "i: 4000 , cost :  0.457071 , training accuracy :  0.791246\n",
      "i: 4100 , cost :  0.45986 , training accuracy :  0.794613\n",
      "i: 4200 , cost :  0.460939 , training accuracy :  0.796857\n",
      "i: 4300 , cost :  0.458911 , training accuracy :  0.781145\n",
      "i: 4400 , cost :  0.459936 , training accuracy :  0.777778\n",
      "i: 4500 , cost :  0.459998 , training accuracy :  0.777778\n",
      "i: 4600 , cost :  0.457187 , training accuracy :  0.792368\n",
      "i: 4700 , cost :  0.457875 , training accuracy :  0.792368\n",
      "i: 4800 , cost :  0.456233 , training accuracy :  0.786756\n",
      "i: 4900 , cost :  0.456325 , training accuracy :  0.79349\n",
      "i: 5000 , cost :  0.457076 , training accuracy :  0.79349\n",
      "i: 5100 , cost :  0.457568 , training accuracy :  0.794613\n",
      "i: 5200 , cost :  0.459324 , training accuracy :  0.79798\n",
      "i: 5300 , cost :  0.461883 , training accuracy :  0.794613\n",
      "i: 5400 , cost :  0.457053 , training accuracy :  0.79349\n",
      "i: 5500 , cost :  0.460941 , training accuracy :  0.794613\n",
      "i: 5600 , cost :  0.457477 , training accuracy :  0.795735\n",
      "i: 5700 , cost :  0.4655 , training accuracy :  0.775533\n",
      "i: 5800 , cost :  0.456655 , training accuracy :  0.794613\n",
      "i: 5900 , cost :  0.456739 , training accuracy :  0.781145\n",
      "i: 6000 , cost :  0.456221 , training accuracy :  0.79349\n",
      "i: 6100 , cost :  0.455666 , training accuracy :  0.789001\n",
      "i: 6200 , cost :  0.457392 , training accuracy :  0.796857\n",
      "i: 6300 , cost :  0.455762 , training accuracy :  0.794613\n",
      "i: 6400 , cost :  0.456231 , training accuracy :  0.795735\n",
      "i: 6500 , cost :  0.45648 , training accuracy :  0.796857\n",
      "i: 6600 , cost :  0.458888 , training accuracy :  0.780022\n",
      "i: 6700 , cost :  0.455496 , training accuracy :  0.783389\n",
      "i: 6800 , cost :  0.455422 , training accuracy :  0.784512\n",
      "i: 6900 , cost :  0.456228 , training accuracy :  0.79349\n",
      "i: 7000 , cost :  0.458222 , training accuracy :  0.796857\n",
      "i: 7100 , cost :  0.457675 , training accuracy :  0.7789\n",
      "i: 7200 , cost :  0.458108 , training accuracy :  0.796857\n",
      "i: 7300 , cost :  0.459078 , training accuracy :  0.794613\n",
      "i: 7400 , cost :  0.455964 , training accuracy :  0.781145\n",
      "i: 7500 , cost :  0.456219 , training accuracy :  0.791246\n",
      "i: 7600 , cost :  0.455564 , training accuracy :  0.780022\n",
      "i: 7700 , cost :  0.466584 , training accuracy :  0.775533\n",
      "i: 7800 , cost :  0.454924 , training accuracy :  0.789001\n",
      "i: 7900 , cost :  0.455182 , training accuracy :  0.780022\n",
      "i: 8000 , cost :  0.454904 , training accuracy :  0.783389\n",
      "i: 8100 , cost :  0.45658 , training accuracy :  0.795735\n",
      "i: 8200 , cost :  0.455206 , training accuracy :  0.781145\n",
      "i: 8300 , cost :  0.457085 , training accuracy :  0.782267\n",
      "i: 8400 , cost :  0.456713 , training accuracy :  0.795735\n",
      "i: 8500 , cost :  0.454155 , training accuracy :  0.791246\n",
      "i: 8600 , cost :  0.456857 , training accuracy :  0.780022\n",
      "i: 8700 , cost :  0.454373 , training accuracy :  0.790123\n",
      "i: 8800 , cost :  0.453746 , training accuracy :  0.783389\n",
      "i: 8900 , cost :  0.456844 , training accuracy :  0.783389\n",
      "i: 9000 , cost :  0.453526 , training accuracy :  0.783389\n",
      "i: 9100 , cost :  0.451849 , training accuracy :  0.801347\n",
      "i: 9200 , cost :  0.457021 , training accuracy :  0.784512\n",
      "i: 9300 , cost :  0.455307 , training accuracy :  0.785634\n",
      "i: 9400 , cost :  0.453728 , training accuracy :  0.785634\n",
      "i: 9500 , cost :  0.449052 , training accuracy :  0.79349\n",
      "i: 9600 , cost :  0.449862 , training accuracy :  0.786756\n",
      "i: 9700 , cost :  0.454168 , training accuracy :  0.800224\n",
      "i: 9800 , cost :  0.449796 , training accuracy :  0.802469\n",
      "i: 9900 , cost :  0.447507 , training accuracy :  0.79349\n",
      "i: 10000 , cost :  0.448235 , training accuracy :  0.804714\n",
      "i: 10100 , cost :  0.453386 , training accuracy :  0.790123\n",
      "i: 10200 , cost :  0.448341 , training accuracy :  0.796857\n",
      "i: 10300 , cost :  0.449065 , training accuracy :  0.802469\n",
      "i: 10400 , cost :  0.438545 , training accuracy :  0.809203\n",
      "i: 10500 , cost :  0.434904 , training accuracy :  0.81257\n",
      "i: 10600 , cost :  0.433529 , training accuracy :  0.815937\n",
      "i: 10700 , cost :  0.433079 , training accuracy :  0.817059\n",
      "i: 10800 , cost :  0.433424 , training accuracy :  0.815937\n",
      "i: 10900 , cost :  0.432756 , training accuracy :  0.810326\n",
      "i: 11000 , cost :  0.43313 , training accuracy :  0.806958\n",
      "i: 11100 , cost :  0.441536 , training accuracy :  0.811448\n",
      "i: 11200 , cost :  0.433575 , training accuracy :  0.809203\n",
      "i: 11300 , cost :  0.434667 , training accuracy :  0.809203\n",
      "i: 11400 , cost :  0.439273 , training accuracy :  0.810326\n",
      "i: 11500 , cost :  0.436586 , training accuracy :  0.810326\n",
      "i: 11600 , cost :  0.432209 , training accuracy :  0.808081\n",
      "i: 11700 , cost :  0.43028 , training accuracy :  0.811448\n",
      "i: 11800 , cost :  0.433718 , training accuracy :  0.810326\n",
      "i: 11900 , cost :  0.435018 , training accuracy :  0.81257\n",
      "i: 12000 , cost :  0.433402 , training accuracy :  0.810326\n",
      "i: 12100 , cost :  0.438181 , training accuracy :  0.813693\n",
      "i: 12200 , cost :  0.429654 , training accuracy :  0.806958\n",
      "i: 12300 , cost :  0.434626 , training accuracy :  0.817059\n",
      "i: 12400 , cost :  0.429252 , training accuracy :  0.811448\n",
      "i: 12500 , cost :  0.429207 , training accuracy :  0.809203\n",
      "i: 12600 , cost :  0.433633 , training accuracy :  0.813693\n",
      "i: 12700 , cost :  0.429979 , training accuracy :  0.811448\n",
      "i: 12800 , cost :  0.429045 , training accuracy :  0.805836\n",
      "i: 12900 , cost :  0.429765 , training accuracy :  0.810326\n",
      "i: 13000 , cost :  0.430018 , training accuracy :  0.811448\n",
      "i: 13100 , cost :  0.429518 , training accuracy :  0.810326\n",
      "i: 13200 , cost :  0.429268 , training accuracy :  0.808081\n",
      "i: 13300 , cost :  0.432825 , training accuracy :  0.806958\n",
      "i: 13400 , cost :  0.43183 , training accuracy :  0.81257\n",
      "i: 13500 , cost :  0.430769 , training accuracy :  0.810326\n",
      "i: 13600 , cost :  0.429241 , training accuracy :  0.808081\n",
      "i: 13700 , cost :  0.428973 , training accuracy :  0.804714\n",
      "i: 13800 , cost :  0.432755 , training accuracy :  0.814815\n",
      "i: 13900 , cost :  0.429804 , training accuracy :  0.810326\n",
      "i: 14000 , cost :  0.429386 , training accuracy :  0.809203\n",
      "i: 14100 , cost :  0.429056 , training accuracy :  0.810326\n",
      "i: 14200 , cost :  0.430776 , training accuracy :  0.811448\n",
      "i: 14300 , cost :  0.429949 , training accuracy :  0.811448\n",
      "i: 14400 , cost :  0.42909 , training accuracy :  0.810326\n",
      "i: 14500 , cost :  0.431427 , training accuracy :  0.809203\n",
      "i: 14600 , cost :  0.433764 , training accuracy :  0.805836\n",
      "i: 14700 , cost :  0.429125 , training accuracy :  0.808081\n",
      "i: 14800 , cost :  0.429024 , training accuracy :  0.808081\n",
      "i: 14900 , cost :  0.433025 , training accuracy :  0.81257\n",
      "i: 15000 , cost :  0.430932 , training accuracy :  0.81257\n",
      "i: 15100 , cost :  0.432001 , training accuracy :  0.809203\n",
      "i: 15200 , cost :  0.429376 , training accuracy :  0.808081\n",
      "i: 15300 , cost :  0.42955 , training accuracy :  0.809203\n",
      "i: 15400 , cost :  0.429085 , training accuracy :  0.808081\n",
      "i: 15500 , cost :  0.434993 , training accuracy :  0.815937\n",
      "i: 15600 , cost :  0.4293 , training accuracy :  0.809203\n",
      "i: 15700 , cost :  0.429409 , training accuracy :  0.810326\n",
      "i: 15800 , cost :  0.428909 , training accuracy :  0.808081\n",
      "i: 15900 , cost :  0.42891 , training accuracy :  0.809203\n",
      "i: 16000 , cost :  0.430155 , training accuracy :  0.811448\n",
      "i: 16100 , cost :  0.433347 , training accuracy :  0.805836\n",
      "i: 16200 , cost :  0.429166 , training accuracy :  0.806958\n",
      "i: 16300 , cost :  0.429954 , training accuracy :  0.811448\n",
      "i: 16400 , cost :  0.429349 , training accuracy :  0.809203\n",
      "i: 16500 , cost :  0.429921 , training accuracy :  0.810326\n",
      "i: 16600 , cost :  0.429188 , training accuracy :  0.808081\n",
      "i: 16700 , cost :  0.437357 , training accuracy :  0.811448\n",
      "i: 16800 , cost :  0.429813 , training accuracy :  0.810326\n",
      "i: 16900 , cost :  0.428925 , training accuracy :  0.808081\n",
      "i: 17000 , cost :  0.429491 , training accuracy :  0.810326\n",
      "i: 17100 , cost :  0.432894 , training accuracy :  0.81257\n",
      "i: 17200 , cost :  0.431033 , training accuracy :  0.810326\n",
      "i: 17300 , cost :  0.430675 , training accuracy :  0.810326\n",
      "i: 17400 , cost :  0.429499 , training accuracy :  0.808081\n",
      "i: 17500 , cost :  0.432107 , training accuracy :  0.811448\n",
      "i: 17600 , cost :  0.438776 , training accuracy :  0.811448\n",
      "i: 17700 , cost :  0.444661 , training accuracy :  0.809203\n",
      "i: 17800 , cost :  0.428965 , training accuracy :  0.806958\n",
      "i: 17900 , cost :  0.429549 , training accuracy :  0.808081\n",
      "i: 18000 , cost :  0.429783 , training accuracy :  0.81257\n",
      "i: 18100 , cost :  0.429984 , training accuracy :  0.811448\n",
      "i: 18200 , cost :  0.430627 , training accuracy :  0.81257\n",
      "i: 18300 , cost :  0.440108 , training accuracy :  0.803591\n",
      "i: 18400 , cost :  0.42903 , training accuracy :  0.810326\n",
      "i: 18500 , cost :  0.430779 , training accuracy :  0.810326\n",
      "i: 18600 , cost :  0.432349 , training accuracy :  0.809203\n",
      "i: 18700 , cost :  0.444222 , training accuracy :  0.811448\n",
      "i: 18800 , cost :  0.428912 , training accuracy :  0.809203\n",
      "i: 18900 , cost :  0.430389 , training accuracy :  0.81257\n",
      "i: 19000 , cost :  0.428945 , training accuracy :  0.805836\n",
      "i: 19100 , cost :  0.428941 , training accuracy :  0.809203\n",
      "i: 19200 , cost :  0.429729 , training accuracy :  0.81257\n",
      "i: 19300 , cost :  0.429522 , training accuracy :  0.808081\n",
      "i: 19400 , cost :  0.42935 , training accuracy :  0.806958\n",
      "i: 19500 , cost :  0.428947 , training accuracy :  0.805836\n",
      "i: 19600 , cost :  0.430247 , training accuracy :  0.811448\n",
      "i: 19700 , cost :  0.43053 , training accuracy :  0.81257\n",
      "i: 19800 , cost :  0.431668 , training accuracy :  0.809203\n",
      "i: 19900 , cost :  0.434324 , training accuracy :  0.815937\n",
      "i: 20000 , cost :  0.431547 , training accuracy :  0.811448\n",
      "i: 20100 , cost :  0.433763 , training accuracy :  0.805836\n",
      "i: 20200 , cost :  0.438125 , training accuracy :  0.811448\n",
      "i: 20300 , cost :  0.430373 , training accuracy :  0.811448\n",
      "i: 20400 , cost :  0.433468 , training accuracy :  0.813693\n",
      "i: 20500 , cost :  0.431775 , training accuracy :  0.809203\n",
      "i: 20600 , cost :  0.428961 , training accuracy :  0.809203\n",
      "i: 20700 , cost :  0.428902 , training accuracy :  0.805836\n",
      "i: 20800 , cost :  0.429848 , training accuracy :  0.811448\n",
      "i: 20900 , cost :  0.429248 , training accuracy :  0.808081\n",
      "i: 21000 , cost :  0.429158 , training accuracy :  0.808081\n",
      "i: 21100 , cost :  0.43186 , training accuracy :  0.809203\n",
      "i: 21200 , cost :  0.428908 , training accuracy :  0.809203\n",
      "i: 21300 , cost :  0.439706 , training accuracy :  0.809203\n",
      "i: 21400 , cost :  0.429193 , training accuracy :  0.810326\n",
      "i: 21500 , cost :  0.429676 , training accuracy :  0.81257\n",
      "i: 21600 , cost :  0.429607 , training accuracy :  0.806958\n",
      "i: 21700 , cost :  0.43129 , training accuracy :  0.811448\n",
      "i: 21800 , cost :  0.428907 , training accuracy :  0.806958\n",
      "i: 21900 , cost :  0.429092 , training accuracy :  0.81257\n",
      "i: 22000 , cost :  0.430306 , training accuracy :  0.811448\n",
      "i: 22100 , cost :  0.428936 , training accuracy :  0.805836\n",
      "i: 22200 , cost :  0.428944 , training accuracy :  0.805836\n",
      "i: 22300 , cost :  0.431744 , training accuracy :  0.811448\n",
      "i: 22400 , cost :  0.433956 , training accuracy :  0.817059\n",
      "i: 22500 , cost :  0.430055 , training accuracy :  0.811448\n",
      "i: 22600 , cost :  0.430543 , training accuracy :  0.811448\n",
      "i: 22700 , cost :  0.429559 , training accuracy :  0.81257\n",
      "i: 22800 , cost :  0.431086 , training accuracy :  0.810326\n",
      "i: 22900 , cost :  0.430768 , training accuracy :  0.81257\n",
      "i: 23000 , cost :  0.429496 , training accuracy :  0.806958\n",
      "i: 23100 , cost :  0.430634 , training accuracy :  0.81257\n",
      "i: 23200 , cost :  0.429668 , training accuracy :  0.810326\n",
      "i: 23300 , cost :  0.428969 , training accuracy :  0.808081\n",
      "i: 23400 , cost :  0.428933 , training accuracy :  0.806958\n",
      "i: 23500 , cost :  0.429521 , training accuracy :  0.810326\n",
      "i: 23600 , cost :  0.429567 , training accuracy :  0.81257\n",
      "i: 23700 , cost :  0.429187 , training accuracy :  0.808081\n",
      "i: 23800 , cost :  0.429709 , training accuracy :  0.811448\n",
      "i: 23900 , cost :  0.433453 , training accuracy :  0.805836\n",
      "i: 24000 , cost :  0.429314 , training accuracy :  0.809203\n",
      "i: 24100 , cost :  0.428984 , training accuracy :  0.809203\n",
      "i: 24200 , cost :  0.431015 , training accuracy :  0.811448\n",
      "i: 24300 , cost :  0.436171 , training accuracy :  0.808081\n",
      "i: 24400 , cost :  0.429364 , training accuracy :  0.805836\n",
      "i: 24500 , cost :  0.429416 , training accuracy :  0.809203\n",
      "i: 24600 , cost :  0.429871 , training accuracy :  0.809203\n",
      "i: 24700 , cost :  0.42982 , training accuracy :  0.810326\n",
      "i: 24800 , cost :  0.42954 , training accuracy :  0.806958\n",
      "i: 24900 , cost :  0.430241 , training accuracy :  0.811448\n",
      "i: 25000 , cost :  0.42952 , training accuracy :  0.810326\n",
      "i: 25100 , cost :  0.428912 , training accuracy :  0.805836\n",
      "i: 25200 , cost :  0.429318 , training accuracy :  0.808081\n",
      "i: 25300 , cost :  0.431957 , training accuracy :  0.809203\n",
      "i: 25400 , cost :  0.437017 , training accuracy :  0.806958\n",
      "i: 25500 , cost :  0.428958 , training accuracy :  0.809203\n",
      "i: 25600 , cost :  0.429408 , training accuracy :  0.810326\n",
      "i: 25700 , cost :  0.428903 , training accuracy :  0.808081\n",
      "i: 25800 , cost :  0.428996 , training accuracy :  0.808081\n",
      "i: 25900 , cost :  0.430333 , training accuracy :  0.811448\n",
      "i: 26000 , cost :  0.43659 , training accuracy :  0.813693\n",
      "i: 26100 , cost :  0.430466 , training accuracy :  0.811448\n",
      "i: 26200 , cost :  0.429262 , training accuracy :  0.808081\n",
      "i: 26300 , cost :  0.429019 , training accuracy :  0.809203\n",
      "i: 26400 , cost :  0.429499 , training accuracy :  0.808081\n",
      "i: 26500 , cost :  0.431191 , training accuracy :  0.810326\n",
      "i: 26600 , cost :  0.433568 , training accuracy :  0.805836\n",
      "i: 26700 , cost :  0.434437 , training accuracy :  0.806958\n",
      "i: 26800 , cost :  0.438407 , training accuracy :  0.804714\n",
      "i: 26900 , cost :  0.428935 , training accuracy :  0.806958\n",
      "i: 27000 , cost :  0.428919 , training accuracy :  0.805836\n",
      "i: 27100 , cost :  0.429176 , training accuracy :  0.808081\n",
      "i: 27200 , cost :  0.432873 , training accuracy :  0.806958\n",
      "i: 27300 , cost :  0.430139 , training accuracy :  0.811448\n",
      "i: 27400 , cost :  0.434565 , training accuracy :  0.806958\n",
      "i: 27500 , cost :  0.431206 , training accuracy :  0.811448\n",
      "i: 27600 , cost :  0.430734 , training accuracy :  0.81257\n",
      "i: 27700 , cost :  0.431708 , training accuracy :  0.809203\n",
      "i: 27800 , cost :  0.43337 , training accuracy :  0.811448\n",
      "i: 27900 , cost :  0.4312 , training accuracy :  0.811448\n",
      "i: 28000 , cost :  0.442437 , training accuracy :  0.808081\n",
      "i: 28100 , cost :  0.429317 , training accuracy :  0.809203\n",
      "i: 28200 , cost :  0.429983 , training accuracy :  0.811448\n",
      "i: 28300 , cost :  0.429067 , training accuracy :  0.806958\n",
      "i: 28400 , cost :  0.429181 , training accuracy :  0.808081\n",
      "i: 28500 , cost :  0.429695 , training accuracy :  0.811448\n",
      "i: 28600 , cost :  0.429914 , training accuracy :  0.811448\n",
      "i: 28700 , cost :  0.432387 , training accuracy :  0.808081\n",
      "i: 28800 , cost :  0.428917 , training accuracy :  0.804714\n",
      "i: 28900 , cost :  0.430524 , training accuracy :  0.81257\n",
      "i: 29000 , cost :  0.428957 , training accuracy :  0.809203\n",
      "i: 29100 , cost :  0.430251 , training accuracy :  0.811448\n",
      "i: 29200 , cost :  0.430919 , training accuracy :  0.81257\n",
      "i: 29300 , cost :  0.434011 , training accuracy :  0.817059\n",
      "i: 29400 , cost :  0.430008 , training accuracy :  0.811448\n",
      "i: 29500 , cost :  0.429172 , training accuracy :  0.808081\n",
      "i: 29600 , cost :  0.442649 , training accuracy :  0.806958\n",
      "i: 29700 , cost :  0.429004 , training accuracy :  0.810326\n",
      "i: 29800 , cost :  0.434597 , training accuracy :  0.815937\n",
      "i: 29900 , cost :  0.432633 , training accuracy :  0.806958\n",
      "Final training accuracy :  0.810326\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 30000 ][hidden: 3 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.810326\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  395.52 , training accuracy :  0.616162\n",
      "i: 100 , cost :  17.4985 , training accuracy :  0.69248\n",
      "i: 200 , cost :  5.81384 , training accuracy :  0.652076\n",
      "i: 300 , cost :  2.22032 , training accuracy :  0.698092\n",
      "i: 400 , cost :  1.59368 , training accuracy :  0.717172\n",
      "i: 500 , cost :  1.19457 , training accuracy :  0.727273\n",
      "i: 600 , cost :  1.03013 , training accuracy :  0.734007\n",
      "i: 700 , cost :  0.909413 , training accuracy :  0.744108\n",
      "i: 800 , cost :  0.770575 , training accuracy :  0.767677\n",
      "i: 900 , cost :  0.707687 , training accuracy :  0.781145\n",
      "i: 1000 , cost :  0.64661 , training accuracy :  0.7789\n",
      "i: 1100 , cost :  0.579189 , training accuracy :  0.776655\n",
      "i: 1200 , cost :  0.52232 , training accuracy :  0.7789\n",
      "i: 1300 , cost :  0.491838 , training accuracy :  0.79349\n",
      "i: 1400 , cost :  0.47994 , training accuracy :  0.804714\n",
      "i: 1500 , cost :  0.463339 , training accuracy :  0.79798\n",
      "i: 1600 , cost :  0.45201 , training accuracy :  0.800224\n",
      "i: 1700 , cost :  0.443291 , training accuracy :  0.805836\n",
      "i: 1800 , cost :  0.444749 , training accuracy :  0.794613\n",
      "i: 1900 , cost :  0.47401 , training accuracy :  0.799102\n",
      "i: 2000 , cost :  0.441005 , training accuracy :  0.806958\n",
      "i: 2100 , cost :  0.451395 , training accuracy :  0.809203\n",
      "i: 2200 , cost :  0.437168 , training accuracy :  0.794613\n",
      "i: 2300 , cost :  0.429755 , training accuracy :  0.808081\n",
      "i: 2400 , cost :  0.42451 , training accuracy :  0.800224\n",
      "i: 2500 , cost :  0.427521 , training accuracy :  0.796857\n",
      "i: 2600 , cost :  0.43811 , training accuracy :  0.813693\n",
      "i: 2700 , cost :  0.451556 , training accuracy :  0.792368\n",
      "i: 2800 , cost :  0.426665 , training accuracy :  0.811448\n",
      "i: 2900 , cost :  0.429297 , training accuracy :  0.813693\n",
      "i: 3000 , cost :  0.449968 , training accuracy :  0.795735\n",
      "i: 3100 , cost :  0.422019 , training accuracy :  0.803591\n",
      "i: 3200 , cost :  0.445747 , training accuracy :  0.813693\n",
      "i: 3300 , cost :  0.425114 , training accuracy :  0.796857\n",
      "i: 3400 , cost :  0.433525 , training accuracy :  0.796857\n",
      "i: 3500 , cost :  0.421018 , training accuracy :  0.81257\n",
      "i: 3600 , cost :  0.448841 , training accuracy :  0.818182\n",
      "i: 3700 , cost :  0.419412 , training accuracy :  0.81257\n",
      "i: 3800 , cost :  0.435634 , training accuracy :  0.796857\n",
      "i: 3900 , cost :  0.443852 , training accuracy :  0.801347\n",
      "i: 4000 , cost :  0.420932 , training accuracy :  0.803591\n",
      "i: 4100 , cost :  0.464625 , training accuracy :  0.805836\n",
      "i: 4200 , cost :  0.446524 , training accuracy :  0.801347\n",
      "i: 4300 , cost :  0.423817 , training accuracy :  0.814815\n",
      "i: 4400 , cost :  0.423524 , training accuracy :  0.814815\n",
      "i: 4500 , cost :  0.421787 , training accuracy :  0.802469\n",
      "i: 4600 , cost :  0.416971 , training accuracy :  0.804714\n",
      "i: 4700 , cost :  0.4195 , training accuracy :  0.802469\n",
      "i: 4800 , cost :  0.416944 , training accuracy :  0.815937\n",
      "i: 4900 , cost :  0.419557 , training accuracy :  0.802469\n",
      "i: 5000 , cost :  0.416237 , training accuracy :  0.805836\n",
      "i: 5100 , cost :  0.432269 , training accuracy :  0.799102\n",
      "i: 5200 , cost :  0.453322 , training accuracy :  0.808081\n",
      "i: 5300 , cost :  0.415771 , training accuracy :  0.808081\n",
      "i: 5400 , cost :  0.434616 , training accuracy :  0.804714\n",
      "i: 5500 , cost :  0.428862 , training accuracy :  0.815937\n",
      "i: 5600 , cost :  0.432621 , training accuracy :  0.819304\n",
      "i: 5700 , cost :  0.42305 , training accuracy :  0.817059\n",
      "i: 5800 , cost :  0.416864 , training accuracy :  0.804714\n",
      "i: 5900 , cost :  0.414952 , training accuracy :  0.805836\n",
      "i: 6000 , cost :  0.416161 , training accuracy :  0.814815\n",
      "i: 6100 , cost :  0.426663 , training accuracy :  0.804714\n",
      "i: 6200 , cost :  0.414803 , training accuracy :  0.806958\n",
      "i: 6300 , cost :  0.435095 , training accuracy :  0.819304\n",
      "i: 6400 , cost :  0.453293 , training accuracy :  0.792368\n",
      "i: 6500 , cost :  0.41564 , training accuracy :  0.806958\n",
      "i: 6600 , cost :  0.43867 , training accuracy :  0.818182\n",
      "i: 6700 , cost :  0.433169 , training accuracy :  0.819304\n",
      "i: 6800 , cost :  0.414352 , training accuracy :  0.814815\n",
      "i: 6900 , cost :  0.413976 , training accuracy :  0.81257\n",
      "i: 7000 , cost :  0.454063 , training accuracy :  0.79349\n",
      "i: 7100 , cost :  0.421111 , training accuracy :  0.819304\n",
      "i: 7200 , cost :  0.430726 , training accuracy :  0.804714\n",
      "i: 7300 , cost :  0.413974 , training accuracy :  0.806958\n",
      "i: 7400 , cost :  0.430333 , training accuracy :  0.820426\n",
      "i: 7500 , cost :  0.413988 , training accuracy :  0.806958\n",
      "i: 7600 , cost :  0.413343 , training accuracy :  0.81257\n",
      "i: 7700 , cost :  0.413685 , training accuracy :  0.817059\n",
      "i: 7800 , cost :  0.417249 , training accuracy :  0.820426\n",
      "i: 7900 , cost :  0.417016 , training accuracy :  0.819304\n",
      "i: 8000 , cost :  0.442584 , training accuracy :  0.804714\n",
      "i: 8100 , cost :  0.448452 , training accuracy :  0.796857\n",
      "i: 8200 , cost :  0.413329 , training accuracy :  0.810326\n",
      "i: 8300 , cost :  0.417332 , training accuracy :  0.810326\n",
      "i: 8400 , cost :  0.425713 , training accuracy :  0.804714\n",
      "i: 8500 , cost :  0.413067 , training accuracy :  0.817059\n",
      "i: 8600 , cost :  0.418855 , training accuracy :  0.817059\n",
      "i: 8700 , cost :  0.416847 , training accuracy :  0.820426\n",
      "i: 8800 , cost :  0.431681 , training accuracy :  0.819304\n",
      "i: 8900 , cost :  0.41257 , training accuracy :  0.818182\n",
      "i: 9000 , cost :  0.417144 , training accuracy :  0.818182\n",
      "i: 9100 , cost :  0.422921 , training accuracy :  0.820426\n",
      "i: 9200 , cost :  0.417293 , training accuracy :  0.809203\n",
      "i: 9300 , cost :  0.419203 , training accuracy :  0.819304\n",
      "i: 9400 , cost :  0.432475 , training accuracy :  0.819304\n",
      "i: 9500 , cost :  0.413759 , training accuracy :  0.820426\n",
      "i: 9600 , cost :  0.435943 , training accuracy :  0.819304\n",
      "i: 9700 , cost :  0.421502 , training accuracy :  0.818182\n",
      "i: 9800 , cost :  0.412003 , training accuracy :  0.808081\n",
      "i: 9900 , cost :  0.413913 , training accuracy :  0.810326\n",
      "i: 10000 , cost :  0.413328 , training accuracy :  0.805836\n",
      "i: 10100 , cost :  0.427558 , training accuracy :  0.808081\n",
      "i: 10200 , cost :  0.41909 , training accuracy :  0.809203\n",
      "i: 10300 , cost :  0.411915 , training accuracy :  0.808081\n",
      "i: 10400 , cost :  0.417756 , training accuracy :  0.810326\n",
      "i: 10500 , cost :  0.42793 , training accuracy :  0.806958\n",
      "i: 10600 , cost :  0.427214 , training accuracy :  0.819304\n",
      "i: 10700 , cost :  0.418476 , training accuracy :  0.819304\n",
      "i: 10800 , cost :  0.440515 , training accuracy :  0.805836\n",
      "i: 10900 , cost :  0.411389 , training accuracy :  0.814815\n",
      "i: 11000 , cost :  0.420202 , training accuracy :  0.81257\n",
      "i: 11100 , cost :  0.412058 , training accuracy :  0.81257\n",
      "i: 11200 , cost :  0.42963 , training accuracy :  0.823793\n",
      "i: 11300 , cost :  0.410736 , training accuracy :  0.809203\n",
      "i: 11400 , cost :  0.422545 , training accuracy :  0.810326\n",
      "i: 11500 , cost :  0.416996 , training accuracy :  0.810326\n",
      "i: 11600 , cost :  0.420912 , training accuracy :  0.811448\n",
      "i: 11700 , cost :  0.459122 , training accuracy :  0.802469\n",
      "i: 11800 , cost :  0.4108 , training accuracy :  0.819304\n",
      "i: 11900 , cost :  0.411288 , training accuracy :  0.815937\n",
      "i: 12000 , cost :  0.414213 , training accuracy :  0.810326\n",
      "i: 12100 , cost :  0.426737 , training accuracy :  0.809203\n",
      "i: 12200 , cost :  0.415044 , training accuracy :  0.823793\n",
      "i: 12300 , cost :  0.414931 , training accuracy :  0.81257\n",
      "i: 12400 , cost :  0.437509 , training accuracy :  0.821549\n",
      "i: 12500 , cost :  0.411879 , training accuracy :  0.820426\n",
      "i: 12600 , cost :  0.433991 , training accuracy :  0.822671\n",
      "i: 12700 , cost :  0.411053 , training accuracy :  0.810326\n",
      "i: 12800 , cost :  0.420843 , training accuracy :  0.823793\n",
      "i: 12900 , cost :  0.431026 , training accuracy :  0.810326\n",
      "i: 13000 , cost :  0.41353 , training accuracy :  0.81257\n",
      "i: 13100 , cost :  0.415838 , training accuracy :  0.810326\n",
      "i: 13200 , cost :  0.409978 , training accuracy :  0.811448\n",
      "i: 13300 , cost :  0.425691 , training accuracy :  0.822671\n",
      "i: 13400 , cost :  0.429702 , training accuracy :  0.823793\n",
      "i: 13500 , cost :  0.425694 , training accuracy :  0.822671\n",
      "i: 13600 , cost :  0.423548 , training accuracy :  0.822671\n",
      "i: 13700 , cost :  0.409419 , training accuracy :  0.810326\n",
      "i: 13800 , cost :  0.410283 , training accuracy :  0.81257\n",
      "i: 13900 , cost :  0.411556 , training accuracy :  0.808081\n",
      "i: 14000 , cost :  0.418597 , training accuracy :  0.822671\n",
      "i: 14100 , cost :  0.423167 , training accuracy :  0.808081\n",
      "i: 14200 , cost :  0.412347 , training accuracy :  0.810326\n",
      "i: 14300 , cost :  0.420057 , training accuracy :  0.809203\n",
      "i: 14400 , cost :  0.421251 , training accuracy :  0.808081\n",
      "i: 14500 , cost :  0.423761 , training accuracy :  0.823793\n",
      "i: 14600 , cost :  0.413101 , training accuracy :  0.826038\n",
      "i: 14700 , cost :  0.40961 , training accuracy :  0.820426\n",
      "i: 14800 , cost :  0.409747 , training accuracy :  0.818182\n",
      "i: 14900 , cost :  0.428512 , training accuracy :  0.809203\n",
      "i: 15000 , cost :  0.408514 , training accuracy :  0.813693\n",
      "i: 15100 , cost :  0.415833 , training accuracy :  0.822671\n",
      "i: 15200 , cost :  0.427855 , training accuracy :  0.824916\n",
      "i: 15300 , cost :  0.424518 , training accuracy :  0.823793\n",
      "i: 15400 , cost :  0.421727 , training accuracy :  0.809203\n",
      "i: 15500 , cost :  0.413279 , training accuracy :  0.826038\n",
      "i: 15600 , cost :  0.41704 , training accuracy :  0.823793\n",
      "i: 15700 , cost :  0.41006 , training accuracy :  0.81257\n",
      "i: 15800 , cost :  0.413761 , training accuracy :  0.810326\n",
      "i: 15900 , cost :  0.427866 , training accuracy :  0.810326\n",
      "i: 16000 , cost :  0.409425 , training accuracy :  0.813693\n",
      "i: 16100 , cost :  0.420439 , training accuracy :  0.810326\n",
      "i: 16200 , cost :  0.412194 , training accuracy :  0.81257\n",
      "i: 16300 , cost :  0.408046 , training accuracy :  0.818182\n",
      "i: 16400 , cost :  0.408603 , training accuracy :  0.815937\n",
      "i: 16500 , cost :  0.407962 , training accuracy :  0.822671\n",
      "i: 16600 , cost :  0.459961 , training accuracy :  0.811448\n",
      "i: 16700 , cost :  0.410647 , training accuracy :  0.82716\n",
      "i: 16800 , cost :  0.406383 , training accuracy :  0.822671\n",
      "i: 16900 , cost :  0.421217 , training accuracy :  0.809203\n",
      "i: 17000 , cost :  0.411522 , training accuracy :  0.815937\n",
      "i: 17100 , cost :  0.417358 , training accuracy :  0.821549\n",
      "i: 17200 , cost :  0.455657 , training accuracy :  0.81257\n",
      "i: 17300 , cost :  0.421156 , training accuracy :  0.821549\n",
      "i: 17400 , cost :  0.405148 , training accuracy :  0.835017\n",
      "i: 17500 , cost :  0.413488 , training accuracy :  0.824916\n",
      "i: 17600 , cost :  0.429341 , training accuracy :  0.800224\n",
      "i: 17700 , cost :  0.415454 , training accuracy :  0.811448\n",
      "i: 17800 , cost :  0.40122 , training accuracy :  0.824916\n",
      "i: 17900 , cost :  0.413715 , training accuracy :  0.826038\n",
      "i: 18000 , cost :  0.428856 , training accuracy :  0.802469\n",
      "i: 18100 , cost :  0.40382 , training accuracy :  0.833894\n",
      "i: 18200 , cost :  0.417267 , training accuracy :  0.804714\n",
      "i: 18300 , cost :  0.400059 , training accuracy :  0.828283\n",
      "i: 18400 , cost :  0.400594 , training accuracy :  0.826038\n",
      "i: 18500 , cost :  0.399814 , training accuracy :  0.82716\n",
      "i: 18600 , cost :  0.411844 , training accuracy :  0.809203\n",
      "i: 18700 , cost :  0.430642 , training accuracy :  0.794613\n",
      "i: 18800 , cost :  0.40713 , training accuracy :  0.817059\n",
      "i: 18900 , cost :  0.398953 , training accuracy :  0.824916\n",
      "i: 19000 , cost :  0.404413 , training accuracy :  0.820426\n",
      "i: 19100 , cost :  0.417002 , training accuracy :  0.809203\n",
      "i: 19200 , cost :  0.402732 , training accuracy :  0.83165\n",
      "i: 19300 , cost :  0.411456 , training accuracy :  0.829405\n",
      "i: 19400 , cost :  0.429383 , training accuracy :  0.79798\n",
      "i: 19500 , cost :  0.400551 , training accuracy :  0.819304\n",
      "i: 19600 , cost :  0.397933 , training accuracy :  0.824916\n",
      "i: 19700 , cost :  0.399227 , training accuracy :  0.832772\n",
      "i: 19800 , cost :  0.41608 , training accuracy :  0.823793\n",
      "i: 19900 , cost :  0.413599 , training accuracy :  0.81257\n",
      "i: 20000 , cost :  0.406194 , training accuracy :  0.826038\n",
      "i: 20100 , cost :  0.41819 , training accuracy :  0.822671\n",
      "i: 20200 , cost :  0.400801 , training accuracy :  0.83165\n",
      "i: 20300 , cost :  0.398948 , training accuracy :  0.817059\n",
      "i: 20400 , cost :  0.397947 , training accuracy :  0.829405\n",
      "i: 20500 , cost :  0.418526 , training accuracy :  0.804714\n",
      "i: 20600 , cost :  0.401361 , training accuracy :  0.83165\n",
      "i: 20700 , cost :  0.396813 , training accuracy :  0.828283\n",
      "i: 20800 , cost :  0.418587 , training accuracy :  0.821549\n",
      "i: 20900 , cost :  0.404944 , training accuracy :  0.829405\n",
      "i: 21000 , cost :  0.414855 , training accuracy :  0.821549\n",
      "i: 21100 , cost :  0.396388 , training accuracy :  0.82716\n",
      "i: 21200 , cost :  0.41996 , training accuracy :  0.804714\n",
      "i: 21300 , cost :  0.397529 , training accuracy :  0.826038\n",
      "i: 21400 , cost :  0.39689 , training accuracy :  0.821549\n",
      "i: 21500 , cost :  0.409933 , training accuracy :  0.814815\n",
      "i: 21600 , cost :  0.417836 , training accuracy :  0.803591\n",
      "i: 21700 , cost :  0.426844 , training accuracy :  0.822671\n",
      "i: 21800 , cost :  0.399663 , training accuracy :  0.835017\n",
      "i: 21900 , cost :  0.413319 , training accuracy :  0.828283\n",
      "i: 22000 , cost :  0.417818 , training accuracy :  0.804714\n",
      "i: 22100 , cost :  0.396481 , training accuracy :  0.829405\n",
      "i: 22200 , cost :  0.417833 , training accuracy :  0.826038\n",
      "i: 22300 , cost :  0.395557 , training accuracy :  0.830527\n",
      "i: 22400 , cost :  0.41712 , training accuracy :  0.823793\n",
      "i: 22500 , cost :  0.400451 , training accuracy :  0.829405\n",
      "i: 22600 , cost :  0.396123 , training accuracy :  0.824916\n",
      "i: 22700 , cost :  0.399437 , training accuracy :  0.822671\n",
      "i: 22800 , cost :  0.403775 , training accuracy :  0.829405\n",
      "i: 22900 , cost :  0.416175 , training accuracy :  0.823793\n",
      "i: 23000 , cost :  0.398825 , training accuracy :  0.820426\n",
      "i: 23100 , cost :  0.395182 , training accuracy :  0.830527\n",
      "i: 23200 , cost :  0.42052 , training accuracy :  0.800224\n",
      "i: 23300 , cost :  0.397012 , training accuracy :  0.823793\n",
      "i: 23400 , cost :  0.413868 , training accuracy :  0.822671\n",
      "i: 23500 , cost :  0.395436 , training accuracy :  0.833894\n",
      "i: 23600 , cost :  0.429816 , training accuracy :  0.820426\n",
      "i: 23700 , cost :  0.401417 , training accuracy :  0.823793\n",
      "i: 23800 , cost :  0.428513 , training accuracy :  0.821549\n",
      "i: 23900 , cost :  0.394894 , training accuracy :  0.83165\n",
      "i: 24000 , cost :  0.403379 , training accuracy :  0.822671\n",
      "i: 24100 , cost :  0.397193 , training accuracy :  0.828283\n",
      "i: 24200 , cost :  0.400552 , training accuracy :  0.82716\n",
      "i: 24300 , cost :  0.395483 , training accuracy :  0.828283\n",
      "i: 24400 , cost :  0.398939 , training accuracy :  0.832772\n",
      "i: 24500 , cost :  0.408073 , training accuracy :  0.818182\n",
      "i: 24600 , cost :  0.413608 , training accuracy :  0.826038\n",
      "i: 24700 , cost :  0.395135 , training accuracy :  0.826038\n",
      "i: 24800 , cost :  0.397896 , training accuracy :  0.820426\n",
      "i: 24900 , cost :  0.406922 , training accuracy :  0.82716\n",
      "i: 25000 , cost :  0.415 , training accuracy :  0.826038\n",
      "i: 25100 , cost :  0.396833 , training accuracy :  0.836139\n",
      "i: 25200 , cost :  0.394615 , training accuracy :  0.829405\n",
      "i: 25300 , cost :  0.413324 , training accuracy :  0.81257\n",
      "i: 25400 , cost :  0.414844 , training accuracy :  0.828283\n",
      "i: 25500 , cost :  0.406639 , training accuracy :  0.817059\n",
      "i: 25600 , cost :  0.397878 , training accuracy :  0.83165\n",
      "i: 25700 , cost :  0.393895 , training accuracy :  0.83165\n",
      "i: 25800 , cost :  0.422747 , training accuracy :  0.824916\n",
      "i: 25900 , cost :  0.409836 , training accuracy :  0.82716\n",
      "i: 26000 , cost :  0.399854 , training accuracy :  0.828283\n",
      "i: 26100 , cost :  0.43602 , training accuracy :  0.794613\n",
      "i: 26200 , cost :  0.393701 , training accuracy :  0.829405\n",
      "i: 26300 , cost :  0.40692 , training accuracy :  0.818182\n",
      "i: 26400 , cost :  0.409402 , training accuracy :  0.82716\n",
      "i: 26500 , cost :  0.417304 , training accuracy :  0.82716\n",
      "i: 26600 , cost :  0.401476 , training accuracy :  0.820426\n",
      "i: 26700 , cost :  0.400871 , training accuracy :  0.829405\n",
      "i: 26800 , cost :  0.403081 , training accuracy :  0.824916\n",
      "i: 26900 , cost :  0.396908 , training accuracy :  0.822671\n",
      "i: 27000 , cost :  0.403906 , training accuracy :  0.822671\n",
      "i: 27100 , cost :  0.409085 , training accuracy :  0.814815\n",
      "i: 27200 , cost :  0.41036 , training accuracy :  0.815937\n",
      "i: 27300 , cost :  0.393338 , training accuracy :  0.830527\n",
      "i: 27400 , cost :  0.405855 , training accuracy :  0.818182\n",
      "i: 27500 , cost :  0.414239 , training accuracy :  0.828283\n",
      "i: 27600 , cost :  0.403474 , training accuracy :  0.835017\n",
      "i: 27700 , cost :  0.409602 , training accuracy :  0.814815\n",
      "i: 27800 , cost :  0.402332 , training accuracy :  0.823793\n",
      "i: 27900 , cost :  0.432256 , training accuracy :  0.817059\n",
      "i: 28000 , cost :  0.395883 , training accuracy :  0.833894\n",
      "i: 28100 , cost :  0.423311 , training accuracy :  0.826038\n",
      "i: 28200 , cost :  0.419155 , training accuracy :  0.803591\n",
      "i: 28300 , cost :  0.39572 , training accuracy :  0.833894\n",
      "i: 28400 , cost :  0.405546 , training accuracy :  0.82716\n",
      "i: 28500 , cost :  0.399479 , training accuracy :  0.821549\n",
      "i: 28600 , cost :  0.397087 , training accuracy :  0.830527\n",
      "i: 28700 , cost :  0.408058 , training accuracy :  0.813693\n",
      "i: 28800 , cost :  0.425228 , training accuracy :  0.821549\n",
      "i: 28900 , cost :  0.393655 , training accuracy :  0.82716\n",
      "i: 29000 , cost :  0.393207 , training accuracy :  0.828283\n",
      "i: 29100 , cost :  0.40298 , training accuracy :  0.833894\n",
      "i: 29200 , cost :  0.418409 , training accuracy :  0.82716\n",
      "i: 29300 , cost :  0.412682 , training accuracy :  0.805836\n",
      "i: 29400 , cost :  0.403845 , training accuracy :  0.821549\n",
      "i: 29500 , cost :  0.393356 , training accuracy :  0.829405\n",
      "i: 29600 , cost :  0.400769 , training accuracy :  0.821549\n",
      "i: 29700 , cost :  0.417122 , training accuracy :  0.828283\n",
      "i: 29800 , cost :  0.393484 , training accuracy :  0.83165\n",
      "i: 29900 , cost :  0.402549 , training accuracy :  0.822671\n",
      "Final training accuracy :  0.821549\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 30000 ][hidden: 10 ][file: bhavul_tr_acc_0.82_prediction.csv ] ACCURACY :  0.821549\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  525.714 , training accuracy :  0.377104\n",
      "i: 100 , cost :  14.2805 , training accuracy :  0.555556\n",
      "i: 200 , cost :  5.14412 , training accuracy :  0.581369\n",
      "i: 300 , cost :  2.96471 , training accuracy :  0.668911\n",
      "i: 400 , cost :  1.53777 , training accuracy :  0.718294\n",
      "i: 500 , cost :  0.850445 , training accuracy :  0.765432\n",
      "i: 600 , cost :  0.699362 , training accuracy :  0.777778\n",
      "i: 700 , cost :  0.675103 , training accuracy :  0.780022\n",
      "i: 800 , cost :  0.594916 , training accuracy :  0.787879\n",
      "i: 900 , cost :  0.607248 , training accuracy :  0.777778\n",
      "i: 1000 , cost :  0.623741 , training accuracy :  0.781145\n",
      "i: 1100 , cost :  0.592277 , training accuracy :  0.775533\n",
      "i: 1200 , cost :  0.622046 , training accuracy :  0.777778\n",
      "i: 1300 , cost :  0.581377 , training accuracy :  0.769921\n",
      "i: 1400 , cost :  0.610265 , training accuracy :  0.774411\n",
      "i: 1500 , cost :  0.563232 , training accuracy :  0.773288\n",
      "i: 1600 , cost :  0.585609 , training accuracy :  0.776655\n",
      "i: 1700 , cost :  0.495953 , training accuracy :  0.799102\n",
      "i: 1800 , cost :  0.472968 , training accuracy :  0.809203\n",
      "i: 1900 , cost :  0.478535 , training accuracy :  0.810326\n",
      "i: 2000 , cost :  0.48905 , training accuracy :  0.792368\n",
      "i: 2100 , cost :  0.524733 , training accuracy :  0.796857\n",
      "i: 2200 , cost :  0.516221 , training accuracy :  0.781145\n",
      "i: 2300 , cost :  0.546829 , training accuracy :  0.783389\n",
      "i: 2400 , cost :  0.473536 , training accuracy :  0.796857\n",
      "i: 2500 , cost :  0.454033 , training accuracy :  0.811448\n",
      "i: 2600 , cost :  0.489222 , training accuracy :  0.801347\n",
      "i: 2700 , cost :  0.507019 , training accuracy :  0.790123\n",
      "i: 2800 , cost :  0.49726 , training accuracy :  0.802469\n",
      "i: 2900 , cost :  0.445429 , training accuracy :  0.817059\n",
      "i: 3000 , cost :  0.460028 , training accuracy :  0.804714\n",
      "i: 3100 , cost :  0.513185 , training accuracy :  0.795735\n",
      "i: 3200 , cost :  0.496592 , training accuracy :  0.792368\n",
      "i: 3300 , cost :  0.455842 , training accuracy :  0.818182\n",
      "i: 3400 , cost :  0.441733 , training accuracy :  0.820426\n",
      "i: 3500 , cost :  0.473473 , training accuracy :  0.800224\n",
      "i: 3600 , cost :  0.531284 , training accuracy :  0.786756\n",
      "i: 3700 , cost :  0.455402 , training accuracy :  0.81257\n",
      "i: 3800 , cost :  0.438282 , training accuracy :  0.817059\n",
      "i: 3900 , cost :  0.493938 , training accuracy :  0.802469\n",
      "i: 4000 , cost :  0.487388 , training accuracy :  0.79798\n",
      "i: 4100 , cost :  0.436296 , training accuracy :  0.819304\n",
      "i: 4200 , cost :  0.448802 , training accuracy :  0.819304\n",
      "i: 4300 , cost :  0.485114 , training accuracy :  0.79349\n",
      "i: 4400 , cost :  0.498531 , training accuracy :  0.805836\n",
      "i: 4500 , cost :  0.434428 , training accuracy :  0.813693\n",
      "i: 4600 , cost :  0.437124 , training accuracy :  0.81257\n",
      "i: 4700 , cost :  0.481816 , training accuracy :  0.808081\n",
      "i: 4800 , cost :  0.488635 , training accuracy :  0.79349\n",
      "i: 4900 , cost :  0.466158 , training accuracy :  0.810326\n",
      "i: 5000 , cost :  0.428711 , training accuracy :  0.821549\n",
      "i: 5100 , cost :  0.427399 , training accuracy :  0.821549\n",
      "i: 5200 , cost :  0.473014 , training accuracy :  0.800224\n",
      "i: 5300 , cost :  0.486371 , training accuracy :  0.795735\n",
      "i: 5400 , cost :  0.477886 , training accuracy :  0.799102\n",
      "i: 5500 , cost :  0.460529 , training accuracy :  0.805836\n",
      "i: 5600 , cost :  0.444652 , training accuracy :  0.814815\n",
      "i: 5700 , cost :  0.430527 , training accuracy :  0.818182\n",
      "i: 5800 , cost :  0.425766 , training accuracy :  0.820426\n",
      "i: 5900 , cost :  0.426602 , training accuracy :  0.82716\n",
      "i: 6000 , cost :  0.439617 , training accuracy :  0.821549\n",
      "i: 6100 , cost :  0.452981 , training accuracy :  0.820426\n",
      "i: 6200 , cost :  0.461453 , training accuracy :  0.815937\n",
      "i: 6300 , cost :  0.499633 , training accuracy :  0.801347\n",
      "i: 6400 , cost :  0.532323 , training accuracy :  0.785634\n",
      "i: 6500 , cost :  0.451123 , training accuracy :  0.818182\n",
      "i: 6600 , cost :  0.508105 , training accuracy :  0.796857\n",
      "i: 6700 , cost :  0.444489 , training accuracy :  0.813693\n",
      "i: 6800 , cost :  0.432577 , training accuracy :  0.819304\n",
      "i: 6900 , cost :  0.423058 , training accuracy :  0.828283\n",
      "i: 7000 , cost :  0.456157 , training accuracy :  0.814815\n",
      "i: 7100 , cost :  0.49209 , training accuracy :  0.801347\n",
      "i: 7200 , cost :  0.508711 , training accuracy :  0.79798\n",
      "i: 7300 , cost :  0.453276 , training accuracy :  0.817059\n",
      "i: 7400 , cost :  0.422059 , training accuracy :  0.826038\n",
      "i: 7500 , cost :  0.436202 , training accuracy :  0.818182\n",
      "i: 7600 , cost :  0.46706 , training accuracy :  0.802469\n",
      "i: 7700 , cost :  0.477916 , training accuracy :  0.792368\n",
      "i: 7800 , cost :  0.441687 , training accuracy :  0.814815\n",
      "i: 7900 , cost :  0.421794 , training accuracy :  0.828283\n",
      "i: 8000 , cost :  0.448311 , training accuracy :  0.824916\n",
      "i: 8100 , cost :  0.497008 , training accuracy :  0.800224\n",
      "i: 8200 , cost :  0.482585 , training accuracy :  0.811448\n",
      "i: 8300 , cost :  0.423029 , training accuracy :  0.830527\n",
      "i: 8400 , cost :  0.433795 , training accuracy :  0.820426\n",
      "i: 8500 , cost :  0.46624 , training accuracy :  0.802469\n",
      "i: 8600 , cost :  0.471265 , training accuracy :  0.794613\n",
      "i: 8700 , cost :  0.425218 , training accuracy :  0.822671\n",
      "i: 8800 , cost :  0.433399 , training accuracy :  0.828283\n",
      "i: 8900 , cost :  0.482332 , training accuracy :  0.809203\n",
      "i: 9000 , cost :  0.487936 , training accuracy :  0.809203\n",
      "i: 9100 , cost :  0.422441 , training accuracy :  0.826038\n",
      "i: 9200 , cost :  0.431215 , training accuracy :  0.821549\n",
      "i: 9300 , cost :  0.465868 , training accuracy :  0.800224\n",
      "i: 9400 , cost :  0.463709 , training accuracy :  0.800224\n",
      "i: 9500 , cost :  0.419508 , training accuracy :  0.819304\n",
      "i: 9600 , cost :  0.441342 , training accuracy :  0.824916\n",
      "i: 9700 , cost :  0.489835 , training accuracy :  0.804714\n",
      "i: 9800 , cost :  0.471297 , training accuracy :  0.814815\n",
      "i: 9900 , cost :  0.417386 , training accuracy :  0.830527\n",
      "i: 10000 , cost :  0.440819 , training accuracy :  0.818182\n",
      "i: 10100 , cost :  0.472022 , training accuracy :  0.795735\n",
      "i: 10200 , cost :  0.442969 , training accuracy :  0.815937\n",
      "i: 10300 , cost :  0.418613 , training accuracy :  0.829405\n",
      "i: 10400 , cost :  0.458932 , training accuracy :  0.813693\n",
      "i: 10500 , cost :  0.495712 , training accuracy :  0.802469\n",
      "i: 10600 , cost :  0.441409 , training accuracy :  0.828283\n",
      "i: 10700 , cost :  0.415668 , training accuracy :  0.820426\n",
      "i: 10800 , cost :  0.443754 , training accuracy :  0.814815\n",
      "i: 10900 , cost :  0.468604 , training accuracy :  0.795735\n",
      "i: 11000 , cost :  0.431778 , training accuracy :  0.819304\n",
      "i: 11100 , cost :  0.416835 , training accuracy :  0.830527\n",
      "i: 11200 , cost :  0.461063 , training accuracy :  0.811448\n",
      "i: 11300 , cost :  0.491195 , training accuracy :  0.805836\n",
      "i: 11400 , cost :  0.420634 , training accuracy :  0.828283\n",
      "i: 11500 , cost :  0.421081 , training accuracy :  0.822671\n",
      "i: 11600 , cost :  0.455301 , training accuracy :  0.804714\n",
      "i: 11700 , cost :  0.455899 , training accuracy :  0.804714\n",
      "i: 11800 , cost :  0.411168 , training accuracy :  0.820426\n",
      "i: 11900 , cost :  0.43606 , training accuracy :  0.823793\n",
      "i: 12000 , cost :  0.486433 , training accuracy :  0.802469\n",
      "i: 12100 , cost :  0.446162 , training accuracy :  0.817059\n",
      "i: 12200 , cost :  0.410181 , training accuracy :  0.820426\n",
      "i: 12300 , cost :  0.438742 , training accuracy :  0.815937\n",
      "i: 12400 , cost :  0.46327 , training accuracy :  0.79798\n",
      "i: 12500 , cost :  0.422429 , training accuracy :  0.822671\n",
      "i: 12600 , cost :  0.41526 , training accuracy :  0.829405\n",
      "i: 12700 , cost :  0.466718 , training accuracy :  0.813693\n",
      "i: 12800 , cost :  0.475102 , training accuracy :  0.81257\n",
      "i: 12900 , cost :  0.408091 , training accuracy :  0.832772\n",
      "i: 13000 , cost :  0.427184 , training accuracy :  0.822671\n",
      "i: 13100 , cost :  0.460675 , training accuracy :  0.796857\n",
      "i: 13200 , cost :  0.430929 , training accuracy :  0.818182\n",
      "i: 13300 , cost :  0.410069 , training accuracy :  0.830527\n",
      "i: 13400 , cost :  0.459085 , training accuracy :  0.814815\n",
      "i: 13500 , cost :  0.476398 , training accuracy :  0.815937\n",
      "i: 13600 , cost :  0.406137 , training accuracy :  0.832772\n",
      "i: 13700 , cost :  0.426948 , training accuracy :  0.820426\n",
      "i: 13800 , cost :  0.459339 , training accuracy :  0.79798\n",
      "i: 13900 , cost :  0.419803 , training accuracy :  0.823793\n",
      "i: 14000 , cost :  0.415844 , training accuracy :  0.832772\n",
      "i: 14100 , cost :  0.472778 , training accuracy :  0.813693\n",
      "i: 14200 , cost :  0.445899 , training accuracy :  0.819304\n",
      "i: 14300 , cost :  0.40623 , training accuracy :  0.821549\n",
      "i: 14400 , cost :  0.443275 , training accuracy :  0.814815\n",
      "i: 14500 , cost :  0.448179 , training accuracy :  0.806958\n",
      "i: 14600 , cost :  0.403499 , training accuracy :  0.830527\n",
      "i: 14700 , cost :  0.444623 , training accuracy :  0.817059\n",
      "i: 14800 , cost :  0.480126 , training accuracy :  0.811448\n",
      "i: 14900 , cost :  0.405163 , training accuracy :  0.828283\n",
      "i: 15000 , cost :  0.425277 , training accuracy :  0.821549\n",
      "i: 15100 , cost :  0.456619 , training accuracy :  0.796857\n",
      "i: 15200 , cost :  0.418787 , training accuracy :  0.821549\n",
      "i: 15300 , cost :  0.411765 , training accuracy :  0.833894\n",
      "i: 15400 , cost :  0.463011 , training accuracy :  0.817059\n",
      "i: 15500 , cost :  0.461854 , training accuracy :  0.815937\n",
      "i: 15600 , cost :  0.402407 , training accuracy :  0.830527\n",
      "i: 15700 , cost :  0.43014 , training accuracy :  0.817059\n",
      "i: 15800 , cost :  0.455232 , training accuracy :  0.799102\n",
      "i: 15900 , cost :  0.411471 , training accuracy :  0.824916\n",
      "i: 16000 , cost :  0.417004 , training accuracy :  0.82716\n",
      "i: 16100 , cost :  0.472891 , training accuracy :  0.811448\n",
      "i: 16200 , cost :  0.440153 , training accuracy :  0.819304\n",
      "i: 16300 , cost :  0.404311 , training accuracy :  0.821549\n",
      "i: 16400 , cost :  0.439807 , training accuracy :  0.817059\n",
      "i: 16500 , cost :  0.446624 , training accuracy :  0.805836\n",
      "i: 16600 , cost :  0.40192 , training accuracy :  0.828283\n",
      "i: 16700 , cost :  0.433677 , training accuracy :  0.823793\n",
      "i: 16800 , cost :  0.480717 , training accuracy :  0.809203\n",
      "i: 16900 , cost :  0.412439 , training accuracy :  0.836139\n",
      "i: 17000 , cost :  0.414635 , training accuracy :  0.821549\n",
      "i: 17100 , cost :  0.451882 , training accuracy :  0.802469\n",
      "i: 17200 , cost :  0.425374 , training accuracy :  0.817059\n",
      "i: 17300 , cost :  0.408738 , training accuracy :  0.832772\n",
      "i: 17400 , cost :  0.462425 , training accuracy :  0.813693\n",
      "i: 17500 , cost :  0.441284 , training accuracy :  0.820426\n",
      "i: 17600 , cost :  0.404181 , training accuracy :  0.821549\n",
      "i: 17700 , cost :  0.441102 , training accuracy :  0.814815\n",
      "i: 17800 , cost :  0.439795 , training accuracy :  0.810326\n",
      "i: 17900 , cost :  0.401358 , training accuracy :  0.829405\n",
      "i: 18000 , cost :  0.448113 , training accuracy :  0.815937\n",
      "i: 18100 , cost :  0.466497 , training accuracy :  0.814815\n",
      "i: 18200 , cost :  0.400358 , training accuracy :  0.829405\n",
      "i: 18300 , cost :  0.430035 , training accuracy :  0.818182\n",
      "i: 18400 , cost :  0.450761 , training accuracy :  0.801347\n",
      "i: 18500 , cost :  0.401536 , training accuracy :  0.821549\n",
      "i: 18600 , cost :  0.432928 , training accuracy :  0.823793\n",
      "i: 18700 , cost :  0.475157 , training accuracy :  0.81257\n",
      "i: 18800 , cost :  0.403681 , training accuracy :  0.82716\n",
      "i: 18900 , cost :  0.421179 , training accuracy :  0.826038\n",
      "i: 19000 , cost :  0.451984 , training accuracy :  0.802469\n",
      "i: 19100 , cost :  0.402752 , training accuracy :  0.823793\n",
      "i: 19200 , cost :  0.429774 , training accuracy :  0.826038\n",
      "i: 19300 , cost :  0.474391 , training accuracy :  0.813693\n",
      "i: 19400 , cost :  0.401033 , training accuracy :  0.82716\n",
      "i: 19500 , cost :  0.426568 , training accuracy :  0.821549\n",
      "i: 19600 , cost :  0.449238 , training accuracy :  0.803591\n",
      "i: 19700 , cost :  0.399426 , training accuracy :  0.828283\n",
      "i: 19800 , cost :  0.43909 , training accuracy :  0.819304\n",
      "i: 19900 , cost :  0.465723 , training accuracy :  0.814815\n",
      "i: 20000 , cost :  0.399047 , training accuracy :  0.828283\n",
      "i: 20100 , cost :  0.434174 , training accuracy :  0.818182\n",
      "i: 20200 , cost :  0.441616 , training accuracy :  0.811448\n",
      "i: 20300 , cost :  0.399313 , training accuracy :  0.832772\n",
      "i: 20400 , cost :  0.445839 , training accuracy :  0.814815\n",
      "i: 20500 , cost :  0.454037 , training accuracy :  0.817059\n",
      "i: 20600 , cost :  0.400188 , training accuracy :  0.821549\n",
      "i: 20700 , cost :  0.437959 , training accuracy :  0.814815\n",
      "i: 20800 , cost :  0.434975 , training accuracy :  0.81257\n",
      "i: 20900 , cost :  0.401593 , training accuracy :  0.832772\n",
      "i: 21000 , cost :  0.456864 , training accuracy :  0.815937\n",
      "i: 21100 , cost :  0.436702 , training accuracy :  0.821549\n",
      "i: 21200 , cost :  0.40325 , training accuracy :  0.829405\n",
      "i: 21300 , cost :  0.442548 , training accuracy :  0.808081\n",
      "i: 21400 , cost :  0.424057 , training accuracy :  0.818182\n",
      "i: 21500 , cost :  0.408718 , training accuracy :  0.836139\n",
      "i: 21600 , cost :  0.469166 , training accuracy :  0.81257\n",
      "i: 21700 , cost :  0.40854 , training accuracy :  0.836139\n",
      "i: 21800 , cost :  0.419206 , training accuracy :  0.822671\n",
      "i: 21900 , cost :  0.449302 , training accuracy :  0.802469\n",
      "i: 22000 , cost :  0.397773 , training accuracy :  0.828283\n",
      "i: 22100 , cost :  0.439155 , training accuracy :  0.817059\n",
      "i: 22200 , cost :  0.449618 , training accuracy :  0.818182\n",
      "i: 22300 , cost :  0.400717 , training accuracy :  0.822671\n",
      "i: 22400 , cost :  0.441093 , training accuracy :  0.808081\n",
      "i: 22500 , cost :  0.418045 , training accuracy :  0.819304\n",
      "i: 22600 , cost :  0.414043 , training accuracy :  0.829405\n",
      "i: 22700 , cost :  0.471973 , training accuracy :  0.810326\n",
      "i: 22800 , cost :  0.402037 , training accuracy :  0.83165\n",
      "i: 22900 , cost :  0.422669 , training accuracy :  0.822671\n",
      "i: 23000 , cost :  0.445901 , training accuracy :  0.805836\n",
      "i: 23100 , cost :  0.39673 , training accuracy :  0.83165\n",
      "i: 23200 , cost :  0.445574 , training accuracy :  0.814815\n",
      "i: 23300 , cost :  0.450886 , training accuracy :  0.818182\n",
      "i: 23400 , cost :  0.399388 , training accuracy :  0.821549\n",
      "i: 23500 , cost :  0.440391 , training accuracy :  0.809203\n",
      "i: 23600 , cost :  0.419775 , training accuracy :  0.818182\n",
      "i: 23700 , cost :  0.409445 , training accuracy :  0.832772\n",
      "i: 23800 , cost :  0.467576 , training accuracy :  0.81257\n",
      "i: 23900 , cost :  0.406012 , training accuracy :  0.835017\n",
      "i: 24000 , cost :  0.417249 , training accuracy :  0.823793\n",
      "i: 24100 , cost :  0.445214 , training accuracy :  0.804714\n",
      "i: 24200 , cost :  0.397227 , training accuracy :  0.824916\n",
      "i: 24300 , cost :  0.435701 , training accuracy :  0.819304\n",
      "i: 24400 , cost :  0.458915 , training accuracy :  0.814815\n",
      "i: 24500 , cost :  0.396303 , training accuracy :  0.824916\n",
      "i: 24600 , cost :  0.432773 , training accuracy :  0.818182\n",
      "i: 24700 , cost :  0.431707 , training accuracy :  0.813693\n",
      "i: 24800 , cost :  0.399097 , training accuracy :  0.833894\n",
      "i: 24900 , cost :  0.453023 , training accuracy :  0.814815\n",
      "i: 25000 , cost :  0.431382 , training accuracy :  0.822671\n",
      "i: 25100 , cost :  0.401245 , training accuracy :  0.829405\n",
      "i: 25200 , cost :  0.440112 , training accuracy :  0.809203\n",
      "i: 25300 , cost :  0.420677 , training accuracy :  0.819304\n",
      "i: 25400 , cost :  0.403523 , training accuracy :  0.838384\n",
      "i: 25500 , cost :  0.46118 , training accuracy :  0.814815\n",
      "i: 25600 , cost :  0.417212 , training accuracy :  0.83165\n",
      "i: 25700 , cost :  0.405755 , training accuracy :  0.82716\n",
      "i: 25800 , cost :  0.443027 , training accuracy :  0.805836\n",
      "i: 25900 , cost :  0.41103 , training accuracy :  0.823793\n",
      "i: 26000 , cost :  0.41 , training accuracy :  0.832772\n",
      "i: 26100 , cost :  0.464349 , training accuracy :  0.813693\n",
      "i: 26200 , cost :  0.411999 , training accuracy :  0.832772\n",
      "i: 26300 , cost :  0.408523 , training accuracy :  0.823793\n",
      "i: 26400 , cost :  0.444567 , training accuracy :  0.804714\n",
      "i: 26500 , cost :  0.406948 , training accuracy :  0.823793\n",
      "i: 26600 , cost :  0.412114 , training accuracy :  0.829405\n",
      "i: 26700 , cost :  0.465671 , training accuracy :  0.814815\n",
      "i: 26800 , cost :  0.407303 , training accuracy :  0.833894\n",
      "i: 26900 , cost :  0.408852 , training accuracy :  0.824916\n",
      "i: 27000 , cost :  0.444681 , training accuracy :  0.804714\n",
      "i: 27100 , cost :  0.40457 , training accuracy :  0.823793\n",
      "i: 27200 , cost :  0.412383 , training accuracy :  0.830527\n",
      "i: 27300 , cost :  0.465944 , training accuracy :  0.814815\n",
      "i: 27400 , cost :  0.403636 , training accuracy :  0.836139\n",
      "i: 27500 , cost :  0.411343 , training accuracy :  0.823793\n",
      "i: 27600 , cost :  0.443656 , training accuracy :  0.804714\n",
      "i: 27700 , cost :  0.401434 , training accuracy :  0.829405\n",
      "i: 27800 , cost :  0.416759 , training accuracy :  0.830527\n",
      "i: 27900 , cost :  0.467753 , training accuracy :  0.81257\n",
      "i: 28000 , cost :  0.400484 , training accuracy :  0.835017\n",
      "i: 28100 , cost :  0.413204 , training accuracy :  0.824916\n",
      "i: 28200 , cost :  0.444532 , training accuracy :  0.804714\n",
      "i: 28300 , cost :  0.399855 , training accuracy :  0.830527\n",
      "i: 28400 , cost :  0.416938 , training accuracy :  0.83165\n",
      "i: 28500 , cost :  0.464124 , training accuracy :  0.817059\n",
      "i: 28600 , cost :  0.40512 , training accuracy :  0.839506\n",
      "i: 28700 , cost :  0.408592 , training accuracy :  0.824916\n",
      "i: 28800 , cost :  0.442189 , training accuracy :  0.805836\n",
      "i: 28900 , cost :  0.410922 , training accuracy :  0.822671\n",
      "i: 29000 , cost :  0.405344 , training accuracy :  0.839506\n",
      "i: 29100 , cost :  0.456349 , training accuracy :  0.814815\n",
      "i: 29200 , cost :  0.426912 , training accuracy :  0.820426\n",
      "i: 29300 , cost :  0.397946 , training accuracy :  0.830527\n",
      "i: 29400 , cost :  0.432623 , training accuracy :  0.818182\n",
      "i: 29500 , cost :  0.429946 , training accuracy :  0.81257\n",
      "i: 29600 , cost :  0.393554 , training accuracy :  0.832772\n",
      "i: 29700 , cost :  0.437496 , training accuracy :  0.819304\n",
      "i: 29800 , cost :  0.458064 , training accuracy :  0.818182\n",
      "i: 29900 , cost :  0.393154 , training accuracy :  0.835017\n",
      "Final training accuracy :  0.822671\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 30000 ][hidden: 15 ][file: bhavul_tr_acc_0.82_prediction.csv ] ACCURACY :  0.822671\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1157.04 , training accuracy :  0.337823\n",
      "i: 100 , cost :  28.5897 , training accuracy :  0.57688\n",
      "i: 200 , cost :  4.27185 , training accuracy :  0.685746\n",
      "i: 300 , cost :  1.70098 , training accuracy :  0.772166\n",
      "i: 400 , cost :  1.35702 , training accuracy :  0.789001\n",
      "i: 500 , cost :  1.15096 , training accuracy :  0.809203\n",
      "i: 600 , cost :  0.930117 , training accuracy :  0.803591\n",
      "i: 700 , cost :  1.818 , training accuracy :  0.754209\n",
      "i: 800 , cost :  0.773842 , training accuracy :  0.817059\n",
      "i: 900 , cost :  2.2469 , training accuracy :  0.750842\n",
      "i: 1000 , cost :  0.771432 , training accuracy :  0.823793\n",
      "i: 1100 , cost :  0.656171 , training accuracy :  0.822671\n",
      "i: 1200 , cost :  1.55417 , training accuracy :  0.762065\n",
      "i: 1300 , cost :  0.636078 , training accuracy :  0.823793\n",
      "i: 1400 , cost :  1.76435 , training accuracy :  0.760943\n",
      "i: 1500 , cost :  0.68527 , training accuracy :  0.828283\n",
      "i: 1600 , cost :  0.580616 , training accuracy :  0.820426\n",
      "i: 1700 , cost :  0.720125 , training accuracy :  0.829405\n",
      "i: 1800 , cost :  0.557894 , training accuracy :  0.823793\n",
      "i: 1900 , cost :  2.35735 , training accuracy :  0.742985\n",
      "i: 2000 , cost :  1.30544 , training accuracy :  0.785634\n",
      "i: 2100 , cost :  0.577103 , training accuracy :  0.828283\n",
      "i: 2200 , cost :  0.578861 , training accuracy :  0.804714\n",
      "i: 2300 , cost :  1.40813 , training accuracy :  0.62963\n",
      "i: 2400 , cost :  0.527244 , training accuracy :  0.842873\n",
      "i: 2500 , cost :  0.678961 , training accuracy :  0.809203\n",
      "i: 2600 , cost :  4.01131 , training accuracy :  0.450056\n",
      "i: 2700 , cost :  1.37603 , training accuracy :  0.700337\n",
      "i: 2800 , cost :  0.511268 , training accuracy :  0.842873\n",
      "i: 2900 , cost :  1.50152 , training accuracy :  0.783389\n",
      "i: 3000 , cost :  0.589182 , training accuracy :  0.837261\n",
      "i: 3100 , cost :  0.473723 , training accuracy :  0.845118\n",
      "i: 3200 , cost :  0.77665 , training accuracy :  0.824916\n",
      "i: 3300 , cost :  0.489356 , training accuracy :  0.848485\n",
      "i: 3400 , cost :  2.01621 , training accuracy :  0.567901\n",
      "i: 3500 , cost :  1.28028 , training accuracy :  0.709315\n",
      "i: 3600 , cost :  0.487255 , training accuracy :  0.849607\n",
      "i: 3700 , cost :  1.14885 , training accuracy :  0.680135\n",
      "i: 3800 , cost :  0.546592 , training accuracy :  0.856341\n",
      "i: 3900 , cost :  0.441905 , training accuracy :  0.847363\n",
      "i: 4000 , cost :  0.963425 , training accuracy :  0.811448\n",
      "i: 4100 , cost :  0.442677 , training accuracy :  0.849607\n",
      "i: 4200 , cost :  3.81033 , training accuracy :  0.463524\n",
      "i: 4300 , cost :  0.480058 , training accuracy :  0.85073\n",
      "i: 4400 , cost :  6.89986 , training accuracy :  0.42312\n",
      "i: 4500 , cost :  0.703555 , training accuracy :  0.832772\n",
      "i: 4600 , cost :  0.45108 , training accuracy :  0.849607\n",
      "i: 4700 , cost :  2.42842 , training accuracy :  0.74523\n",
      "i: 4800 , cost :  0.467774 , training accuracy :  0.849607\n",
      "i: 4900 , cost :  0.826411 , training accuracy :  0.802469\n",
      "i: 5000 , cost :  1.64891 , training accuracy :  0.785634\n",
      "i: 5100 , cost :  0.501111 , training accuracy :  0.855219\n",
      "i: 5200 , cost :  0.4139 , training accuracy :  0.856341\n",
      "i: 5300 , cost :  4.49194 , training accuracy :  0.457912\n",
      "i: 5400 , cost :  0.55931 , training accuracy :  0.861953\n",
      "i: 5500 , cost :  0.411278 , training accuracy :  0.859708\n",
      "i: 5600 , cost :  1.91992 , training accuracy :  0.773288\n",
      "i: 5700 , cost :  0.452297 , training accuracy :  0.859708\n",
      "i: 5800 , cost :  0.381386 , training accuracy :  0.861953\n",
      "i: 5900 , cost :  1.21613 , training accuracy :  0.813693\n",
      "i: 6000 , cost :  0.441995 , training accuracy :  0.864198\n",
      "i: 6100 , cost :  0.380463 , training accuracy :  0.86532\n",
      "i: 6200 , cost :  2.18506 , training accuracy :  0.766554\n",
      "i: 6300 , cost :  0.43916 , training accuracy :  0.863075\n",
      "i: 6400 , cost :  0.378305 , training accuracy :  0.869809\n",
      "i: 6500 , cost :  2.2085 , training accuracy :  0.766554\n",
      "i: 6600 , cost :  0.442309 , training accuracy :  0.860831\n",
      "i: 6700 , cost :  0.377822 , training accuracy :  0.870932\n",
      "i: 6800 , cost :  0.884803 , training accuracy :  0.824916\n",
      "i: 6900 , cost :  0.454496 , training accuracy :  0.870932\n",
      "i: 7000 , cost :  0.379874 , training accuracy :  0.873176\n",
      "i: 7100 , cost :  1.45801 , training accuracy :  0.618406\n",
      "i: 7200 , cost :  0.643799 , training accuracy :  0.82716\n",
      "i: 7300 , cost :  0.432134 , training accuracy :  0.866442\n",
      "i: 7400 , cost :  0.372794 , training accuracy :  0.872054\n",
      "i: 7500 , cost :  6.01625 , training accuracy :  0.409652\n",
      "i: 7600 , cost :  0.507771 , training accuracy :  0.866442\n",
      "i: 7700 , cost :  0.400009 , training accuracy :  0.868687\n",
      "i: 7800 , cost :  0.358901 , training accuracy :  0.867565\n",
      "i: 7900 , cost :  0.90751 , training accuracy :  0.811448\n",
      "i: 8000 , cost :  0.455869 , training accuracy :  0.876543\n",
      "i: 8100 , cost :  0.376878 , training accuracy :  0.872054\n",
      "i: 8200 , cost :  3.1271 , training accuracy :  0.523008\n",
      "i: 8300 , cost :  3.02438 , training accuracy :  0.494949\n",
      "i: 8400 , cost :  0.409814 , training accuracy :  0.870932\n",
      "i: 8500 , cost :  0.360369 , training accuracy :  0.872054\n",
      "i: 8600 , cost :  0.47998 , training accuracy :  0.85073\n",
      "i: 8700 , cost :  0.385474 , training accuracy :  0.872054\n",
      "i: 8800 , cost :  0.362156 , training accuracy :  0.868687\n",
      "i: 8900 , cost :  1.30365 , training accuracy :  0.685746\n",
      "i: 9000 , cost :  0.418524 , training accuracy :  0.873176\n",
      "i: 9100 , cost :  0.359748 , training accuracy :  0.874299\n",
      "i: 9200 , cost :  1.08605 , training accuracy :  0.826038\n",
      "i: 9300 , cost :  0.70675 , training accuracy :  0.826038\n",
      "i: 9400 , cost :  0.394017 , training accuracy :  0.869809\n",
      "i: 9500 , cost :  0.351043 , training accuracy :  0.868687\n",
      "i: 9600 , cost :  5.58208 , training accuracy :  0.419753\n",
      "i: 9700 , cost :  0.527918 , training accuracy :  0.857464\n",
      "i: 9800 , cost :  0.389588 , training accuracy :  0.870932\n",
      "i: 9900 , cost :  0.348005 , training accuracy :  0.869809\n",
      "i: 10000 , cost :  0.402971 , training accuracy :  0.852974\n",
      "i: 10100 , cost :  0.784562 , training accuracy :  0.830527\n",
      "i: 10200 , cost :  1.15342 , training accuracy :  0.823793\n",
      "i: 10300 , cost :  0.457715 , training accuracy :  0.870932\n",
      "i: 10400 , cost :  0.372582 , training accuracy :  0.873176\n",
      "i: 10500 , cost :  0.5393 , training accuracy :  0.841751\n",
      "i: 10600 , cost :  0.509658 , training accuracy :  0.858586\n",
      "i: 10700 , cost :  0.412403 , training accuracy :  0.870932\n",
      "i: 10800 , cost :  0.354833 , training accuracy :  0.873176\n",
      "i: 10900 , cost :  0.890833 , training accuracy :  0.828283\n",
      "i: 11000 , cost :  0.523863 , training accuracy :  0.860831\n",
      "i: 11100 , cost :  0.379406 , training accuracy :  0.875421\n",
      "i: 11200 , cost :  0.343596 , training accuracy :  0.869809\n",
      "i: 11300 , cost :  1.27975 , training accuracy :  0.820426\n",
      "i: 11400 , cost :  0.41387 , training accuracy :  0.874299\n",
      "i: 11500 , cost :  0.352973 , training accuracy :  0.874299\n",
      "i: 11600 , cost :  0.367092 , training accuracy :  0.868687\n",
      "i: 11700 , cost :  0.367697 , training accuracy :  0.867565\n",
      "i: 11800 , cost :  1.9134 , training accuracy :  0.621773\n",
      "i: 11900 , cost :  0.437598 , training accuracy :  0.874299\n",
      "i: 12000 , cost :  0.359899 , training accuracy :  0.877666\n",
      "i: 12100 , cost :  3.02071 , training accuracy :  0.753086\n",
      "i: 12200 , cost :  1.34778 , training accuracy :  0.802469\n",
      "i: 12300 , cost :  0.449345 , training accuracy :  0.870932\n",
      "i: 12400 , cost :  0.358574 , training accuracy :  0.877666\n",
      "i: 12500 , cost :  2.07629 , training accuracy :  0.777778\n",
      "i: 12600 , cost :  0.613047 , training accuracy :  0.833894\n",
      "i: 12700 , cost :  0.390736 , training accuracy :  0.874299\n",
      "i: 12800 , cost :  0.344532 , training accuracy :  0.870932\n",
      "i: 12900 , cost :  1.83825 , training accuracy :  0.768799\n",
      "i: 13000 , cost :  0.447023 , training accuracy :  0.872054\n",
      "i: 13100 , cost :  0.359256 , training accuracy :  0.878788\n",
      "i: 13200 , cost :  0.428692 , training accuracy :  0.839506\n",
      "i: 13300 , cost :  1.60989 , training accuracy :  0.799102\n",
      "i: 13400 , cost :  0.408875 , training accuracy :  0.875421\n",
      "i: 13500 , cost :  0.349194 , training accuracy :  0.873176\n",
      "i: 13600 , cost :  0.872867 , training accuracy :  0.82716\n",
      "i: 13700 , cost :  0.545999 , training accuracy :  0.868687\n",
      "i: 13800 , cost :  0.373887 , training accuracy :  0.877666\n",
      "i: 13900 , cost :  0.338762 , training accuracy :  0.870932\n",
      "i: 14000 , cost :  2.63615 , training accuracy :  0.553311\n",
      "i: 14100 , cost :  0.825553 , training accuracy :  0.79798\n",
      "i: 14200 , cost :  0.429161 , training accuracy :  0.873176\n",
      "i: 14300 , cost :  0.355687 , training accuracy :  0.877666\n",
      "i: 14400 , cost :  7.03599 , training accuracy :  0.407407\n",
      "i: 14500 , cost :  4.00536 , training accuracy :  0.45679\n",
      "i: 14600 , cost :  0.424813 , training accuracy :  0.876543\n",
      "i: 14700 , cost :  0.353105 , training accuracy :  0.877666\n",
      "i: 14800 , cost :  2.41156 , training accuracy :  0.766554\n",
      "i: 14900 , cost :  0.97033 , training accuracy :  0.829405\n",
      "i: 15000 , cost :  0.397458 , training accuracy :  0.87991\n",
      "i: 15100 , cost :  0.345223 , training accuracy :  0.875421\n",
      "i: 15200 , cost :  0.538223 , training accuracy :  0.851852\n",
      "i: 15300 , cost :  0.524655 , training accuracy :  0.874299\n",
      "i: 15400 , cost :  0.468109 , training accuracy :  0.877666\n",
      "i: 15500 , cost :  0.370645 , training accuracy :  0.878788\n",
      "i: 15600 , cost :  0.337018 , training accuracy :  0.870932\n",
      "i: 15700 , cost :  1.00789 , training accuracy :  0.796857\n",
      "i: 15800 , cost :  0.752276 , training accuracy :  0.85073\n",
      "i: 15900 , cost :  0.409046 , training accuracy :  0.877666\n",
      "i: 16000 , cost :  0.345015 , training accuracy :  0.877666\n",
      "i: 16100 , cost :  2.92689 , training accuracy :  0.753086\n",
      "i: 16200 , cost :  1.21993 , training accuracy :  0.829405\n",
      "i: 16300 , cost :  0.383383 , training accuracy :  0.882155\n",
      "i: 16400 , cost :  0.337865 , training accuracy :  0.878788\n",
      "i: 16500 , cost :  2.37531 , training accuracy :  0.757576\n",
      "i: 16600 , cost :  0.708433 , training accuracy :  0.849607\n",
      "i: 16700 , cost :  0.376616 , training accuracy :  0.882155\n",
      "i: 16800 , cost :  0.334877 , training accuracy :  0.877666\n",
      "i: 16900 , cost :  0.888848 , training accuracy :  0.830527\n",
      "i: 17000 , cost :  0.518845 , training accuracy :  0.876543\n",
      "i: 17100 , cost :  0.417379 , training accuracy :  0.881033\n",
      "i: 17200 , cost :  0.347563 , training accuracy :  0.878788\n",
      "i: 17300 , cost :  2.9007 , training accuracy :  0.757576\n",
      "i: 17400 , cost :  1.09398 , training accuracy :  0.829405\n",
      "i: 17500 , cost :  0.449452 , training accuracy :  0.876543\n",
      "i: 17600 , cost :  0.360373 , training accuracy :  0.87991\n",
      "i: 17700 , cost :  0.330397 , training accuracy :  0.875421\n",
      "i: 17800 , cost :  0.84577 , training accuracy :  0.84624\n",
      "i: 17900 , cost :  0.510874 , training accuracy :  0.86532\n",
      "i: 18000 , cost :  0.371174 , training accuracy :  0.87991\n",
      "i: 18100 , cost :  0.333589 , training accuracy :  0.877666\n",
      "i: 18200 , cost :  3.2122 , training accuracy :  0.497194\n",
      "i: 18300 , cost :  1.06526 , training accuracy :  0.738496\n",
      "i: 18400 , cost :  0.418894 , training accuracy :  0.87991\n",
      "i: 18500 , cost :  0.348635 , training accuracy :  0.87991\n",
      "i: 18600 , cost :  1.18106 , training accuracy :  0.784512\n",
      "i: 18700 , cost :  1.77125 , training accuracy :  0.789001\n",
      "i: 18800 , cost :  0.413106 , training accuracy :  0.881033\n",
      "i: 18900 , cost :  0.346035 , training accuracy :  0.87991\n",
      "i: 19000 , cost :  2.82923 , training accuracy :  0.753086\n",
      "i: 19100 , cost :  1.72428 , training accuracy :  0.796857\n",
      "i: 19200 , cost :  1.07096 , training accuracy :  0.829405\n",
      "i: 19300 , cost :  0.419001 , training accuracy :  0.881033\n",
      "i: 19400 , cost :  0.349754 , training accuracy :  0.878788\n",
      "i: 19500 , cost :  0.509088 , training accuracy :  0.843996\n",
      "i: 19600 , cost :  0.759681 , training accuracy :  0.849607\n",
      "i: 19700 , cost :  0.685478 , training accuracy :  0.860831\n",
      "i: 19800 , cost :  0.408828 , training accuracy :  0.87991\n",
      "i: 19900 , cost :  0.345982 , training accuracy :  0.87991\n",
      "i: 20000 , cost :  0.512019 , training accuracy :  0.804714\n",
      "i: 20100 , cost :  0.613577 , training accuracy :  0.864198\n",
      "i: 20200 , cost :  0.440423 , training accuracy :  0.878788\n",
      "i: 20300 , cost :  0.356865 , training accuracy :  0.87991\n",
      "i: 20400 , cost :  0.329418 , training accuracy :  0.877666\n",
      "i: 20500 , cost :  1.87004 , training accuracy :  0.781145\n",
      "i: 20600 , cost :  0.483954 , training accuracy :  0.872054\n",
      "i: 20700 , cost :  0.366873 , training accuracy :  0.878788\n",
      "i: 20800 , cost :  0.331677 , training accuracy :  0.878788\n",
      "i: 20900 , cost :  0.608649 , training accuracy :  0.858586\n",
      "i: 21000 , cost :  1.18676 , training accuracy :  0.829405\n",
      "i: 21100 , cost :  0.384376 , training accuracy :  0.878788\n",
      "i: 21200 , cost :  0.336173 , training accuracy :  0.878788\n",
      "i: 21300 , cost :  0.328192 , training accuracy :  0.875421\n",
      "i: 21400 , cost :  3.5028 , training accuracy :  0.510662\n",
      "i: 21500 , cost :  1.15647 , training accuracy :  0.687991\n",
      "i: 21600 , cost :  0.392965 , training accuracy :  0.87991\n",
      "i: 21700 , cost :  0.336879 , training accuracy :  0.87991\n",
      "i: 21800 , cost :  1.58305 , training accuracy :  0.799102\n",
      "i: 21900 , cost :  0.8649 , training accuracy :  0.845118\n",
      "i: 22000 , cost :  0.439591 , training accuracy :  0.881033\n",
      "i: 22100 , cost :  0.353336 , training accuracy :  0.878788\n",
      "i: 22200 , cost :  0.326253 , training accuracy :  0.876543\n",
      "i: 22300 , cost :  2.56215 , training accuracy :  0.561167\n",
      "i: 22400 , cost :  0.634082 , training accuracy :  0.835017\n",
      "i: 22500 , cost :  0.372234 , training accuracy :  0.881033\n",
      "i: 22600 , cost :  0.331447 , training accuracy :  0.877666\n",
      "i: 22700 , cost :  3.85167 , training accuracy :  0.75982\n",
      "i: 22800 , cost :  0.56821 , training accuracy :  0.872054\n",
      "i: 22900 , cost :  0.785987 , training accuracy :  0.795735\n",
      "i: 23000 , cost :  0.396129 , training accuracy :  0.881033\n",
      "i: 23100 , cost :  0.339224 , training accuracy :  0.882155\n",
      "i: 23200 , cost :  0.664217 , training accuracy :  0.826038\n",
      "i: 23300 , cost :  2.10009 , training accuracy :  0.776655\n",
      "i: 23400 , cost :  0.441524 , training accuracy :  0.878788\n",
      "i: 23500 , cost :  0.351714 , training accuracy :  0.87991\n",
      "i: 23600 , cost :  0.325611 , training accuracy :  0.882155\n",
      "i: 23700 , cost :  2.08891 , training accuracy :  0.7789\n",
      "i: 23800 , cost :  0.458071 , training accuracy :  0.876543\n",
      "i: 23900 , cost :  0.35625 , training accuracy :  0.878788\n",
      "i: 24000 , cost :  0.325619 , training accuracy :  0.875421\n",
      "i: 24100 , cost :  0.769861 , training accuracy :  0.852974\n",
      "i: 24200 , cost :  0.544331 , training accuracy :  0.873176\n",
      "i: 24300 , cost :  0.372216 , training accuracy :  0.878788\n",
      "i: 24400 , cost :  0.330245 , training accuracy :  0.878788\n",
      "i: 24500 , cost :  2.74543 , training accuracy :  0.736251\n",
      "i: 24600 , cost :  1.21284 , training accuracy :  0.83165\n",
      "i: 24700 , cost :  0.494049 , training accuracy :  0.874299\n",
      "i: 24800 , cost :  0.368788 , training accuracy :  0.878788\n",
      "i: 24900 , cost :  0.32898 , training accuracy :  0.878788\n",
      "i: 25000 , cost :  0.549398 , training accuracy :  0.835017\n",
      "i: 25100 , cost :  0.477808 , training accuracy :  0.85073\n",
      "i: 25200 , cost :  1.96694 , training accuracy :  0.586981\n",
      "i: 25300 , cost :  0.549024 , training accuracy :  0.856341\n",
      "i: 25400 , cost :  0.413407 , training accuracy :  0.877666\n",
      "i: 25500 , cost :  0.340242 , training accuracy :  0.87991\n",
      "i: 25600 , cost :  2.79582 , training accuracy :  0.746352\n",
      "i: 25700 , cost :  0.748922 , training accuracy :  0.858586\n",
      "i: 25800 , cost :  0.447417 , training accuracy :  0.877666\n",
      "i: 25900 , cost :  0.35006 , training accuracy :  0.877666\n",
      "i: 26000 , cost :  0.322718 , training accuracy :  0.877666\n",
      "i: 26100 , cost :  0.61802 , training accuracy :  0.829405\n",
      "i: 26200 , cost :  2.55163 , training accuracy :  0.762065\n",
      "i: 26300 , cost :  0.503288 , training accuracy :  0.873176\n",
      "i: 26400 , cost :  0.364018 , training accuracy :  0.876543\n",
      "i: 26500 , cost :  0.324074 , training accuracy :  0.87991\n",
      "i: 26600 , cost :  2.14951 , training accuracy :  0.750842\n",
      "i: 26700 , cost :  0.738084 , training accuracy :  0.861953\n",
      "i: 26800 , cost :  0.451693 , training accuracy :  0.878788\n",
      "i: 26900 , cost :  0.354005 , training accuracy :  0.877666\n",
      "i: 27000 , cost :  0.322406 , training accuracy :  0.878788\n",
      "i: 27100 , cost :  0.54733 , training accuracy :  0.861953\n",
      "i: 27200 , cost :  0.503969 , training accuracy :  0.877666\n",
      "i: 27300 , cost :  0.360358 , training accuracy :  0.877666\n",
      "i: 27400 , cost :  0.324799 , training accuracy :  0.877666\n",
      "i: 27500 , cost :  0.31983 , training accuracy :  0.878788\n",
      "i: 27600 , cost :  1.82543 , training accuracy :  0.795735\n",
      "i: 27700 , cost :  0.807752 , training accuracy :  0.84624\n",
      "i: 27800 , cost :  0.364702 , training accuracy :  0.877666\n",
      "i: 27900 , cost :  0.324708 , training accuracy :  0.881033\n",
      "i: 28000 , cost :  3.97139 , training accuracy :  0.47587\n",
      "i: 28100 , cost :  2.2791 , training accuracy :  0.534231\n",
      "i: 28200 , cost :  0.472816 , training accuracy :  0.874299\n",
      "i: 28300 , cost :  0.354648 , training accuracy :  0.878788\n",
      "i: 28400 , cost :  0.322318 , training accuracy :  0.876543\n",
      "i: 28500 , cost :  3.30173 , training accuracy :  0.526375\n",
      "i: 28600 , cost :  0.99363 , training accuracy :  0.727273\n",
      "i: 28700 , cost :  0.385492 , training accuracy :  0.878788\n",
      "i: 28800 , cost :  0.331471 , training accuracy :  0.882155\n",
      "i: 28900 , cost :  0.670272 , training accuracy :  0.813693\n",
      "i: 29000 , cost :  0.837735 , training accuracy :  0.845118\n",
      "i: 29100 , cost :  0.456 , training accuracy :  0.882155\n",
      "i: 29200 , cost :  0.38346 , training accuracy :  0.878788\n",
      "i: 29300 , cost :  0.331012 , training accuracy :  0.882155\n",
      "i: 29400 , cost :  0.318159 , training accuracy :  0.877666\n",
      "i: 29500 , cost :  0.60489 , training accuracy :  0.842873\n",
      "i: 29600 , cost :  0.765399 , training accuracy :  0.854097\n",
      "i: 29700 , cost :  0.434135 , training accuracy :  0.876543\n",
      "i: 29800 , cost :  0.345398 , training accuracy :  0.87991\n",
      "i: 29900 , cost :  0.318929 , training accuracy :  0.882155\n",
      "Final training accuracy :  0.856341\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 30000 ][hidden: 50 ][file: bhavul_tr_acc_0.86_prediction.csv ] ACCURACY :  0.856341\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  682.982 , training accuracy :  0.622896\n",
      "i: 100 , cost :  9.26491 , training accuracy :  0.685746\n",
      "i: 200 , cost :  3.5821 , training accuracy :  0.74523\n",
      "i: 300 , cost :  4.5328 , training accuracy :  0.628507\n",
      "i: 400 , cost :  3.52972 , training accuracy :  0.698092\n",
      "i: 500 , cost :  1.91154 , training accuracy :  0.780022\n",
      "i: 600 , cost :  1.71017 , training accuracy :  0.800224\n",
      "i: 700 , cost :  1.61571 , training accuracy :  0.805836\n",
      "i: 800 , cost :  2.40523 , training accuracy :  0.790123\n",
      "i: 900 , cost :  1.32 , training accuracy :  0.791246\n",
      "i: 1000 , cost :  1.89629 , training accuracy :  0.790123\n",
      "i: 1100 , cost :  1.91899 , training accuracy :  0.802469\n",
      "i: 1200 , cost :  1.15104 , training accuracy :  0.785634\n",
      "i: 1300 , cost :  1.64093 , training accuracy :  0.813693\n",
      "i: 1400 , cost :  1.41875 , training accuracy :  0.741863\n",
      "i: 1500 , cost :  1.03699 , training accuracy :  0.83165\n",
      "i: 1600 , cost :  2.83515 , training accuracy :  0.742985\n",
      "i: 1700 , cost :  0.906213 , training accuracy :  0.838384\n",
      "i: 1800 , cost :  0.993768 , training accuracy :  0.830527\n",
      "i: 1900 , cost :  1.20855 , training accuracy :  0.813693\n",
      "i: 2000 , cost :  0.81721 , training accuracy :  0.839506\n",
      "i: 2100 , cost :  1.00575 , training accuracy :  0.835017\n",
      "i: 2200 , cost :  1.53272 , training accuracy :  0.810326\n",
      "i: 2300 , cost :  0.97146 , training accuracy :  0.841751\n",
      "i: 2400 , cost :  2.8226 , training accuracy :  0.671156\n",
      "i: 2500 , cost :  8.06388 , training accuracy :  0.71156\n",
      "i: 2600 , cost :  0.709081 , training accuracy :  0.839506\n",
      "i: 2700 , cost :  0.85655 , training accuracy :  0.85073\n",
      "i: 2800 , cost :  4.15867 , training accuracy :  0.552189\n",
      "i: 2900 , cost :  0.7069 , training accuracy :  0.852974\n",
      "i: 3000 , cost :  0.757954 , training accuracy :  0.852974\n",
      "i: 3100 , cost :  2.79376 , training accuracy :  0.792368\n",
      "i: 3200 , cost :  1.83886 , training accuracy :  0.790123\n",
      "i: 3300 , cost :  1.22419 , training accuracy :  0.753086\n",
      "i: 3400 , cost :  0.754473 , training accuracy :  0.857464\n",
      "i: 3500 , cost :  0.940893 , training accuracy :  0.856341\n",
      "i: 3600 , cost :  9.08608 , training accuracy :  0.416386\n",
      "i: 3700 , cost :  1.18087 , training accuracy :  0.750842\n",
      "i: 3800 , cost :  0.829832 , training accuracy :  0.849607\n",
      "i: 3900 , cost :  2.31613 , training accuracy :  0.772166\n",
      "i: 4000 , cost :  4.82932 , training accuracy :  0.771044\n",
      "i: 4100 , cost :  0.875744 , training accuracy :  0.860831\n",
      "i: 4200 , cost :  3.19014 , training accuracy :  0.744108\n",
      "i: 4300 , cost :  2.35636 , training accuracy :  0.804714\n",
      "i: 4400 , cost :  0.657347 , training accuracy :  0.86532\n",
      "i: 4500 , cost :  1.50472 , training accuracy :  0.842873\n",
      "i: 4600 , cost :  0.646054 , training accuracy :  0.86532\n",
      "i: 4700 , cost :  1.27632 , training accuracy :  0.79798\n",
      "i: 4800 , cost :  0.765094 , training accuracy :  0.843996\n",
      "i: 4900 , cost :  0.591415 , training accuracy :  0.86532\n",
      "i: 5000 , cost :  0.72003 , training accuracy :  0.868687\n",
      "i: 5100 , cost :  2.23541 , training accuracy :  0.681257\n",
      "i: 5200 , cost :  1.48054 , training accuracy :  0.684624\n",
      "i: 5300 , cost :  0.936392 , training accuracy :  0.843996\n",
      "i: 5400 , cost :  4.12692 , training accuracy :  0.545455\n",
      "i: 5500 , cost :  1.06677 , training accuracy :  0.843996\n",
      "i: 5600 , cost :  2.34176 , training accuracy :  0.786756\n",
      "i: 5700 , cost :  1.48113 , training accuracy :  0.790123\n",
      "i: 5800 , cost :  0.579651 , training accuracy :  0.868687\n",
      "i: 5900 , cost :  1.09476 , training accuracy :  0.855219\n",
      "i: 6000 , cost :  0.575348 , training accuracy :  0.867565\n",
      "i: 6100 , cost :  0.561743 , training accuracy :  0.874299\n",
      "i: 6200 , cost :  0.770109 , training accuracy :  0.86532\n",
      "i: 6300 , cost :  3.2345 , training accuracy :  0.765432\n",
      "i: 6400 , cost :  1.23421 , training accuracy :  0.843996\n",
      "i: 6500 , cost :  0.647148 , training accuracy :  0.867565\n",
      "i: 6600 , cost :  1.15766 , training accuracy :  0.818182\n",
      "i: 6700 , cost :  0.595753 , training accuracy :  0.870932\n",
      "i: 6800 , cost :  1.07264 , training accuracy :  0.860831\n",
      "i: 6900 , cost :  0.717422 , training accuracy :  0.870932\n",
      "i: 7000 , cost :  1.42193 , training accuracy :  0.823793\n",
      "i: 7100 , cost :  1.05048 , training accuracy :  0.857464\n",
      "i: 7200 , cost :  0.571404 , training accuracy :  0.868687\n",
      "i: 7300 , cost :  1.79571 , training accuracy :  0.836139\n",
      "i: 7400 , cost :  0.585706 , training accuracy :  0.870932\n",
      "i: 7500 , cost :  0.760751 , training accuracy :  0.858586\n",
      "i: 7600 , cost :  3.40295 , training accuracy :  0.602694\n",
      "i: 7700 , cost :  2.64486 , training accuracy :  0.686869\n",
      "i: 7800 , cost :  0.653822 , training accuracy :  0.869809\n",
      "i: 7900 , cost :  2.98116 , training accuracy :  0.76431\n",
      "i: 8000 , cost :  0.807553 , training accuracy :  0.866442\n",
      "i: 8100 , cost :  0.522254 , training accuracy :  0.874299\n",
      "i: 8200 , cost :  1.82242 , training accuracy :  0.837261\n",
      "i: 8300 , cost :  0.68749 , training accuracy :  0.869809\n",
      "i: 8400 , cost :  2.12508 , training accuracy :  0.800224\n",
      "i: 8500 , cost :  0.990375 , training accuracy :  0.841751\n",
      "i: 8600 , cost :  0.525245 , training accuracy :  0.874299\n",
      "i: 8700 , cost :  0.894795 , training accuracy :  0.864198\n",
      "i: 8800 , cost :  0.496156 , training accuracy :  0.875421\n",
      "i: 8900 , cost :  2.86441 , training accuracy :  0.79798\n",
      "i: 9000 , cost :  0.598978 , training accuracy :  0.870932\n",
      "i: 9100 , cost :  1.05544 , training accuracy :  0.858586\n",
      "i: 9200 , cost :  0.669903 , training accuracy :  0.868687\n",
      "i: 9300 , cost :  6.52784 , training accuracy :  0.489338\n",
      "i: 9400 , cost :  0.514169 , training accuracy :  0.874299\n",
      "i: 9500 , cost :  3.94724 , training accuracy :  0.763187\n",
      "i: 9600 , cost :  0.57783 , training accuracy :  0.873176\n",
      "i: 9700 , cost :  1.27171 , training accuracy :  0.83165\n",
      "i: 9800 , cost :  0.492638 , training accuracy :  0.874299\n",
      "i: 9900 , cost :  1.34942 , training accuracy :  0.806958\n",
      "i: 10000 , cost :  2.23743 , training accuracy :  0.824916\n",
      "i: 10100 , cost :  0.548146 , training accuracy :  0.876543\n",
      "i: 10200 , cost :  7.09556 , training accuracy :  0.461279\n",
      "i: 10300 , cost :  0.739841 , training accuracy :  0.883277\n",
      "i: 10400 , cost :  0.619583 , training accuracy :  0.860831\n",
      "i: 10500 , cost :  0.906498 , training accuracy :  0.861953\n",
      "i: 10600 , cost :  0.585577 , training accuracy :  0.873176\n",
      "i: 10700 , cost :  1.40519 , training accuracy :  0.746352\n",
      "i: 10800 , cost :  0.509512 , training accuracy :  0.876543\n",
      "i: 10900 , cost :  4.04553 , training accuracy :  0.635241\n",
      "i: 11000 , cost :  0.617081 , training accuracy :  0.870932\n",
      "i: 11100 , cost :  5.7719 , training accuracy :  0.545455\n",
      "i: 11200 , cost :  0.81207 , training accuracy :  0.877666\n",
      "i: 11300 , cost :  0.485849 , training accuracy :  0.883277\n",
      "i: 11400 , cost :  0.810554 , training accuracy :  0.860831\n",
      "i: 11500 , cost :  6.25192 , training accuracy :  0.462402\n",
      "i: 11600 , cost :  4.09487 , training accuracy :  0.555556\n",
      "i: 11700 , cost :  3.83485 , training accuracy :  0.65881\n",
      "i: 11800 , cost :  0.576642 , training accuracy :  0.873176\n",
      "i: 11900 , cost :  0.990917 , training accuracy :  0.843996\n",
      "i: 12000 , cost :  0.740056 , training accuracy :  0.881033\n",
      "i: 12100 , cost :  0.812971 , training accuracy :  0.790123\n",
      "i: 12200 , cost :  4.20817 , training accuracy :  0.620651\n",
      "i: 12300 , cost :  0.605528 , training accuracy :  0.874299\n",
      "i: 12400 , cost :  1.99946 , training accuracy :  0.838384\n",
      "i: 12500 , cost :  0.688662 , training accuracy :  0.868687\n",
      "i: 12600 , cost :  4.39075 , training accuracy :  0.791246\n",
      "i: 12700 , cost :  5.15095 , training accuracy :  0.772166\n",
      "i: 12800 , cost :  0.748475 , training accuracy :  0.877666\n",
      "i: 12900 , cost :  8.21379 , training accuracy :  0.723906\n",
      "i: 13000 , cost :  0.745373 , training accuracy :  0.856341\n",
      "i: 13100 , cost :  1.21079 , training accuracy :  0.783389\n",
      "i: 13200 , cost :  0.496449 , training accuracy :  0.882155\n",
      "i: 13300 , cost :  1.14571 , training accuracy :  0.851852\n",
      "i: 13400 , cost :  0.469487 , training accuracy :  0.8844\n",
      "i: 13500 , cost :  0.513687 , training accuracy :  0.864198\n",
      "i: 13600 , cost :  2.4203 , training accuracy :  0.699214\n",
      "i: 13700 , cost :  0.555914 , training accuracy :  0.87991\n",
      "i: 13800 , cost :  0.959973 , training accuracy :  0.838384\n",
      "i: 13900 , cost :  0.573385 , training accuracy :  0.878788\n",
      "i: 14000 , cost :  0.952687 , training accuracy :  0.861953\n",
      "i: 14100 , cost :  0.677445 , training accuracy :  0.883277\n",
      "i: 14200 , cost :  4.24936 , training accuracy :  0.780022\n",
      "i: 14300 , cost :  0.701438 , training accuracy :  0.876543\n",
      "i: 14400 , cost :  0.559621 , training accuracy :  0.877666\n",
      "i: 14500 , cost :  9.40739 , training accuracy :  0.713805\n",
      "i: 14600 , cost :  0.580542 , training accuracy :  0.881033\n",
      "i: 14700 , cost :  9.74437 , training accuracy :  0.40853\n",
      "i: 14800 , cost :  0.508025 , training accuracy :  0.8844\n",
      "i: 14900 , cost :  2.59159 , training accuracy :  0.826038\n",
      "i: 15000 , cost :  0.453208 , training accuracy :  0.8844\n",
      "i: 15100 , cost :  1.68583 , training accuracy :  0.849607\n",
      "i: 15200 , cost :  0.518295 , training accuracy :  0.882155\n",
      "i: 15300 , cost :  2.37301 , training accuracy :  0.81257\n",
      "i: 15400 , cost :  0.533103 , training accuracy :  0.877666\n",
      "i: 15500 , cost :  0.97976 , training accuracy :  0.839506\n",
      "i: 15600 , cost :  0.536493 , training accuracy :  0.87991\n",
      "i: 15700 , cost :  0.68152 , training accuracy :  0.869809\n",
      "i: 15800 , cost :  4.49976 , training accuracy :  0.571268\n",
      "i: 15900 , cost :  0.686813 , training accuracy :  0.881033\n",
      "i: 16000 , cost :  1.92279 , training accuracy :  0.832772\n",
      "i: 16100 , cost :  0.577918 , training accuracy :  0.876543\n",
      "i: 16200 , cost :  1.06935 , training accuracy :  0.82716\n",
      "i: 16300 , cost :  0.897404 , training accuracy :  0.851852\n",
      "i: 16400 , cost :  0.432421 , training accuracy :  0.886644\n",
      "i: 16500 , cost :  6.66706 , training accuracy :  0.488215\n",
      "i: 16600 , cost :  0.741663 , training accuracy :  0.867565\n",
      "i: 16700 , cost :  0.421837 , training accuracy :  0.892256\n",
      "i: 16800 , cost :  11.1867 , training accuracy :  0.407407\n",
      "i: 16900 , cost :  0.637472 , training accuracy :  0.8844\n",
      "i: 17000 , cost :  0.886415 , training accuracy :  0.864198\n",
      "i: 17100 , cost :  0.469468 , training accuracy :  0.882155\n",
      "i: 17200 , cost :  0.482331 , training accuracy :  0.878788\n",
      "i: 17300 , cost :  1.94056 , training accuracy :  0.826038\n",
      "i: 17400 , cost :  0.657885 , training accuracy :  0.876543\n",
      "i: 17500 , cost :  0.474127 , training accuracy :  0.859708\n",
      "i: 17600 , cost :  1.2715 , training accuracy :  0.857464\n",
      "i: 17700 , cost :  0.497937 , training accuracy :  0.883277\n",
      "i: 17800 , cost :  0.624458 , training accuracy :  0.860831\n",
      "i: 17900 , cost :  1.41779 , training accuracy :  0.845118\n",
      "i: 18000 , cost :  1.5509 , training accuracy :  0.787879\n",
      "i: 18100 , cost :  0.484827 , training accuracy :  0.888889\n",
      "i: 18200 , cost :  1.74851 , training accuracy :  0.843996\n",
      "i: 18300 , cost :  0.989242 , training accuracy :  0.742985\n",
      "i: 18400 , cost :  1.24151 , training accuracy :  0.860831\n",
      "i: 18500 , cost :  0.620257 , training accuracy :  0.887767\n",
      "i: 18600 , cost :  1.00979 , training accuracy :  0.849607\n",
      "i: 18700 , cost :  0.810321 , training accuracy :  0.860831\n",
      "i: 18800 , cost :  1.6577 , training accuracy :  0.85073\n",
      "i: 18900 , cost :  0.504844 , training accuracy :  0.887767\n",
      "i: 19000 , cost :  4.33305 , training accuracy :  0.767677\n",
      "i: 19100 , cost :  0.418679 , training accuracy :  0.893378\n",
      "i: 19200 , cost :  0.431072 , training accuracy :  0.881033\n",
      "i: 19300 , cost :  1.14944 , training accuracy :  0.857464\n",
      "i: 19400 , cost :  0.942612 , training accuracy :  0.864198\n",
      "i: 19500 , cost :  0.462955 , training accuracy :  0.887767\n",
      "i: 19600 , cost :  0.982997 , training accuracy :  0.786756\n",
      "i: 19700 , cost :  0.848812 , training accuracy :  0.870932\n",
      "i: 19800 , cost :  0.460997 , training accuracy :  0.887767\n",
      "i: 19900 , cost :  2.92603 , training accuracy :  0.787879\n",
      "i: 20000 , cost :  0.670814 , training accuracy :  0.882155\n",
      "i: 20100 , cost :  0.408437 , training accuracy :  0.888889\n",
      "i: 20200 , cost :  2.4299 , training accuracy :  0.81257\n",
      "i: 20300 , cost :  0.52143 , training accuracy :  0.886644\n",
      "i: 20400 , cost :  0.472396 , training accuracy :  0.874299\n",
      "i: 20500 , cost :  0.57275 , training accuracy :  0.886644\n",
      "i: 20600 , cost :  3.2329 , training accuracy :  0.775533\n",
      "i: 20700 , cost :  0.511024 , training accuracy :  0.886644\n",
      "i: 20800 , cost :  2.25267 , training accuracy :  0.832772\n",
      "i: 20900 , cost :  0.571971 , training accuracy :  0.881033\n",
      "i: 21000 , cost :  3.86808 , training accuracy :  0.589226\n",
      "i: 21100 , cost :  1.83404 , training accuracy :  0.838384\n",
      "i: 21200 , cost :  5.4545 , training accuracy :  0.570146\n",
      "i: 21300 , cost :  0.864264 , training accuracy :  0.858586\n",
      "i: 21400 , cost :  0.441534 , training accuracy :  0.887767\n",
      "i: 21500 , cost :  3.99873 , training accuracy :  0.584736\n",
      "i: 21600 , cost :  0.92782 , training accuracy :  0.855219\n",
      "i: 21700 , cost :  0.456752 , training accuracy :  0.886644\n",
      "i: 21800 , cost :  6.73851 , training accuracy :  0.483726\n",
      "i: 21900 , cost :  0.950573 , training accuracy :  0.857464\n",
      "i: 22000 , cost :  0.442668 , training accuracy :  0.885522\n",
      "i: 22100 , cost :  4.35097 , training accuracy :  0.771044\n",
      "i: 22200 , cost :  0.425577 , training accuracy :  0.887767\n",
      "i: 22300 , cost :  1.01611 , training accuracy :  0.870932\n",
      "i: 22400 , cost :  0.662295 , training accuracy :  0.8844\n",
      "i: 22500 , cost :  0.399942 , training accuracy :  0.887767\n",
      "i: 22600 , cost :  1.59028 , training accuracy :  0.845118\n",
      "i: 22700 , cost :  0.595877 , training accuracy :  0.876543\n",
      "i: 22800 , cost :  1.91155 , training accuracy :  0.824916\n",
      "i: 22900 , cost :  0.542889 , training accuracy :  0.87991\n",
      "i: 23000 , cost :  2.83568 , training accuracy :  0.600449\n",
      "i: 23100 , cost :  0.44176 , training accuracy :  0.891134\n",
      "i: 23200 , cost :  2.80411 , training accuracy :  0.806958\n",
      "i: 23300 , cost :  0.690649 , training accuracy :  0.883277\n",
      "i: 23400 , cost :  0.406788 , training accuracy :  0.891134\n",
      "i: 23500 , cost :  0.992902 , training accuracy :  0.858586\n",
      "i: 23600 , cost :  0.401653 , training accuracy :  0.890011\n",
      "i: 23700 , cost :  12.3053 , training accuracy :  0.419753\n",
      "i: 23800 , cost :  1.65573 , training accuracy :  0.765432\n",
      "i: 23900 , cost :  0.782698 , training accuracy :  0.872054\n",
      "i: 24000 , cost :  0.451752 , training accuracy :  0.892256\n",
      "i: 24100 , cost :  5.45619 , training accuracy :  0.531987\n",
      "i: 24200 , cost :  0.894254 , training accuracy :  0.857464\n",
      "i: 24300 , cost :  0.423818 , training accuracy :  0.891134\n",
      "i: 24400 , cost :  4.37046 , training accuracy :  0.765432\n",
      "i: 24500 , cost :  0.659154 , training accuracy :  0.885522\n",
      "i: 24600 , cost :  0.40736 , training accuracy :  0.886644\n",
      "i: 24700 , cost :  6.5384 , training accuracy :  0.754209\n",
      "i: 24800 , cost :  0.735351 , training accuracy :  0.882155\n",
      "i: 24900 , cost :  0.434053 , training accuracy :  0.890011\n",
      "i: 25000 , cost :  1.64 , training accuracy :  0.836139\n",
      "i: 25100 , cost :  1.41586 , training accuracy :  0.792368\n",
      "i: 25200 , cost :  0.515168 , training accuracy :  0.891134\n",
      "i: 25300 , cost :  1.7577 , training accuracy :  0.809203\n",
      "i: 25400 , cost :  3.88128 , training accuracy :  0.786756\n",
      "i: 25500 , cost :  0.498057 , training accuracy :  0.894501\n",
      "i: 25600 , cost :  3.9435 , training accuracy :  0.763187\n",
      "i: 25700 , cost :  0.889731 , training accuracy :  0.8844\n",
      "i: 25800 , cost :  0.51738 , training accuracy :  0.895623\n",
      "i: 25900 , cost :  0.391802 , training accuracy :  0.888889\n",
      "i: 26000 , cost :  0.535309 , training accuracy :  0.894501\n",
      "i: 26100 , cost :  2.43956 , training accuracy :  0.786756\n",
      "i: 26200 , cost :  1.58564 , training accuracy :  0.848485\n",
      "i: 26300 , cost :  0.578365 , training accuracy :  0.890011\n",
      "i: 26400 , cost :  0.403202 , training accuracy :  0.883277\n",
      "i: 26500 , cost :  1.81238 , training accuracy :  0.845118\n",
      "i: 26600 , cost :  0.88127 , training accuracy :  0.882155\n",
      "i: 26700 , cost :  0.421445 , training accuracy :  0.894501\n",
      "i: 26800 , cost :  6.55388 , training accuracy :  0.492705\n",
      "i: 26900 , cost :  1.60036 , training accuracy :  0.781145\n",
      "i: 27000 , cost :  0.443997 , training accuracy :  0.893378\n",
      "i: 27100 , cost :  3.66345 , training accuracy :  0.772166\n",
      "i: 27200 , cost :  1.02919 , training accuracy :  0.867565\n",
      "i: 27300 , cost :  0.425575 , training accuracy :  0.891134\n",
      "i: 27400 , cost :  3.47503 , training accuracy :  0.815937\n",
      "i: 27500 , cost :  0.420314 , training accuracy :  0.894501\n",
      "i: 27600 , cost :  2.15617 , training accuracy :  0.829405\n",
      "i: 27700 , cost :  0.686971 , training accuracy :  0.8844\n",
      "i: 27800 , cost :  0.406553 , training accuracy :  0.894501\n",
      "i: 27900 , cost :  2.14206 , training accuracy :  0.841751\n",
      "i: 28000 , cost :  1.44905 , training accuracy :  0.801347\n",
      "i: 28100 , cost :  0.460746 , training accuracy :  0.894501\n",
      "i: 28200 , cost :  1.06775 , training accuracy :  0.772166\n",
      "i: 28300 , cost :  2.68729 , training accuracy :  0.690236\n",
      "i: 28400 , cost :  0.491201 , training accuracy :  0.890011\n",
      "i: 28500 , cost :  0.445964 , training accuracy :  0.900112\n",
      "i: 28600 , cost :  2.06898 , training accuracy :  0.832772\n",
      "i: 28700 , cost :  0.583999 , training accuracy :  0.886644\n",
      "i: 28800 , cost :  1.02892 , training accuracy :  0.843996\n",
      "i: 28900 , cost :  3.07248 , training accuracy :  0.810326\n",
      "i: 29000 , cost :  0.522658 , training accuracy :  0.888889\n",
      "i: 29100 , cost :  0.533257 , training accuracy :  0.845118\n",
      "i: 29200 , cost :  0.791665 , training accuracy :  0.851852\n",
      "i: 29300 , cost :  0.816906 , training accuracy :  0.867565\n",
      "i: 29400 , cost :  0.417735 , training accuracy :  0.888889\n",
      "i: 29500 , cost :  1.2816 , training accuracy :  0.85073\n",
      "i: 29600 , cost :  0.687205 , training accuracy :  0.892256\n",
      "i: 29700 , cost :  0.406918 , training accuracy :  0.893378\n",
      "i: 29800 , cost :  0.889865 , training accuracy :  0.85073\n",
      "i: 29900 , cost :  0.410401 , training accuracy :  0.885522\n",
      "Final training accuracy :  0.59596\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.005 ][epoch: 30000 ][hidden: 100 ][file: bhavul_tr_acc_0.60_prediction.csv ] ACCURACY :  0.59596\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  42.0943 , training accuracy :  0.400673\n",
      "i: 100 , cost :  0.991706 , training accuracy :  0.659933\n",
      "i: 200 , cost :  0.709694 , training accuracy :  0.653199\n",
      "i: 300 , cost :  0.664838 , training accuracy :  0.646465\n",
      "i: 400 , cost :  0.654796 , training accuracy :  0.640853\n",
      "i: 500 , cost :  0.652919 , training accuracy :  0.634119\n",
      "i: 600 , cost :  0.652562 , training accuracy :  0.634119\n",
      "i: 700 , cost :  0.652016 , training accuracy :  0.639731\n",
      "i: 800 , cost :  0.651537 , training accuracy :  0.639731\n",
      "i: 900 , cost :  0.65124 , training accuracy :  0.638608\n",
      "Final training accuracy :  0.638608\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 1000 ][hidden: 3 ][file: bhavul_tr_acc_0.64_prediction.csv ] ACCURACY :  0.638608\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  266.709 , training accuracy :  0.616162\n",
      "i: 100 , cost :  5.14185 , training accuracy :  0.639731\n",
      "i: 200 , cost :  0.955443 , training accuracy :  0.704826\n",
      "i: 300 , cost :  0.473897 , training accuracy :  0.805836\n",
      "i: 400 , cost :  0.437366 , training accuracy :  0.804714\n",
      "i: 500 , cost :  0.761498 , training accuracy :  0.762065\n",
      "i: 600 , cost :  0.431512 , training accuracy :  0.806958\n",
      "i: 700 , cost :  0.424231 , training accuracy :  0.803591\n",
      "i: 800 , cost :  0.951004 , training accuracy :  0.719416\n",
      "i: 900 , cost :  0.426947 , training accuracy :  0.801347\n",
      "Final training accuracy :  0.803591\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 1000 ][hidden: 10 ][file: bhavul_tr_acc_0.80_prediction.csv ] ACCURACY :  0.803591\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  158.426 , training accuracy :  0.646465\n",
      "i: 100 , cost :  3.40065 , training accuracy :  0.607183\n",
      "i: 200 , cost :  1.81787 , training accuracy :  0.54321\n",
      "i: 300 , cost :  1.54312 , training accuracy :  0.582492\n",
      "i: 400 , cost :  1.12027 , training accuracy :  0.67789\n",
      "i: 500 , cost :  1.23489 , training accuracy :  0.648709\n",
      "i: 600 , cost :  1.21015 , training accuracy :  0.638608\n",
      "i: 700 , cost :  1.15815 , training accuracy :  0.641975\n",
      "i: 800 , cost :  1.13109 , training accuracy :  0.650954\n",
      "i: 900 , cost :  1.12161 , training accuracy :  0.655443\n",
      "Final training accuracy :  0.654321\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 1000 ][hidden: 15 ][file: bhavul_tr_acc_0.65_prediction.csv ] ACCURACY :  0.654321\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1541.69 , training accuracy :  0.383838\n",
      "i: 100 , cost :  7.68327 , training accuracy :  0.656566\n",
      "i: 200 , cost :  2.20643 , training accuracy :  0.729517\n",
      "i: 300 , cost :  2.00041 , training accuracy :  0.773288\n",
      "i: 400 , cost :  12.6191 , training accuracy :  0.43771\n",
      "i: 500 , cost :  1.0654 , training accuracy :  0.800224\n",
      "i: 600 , cost :  1.14453 , training accuracy :  0.809203\n",
      "i: 700 , cost :  3.20619 , training accuracy :  0.655443\n",
      "i: 800 , cost :  1.51508 , training accuracy :  0.801347\n",
      "i: 900 , cost :  0.921402 , training accuracy :  0.828283\n",
      "Final training accuracy :  0.721661\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 1000 ][hidden: 50 ][file: bhavul_tr_acc_0.72_prediction.csv ] ACCURACY :  0.721661\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  363.688 , training accuracy :  0.353535\n",
      "i: 100 , cost :  5.35859 , training accuracy :  0.723906\n",
      "i: 200 , cost :  2.9906 , training accuracy :  0.783389\n",
      "i: 300 , cost :  2.16395 , training accuracy :  0.819304\n",
      "i: 400 , cost :  9.59142 , training accuracy :  0.739618\n",
      "i: 500 , cost :  1.91702 , training accuracy :  0.832772\n",
      "i: 600 , cost :  1.83372 , training accuracy :  0.818182\n",
      "i: 700 , cost :  3.48445 , training accuracy :  0.79349\n",
      "i: 800 , cost :  2.63068 , training accuracy :  0.808081\n",
      "i: 900 , cost :  1.74621 , training accuracy :  0.840629\n",
      "Final training accuracy :  0.824916\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 1000 ][hidden: 100 ][file: bhavul_tr_acc_0.82_prediction.csv ] ACCURACY :  0.824916\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  143.329 , training accuracy :  0.417508\n",
      "i: 100 , cost :  6.84225 , training accuracy :  0.603816\n",
      "i: 200 , cost :  0.671688 , training accuracy :  0.634119\n",
      "i: 300 , cost :  0.626415 , training accuracy :  0.643098\n",
      "i: 400 , cost :  0.622303 , training accuracy :  0.657688\n",
      "i: 500 , cost :  0.619056 , training accuracy :  0.661055\n",
      "i: 600 , cost :  0.616781 , training accuracy :  0.668911\n",
      "i: 700 , cost :  0.622683 , training accuracy :  0.679012\n",
      "i: 800 , cost :  0.613249 , training accuracy :  0.672278\n",
      "i: 900 , cost :  0.614053 , training accuracy :  0.672278\n",
      "i: 1000 , cost :  0.611143 , training accuracy :  0.673401\n",
      "i: 1100 , cost :  0.62122 , training accuracy :  0.689113\n",
      "i: 1200 , cost :  0.616114 , training accuracy :  0.680135\n",
      "i: 1300 , cost :  0.617901 , training accuracy :  0.689113\n",
      "i: 1400 , cost :  0.606597 , training accuracy :  0.679012\n",
      "i: 1500 , cost :  0.605364 , training accuracy :  0.679012\n",
      "i: 1600 , cost :  0.605996 , training accuracy :  0.681257\n",
      "i: 1700 , cost :  0.603013 , training accuracy :  0.680135\n",
      "i: 1800 , cost :  0.610991 , training accuracy :  0.687991\n",
      "i: 1900 , cost :  0.600398 , training accuracy :  0.683502\n",
      "i: 2000 , cost :  0.603516 , training accuracy :  0.69248\n",
      "i: 2100 , cost :  0.60252 , training accuracy :  0.694725\n",
      "i: 2200 , cost :  0.598444 , training accuracy :  0.686869\n",
      "i: 2300 , cost :  0.595537 , training accuracy :  0.686869\n",
      "i: 2400 , cost :  0.59439 , training accuracy :  0.687991\n",
      "i: 2500 , cost :  0.589082 , training accuracy :  0.698092\n",
      "i: 2600 , cost :  0.586784 , training accuracy :  0.702581\n",
      "i: 2700 , cost :  0.582724 , training accuracy :  0.709315\n",
      "i: 2800 , cost :  0.578518 , training accuracy :  0.71156\n",
      "i: 2900 , cost :  0.57343 , training accuracy :  0.714927\n",
      "i: 3000 , cost :  0.56685 , training accuracy :  0.718294\n",
      "i: 3100 , cost :  0.557314 , training accuracy :  0.721661\n",
      "i: 3200 , cost :  0.542692 , training accuracy :  0.73064\n",
      "i: 3300 , cost :  0.526777 , training accuracy :  0.739618\n",
      "i: 3400 , cost :  0.506319 , training accuracy :  0.749719\n",
      "i: 3500 , cost :  0.489812 , training accuracy :  0.783389\n",
      "i: 3600 , cost :  0.482671 , training accuracy :  0.79349\n",
      "i: 3700 , cost :  0.475775 , training accuracy :  0.782267\n",
      "i: 3800 , cost :  0.471131 , training accuracy :  0.792368\n",
      "i: 3900 , cost :  0.46948 , training accuracy :  0.796857\n",
      "i: 4000 , cost :  0.466024 , training accuracy :  0.792368\n",
      "i: 4100 , cost :  0.461662 , training accuracy :  0.790123\n",
      "i: 4200 , cost :  0.45871 , training accuracy :  0.794613\n",
      "i: 4300 , cost :  0.457236 , training accuracy :  0.79798\n",
      "i: 4400 , cost :  0.452746 , training accuracy :  0.792368\n",
      "i: 4500 , cost :  0.448754 , training accuracy :  0.804714\n",
      "i: 4600 , cost :  0.444153 , training accuracy :  0.799102\n",
      "i: 4700 , cost :  0.442261 , training accuracy :  0.800224\n",
      "i: 4800 , cost :  0.441025 , training accuracy :  0.794613\n",
      "i: 4900 , cost :  0.44022 , training accuracy :  0.799102\n",
      "Final training accuracy :  0.799102\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 5000 ][hidden: 3 ][file: bhavul_tr_acc_0.80_prediction.csv ] ACCURACY :  0.799102\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  222.897 , training accuracy :  0.616162\n",
      "i: 100 , cost :  2.73778 , training accuracy :  0.636364\n",
      "i: 200 , cost :  1.14263 , training accuracy :  0.647587\n",
      "i: 300 , cost :  0.72532 , training accuracy :  0.716049\n",
      "i: 400 , cost :  0.637181 , training accuracy :  0.727273\n",
      "i: 500 , cost :  0.532988 , training accuracy :  0.775533\n",
      "i: 600 , cost :  0.613197 , training accuracy :  0.735129\n",
      "i: 700 , cost :  0.478259 , training accuracy :  0.79349\n",
      "i: 800 , cost :  1.15799 , training accuracy :  0.525253\n",
      "i: 900 , cost :  0.487477 , training accuracy :  0.780022\n",
      "i: 1000 , cost :  0.453795 , training accuracy :  0.79798\n",
      "i: 1100 , cost :  0.443008 , training accuracy :  0.799102\n",
      "i: 1200 , cost :  0.494306 , training accuracy :  0.771044\n",
      "i: 1300 , cost :  0.523478 , training accuracy :  0.767677\n",
      "i: 1400 , cost :  0.437465 , training accuracy :  0.817059\n",
      "i: 1500 , cost :  0.440221 , training accuracy :  0.79349\n",
      "i: 1600 , cost :  0.433869 , training accuracy :  0.799102\n",
      "i: 1700 , cost :  0.439895 , training accuracy :  0.813693\n",
      "i: 1800 , cost :  0.45129 , training accuracy :  0.785634\n",
      "i: 1900 , cost :  0.455325 , training accuracy :  0.790123\n",
      "i: 2000 , cost :  0.538786 , training accuracy :  0.776655\n",
      "i: 2100 , cost :  0.422141 , training accuracy :  0.83165\n",
      "i: 2200 , cost :  0.577034 , training accuracy :  0.755331\n",
      "i: 2300 , cost :  0.438567 , training accuracy :  0.820426\n",
      "i: 2400 , cost :  0.49215 , training accuracy :  0.776655\n",
      "i: 2500 , cost :  0.410766 , training accuracy :  0.819304\n",
      "i: 2600 , cost :  0.490803 , training accuracy :  0.776655\n",
      "i: 2700 , cost :  0.434663 , training accuracy :  0.803591\n",
      "i: 2800 , cost :  0.472038 , training accuracy :  0.813693\n",
      "i: 2900 , cost :  0.463365 , training accuracy :  0.789001\n",
      "i: 3000 , cost :  0.448396 , training accuracy :  0.79798\n",
      "i: 3100 , cost :  0.438301 , training accuracy :  0.801347\n",
      "i: 3200 , cost :  0.409736 , training accuracy :  0.814815\n",
      "i: 3300 , cost :  0.467892 , training accuracy :  0.785634\n",
      "i: 3400 , cost :  0.414009 , training accuracy :  0.806958\n",
      "i: 3500 , cost :  0.423673 , training accuracy :  0.833894\n",
      "i: 3600 , cost :  0.497038 , training accuracy :  0.805836\n",
      "i: 3700 , cost :  0.453945 , training accuracy :  0.826038\n",
      "i: 3800 , cost :  0.478789 , training accuracy :  0.813693\n",
      "i: 3900 , cost :  0.415142 , training accuracy :  0.813693\n",
      "i: 4000 , cost :  0.406854 , training accuracy :  0.835017\n",
      "i: 4100 , cost :  0.455908 , training accuracy :  0.79349\n",
      "i: 4200 , cost :  0.447792 , training accuracy :  0.79798\n",
      "i: 4300 , cost :  0.528414 , training accuracy :  0.796857\n",
      "i: 4400 , cost :  0.566443 , training accuracy :  0.774411\n",
      "i: 4500 , cost :  0.455157 , training accuracy :  0.796857\n",
      "i: 4600 , cost :  0.485522 , training accuracy :  0.780022\n",
      "i: 4700 , cost :  0.423209 , training accuracy :  0.81257\n",
      "i: 4800 , cost :  0.478021 , training accuracy :  0.783389\n",
      "i: 4900 , cost :  0.415095 , training accuracy :  0.824916\n",
      "Final training accuracy :  0.811448\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 5000 ][hidden: 10 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.811448\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  768.166 , training accuracy :  0.619529\n",
      "i: 100 , cost :  11.9059 , training accuracy :  0.572391\n",
      "i: 200 , cost :  2.49548 , training accuracy :  0.768799\n",
      "i: 300 , cost :  1.87789 , training accuracy :  0.767677\n",
      "i: 400 , cost :  1.32913 , training accuracy :  0.777778\n",
      "i: 500 , cost :  1.39965 , training accuracy :  0.777778\n",
      "i: 600 , cost :  0.942682 , training accuracy :  0.783389\n",
      "i: 700 , cost :  3.50958 , training accuracy :  0.681257\n",
      "i: 800 , cost :  0.824026 , training accuracy :  0.790123\n",
      "i: 900 , cost :  0.777492 , training accuracy :  0.772166\n",
      "i: 1000 , cost :  0.695797 , training accuracy :  0.79349\n",
      "i: 1100 , cost :  0.622075 , training accuracy :  0.79349\n",
      "i: 1200 , cost :  0.785011 , training accuracy :  0.787879\n",
      "i: 1300 , cost :  1.39272 , training accuracy :  0.751964\n",
      "i: 1400 , cost :  1.95119 , training accuracy :  0.719416\n",
      "i: 1500 , cost :  0.89902 , training accuracy :  0.757576\n",
      "i: 1600 , cost :  0.554203 , training accuracy :  0.799102\n",
      "i: 1700 , cost :  0.805139 , training accuracy :  0.762065\n",
      "i: 1800 , cost :  1.47536 , training accuracy :  0.737374\n",
      "i: 1900 , cost :  0.537703 , training accuracy :  0.803591\n",
      "i: 2000 , cost :  0.669808 , training accuracy :  0.774411\n",
      "i: 2100 , cost :  1.29391 , training accuracy :  0.549944\n",
      "i: 2200 , cost :  0.505615 , training accuracy :  0.811448\n",
      "i: 2300 , cost :  0.738738 , training accuracy :  0.749719\n",
      "i: 2400 , cost :  0.713974 , training accuracy :  0.795735\n",
      "i: 2500 , cost :  0.50106 , training accuracy :  0.811448\n",
      "i: 2600 , cost :  1.33324 , training accuracy :  0.749719\n",
      "i: 2700 , cost :  1.73797 , training accuracy :  0.735129\n",
      "i: 2800 , cost :  0.568046 , training accuracy :  0.800224\n",
      "i: 2900 , cost :  0.470763 , training accuracy :  0.815937\n",
      "i: 3000 , cost :  0.523004 , training accuracy :  0.785634\n",
      "i: 3100 , cost :  0.608383 , training accuracy :  0.794613\n",
      "i: 3200 , cost :  0.51911 , training accuracy :  0.808081\n",
      "i: 3300 , cost :  0.436556 , training accuracy :  0.830527\n",
      "i: 3400 , cost :  0.627762 , training accuracy :  0.794613\n",
      "i: 3500 , cost :  0.48677 , training accuracy :  0.824916\n",
      "i: 3600 , cost :  0.431643 , training accuracy :  0.830527\n",
      "i: 3700 , cost :  0.531417 , training accuracy :  0.822671\n",
      "i: 3800 , cost :  0.430876 , training accuracy :  0.82716\n",
      "i: 3900 , cost :  4.73824 , training accuracy :  0.391695\n",
      "i: 4000 , cost :  0.471108 , training accuracy :  0.832772\n",
      "i: 4100 , cost :  0.874681 , training accuracy :  0.689113\n",
      "i: 4200 , cost :  1.17764 , training accuracy :  0.765432\n",
      "i: 4300 , cost :  0.45141 , training accuracy :  0.833894\n",
      "i: 4400 , cost :  0.410031 , training accuracy :  0.82716\n",
      "i: 4500 , cost :  2.54319 , training accuracy :  0.400673\n",
      "i: 4600 , cost :  0.433652 , training accuracy :  0.82716\n",
      "i: 4700 , cost :  0.411146 , training accuracy :  0.830527\n",
      "i: 4800 , cost :  0.671337 , training accuracy :  0.777778\n",
      "i: 4900 , cost :  0.421903 , training accuracy :  0.828283\n",
      "Final training accuracy :  0.833894\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 5000 ][hidden: 15 ][file: bhavul_tr_acc_0.83_prediction.csv ] ACCURACY :  0.833894\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  355.08 , training accuracy :  0.368126\n",
      "i: 100 , cost :  4.16024 , training accuracy :  0.664422\n",
      "i: 200 , cost :  1.87569 , training accuracy :  0.780022\n",
      "i: 300 , cost :  1.44923 , training accuracy :  0.789001\n",
      "i: 400 , cost :  1.88019 , training accuracy :  0.776655\n",
      "i: 500 , cost :  3.67764 , training accuracy :  0.552189\n",
      "i: 600 , cost :  1.07572 , training accuracy :  0.796857\n",
      "i: 700 , cost :  3.05953 , training accuracy :  0.608305\n",
      "i: 800 , cost :  1.24493 , training accuracy :  0.802469\n",
      "i: 900 , cost :  1.06888 , training accuracy :  0.82716\n",
      "i: 1000 , cost :  1.07492 , training accuracy :  0.82716\n",
      "i: 1100 , cost :  3.77996 , training accuracy :  0.728395\n",
      "i: 1200 , cost :  0.706361 , training accuracy :  0.823793\n",
      "i: 1300 , cost :  0.933236 , training accuracy :  0.829405\n",
      "i: 1400 , cost :  2.21772 , training accuracy :  0.65881\n",
      "i: 1500 , cost :  0.997006 , training accuracy :  0.814815\n",
      "i: 1600 , cost :  8.03117 , training accuracy :  0.694725\n",
      "i: 1700 , cost :  0.778009 , training accuracy :  0.836139\n",
      "i: 1800 , cost :  1.5432 , training accuracy :  0.762065\n",
      "i: 1900 , cost :  1.35855 , training accuracy :  0.82716\n",
      "i: 2000 , cost :  0.664365 , training accuracy :  0.847363\n",
      "i: 2100 , cost :  0.788584 , training accuracy :  0.835017\n",
      "i: 2200 , cost :  0.644002 , training accuracy :  0.84624\n",
      "i: 2300 , cost :  0.785685 , training accuracy :  0.835017\n",
      "i: 2400 , cost :  1.28578 , training accuracy :  0.836139\n",
      "i: 2500 , cost :  0.809354 , training accuracy :  0.843996\n",
      "i: 2600 , cost :  0.51746 , training accuracy :  0.852974\n",
      "i: 2700 , cost :  2.96096 , training accuracy :  0.747475\n",
      "i: 2800 , cost :  3.07629 , training accuracy :  0.751964\n",
      "i: 2900 , cost :  0.823061 , training accuracy :  0.857464\n",
      "i: 3000 , cost :  8.08676 , training accuracy :  0.406285\n",
      "i: 3100 , cost :  5.43731 , training accuracy :  0.462402\n",
      "i: 3200 , cost :  0.642318 , training accuracy :  0.851852\n",
      "i: 3300 , cost :  1.29975 , training accuracy :  0.822671\n",
      "i: 3400 , cost :  1.91548 , training accuracy :  0.676768\n",
      "i: 3500 , cost :  0.499699 , training accuracy :  0.857464\n",
      "i: 3600 , cost :  2.48634 , training accuracy :  0.766554\n",
      "i: 3700 , cost :  0.554652 , training accuracy :  0.857464\n",
      "i: 3800 , cost :  4.35244 , training accuracy :  0.746352\n",
      "i: 3900 , cost :  0.667821 , training accuracy :  0.855219\n",
      "i: 4000 , cost :  2.10706 , training accuracy :  0.803591\n",
      "i: 4100 , cost :  0.678847 , training accuracy :  0.856341\n",
      "i: 4200 , cost :  1.70606 , training accuracy :  0.690236\n",
      "i: 4300 , cost :  1.71135 , training accuracy :  0.813693\n",
      "i: 4400 , cost :  0.954453 , training accuracy :  0.848485\n",
      "i: 4500 , cost :  0.527085 , training accuracy :  0.859708\n",
      "i: 4600 , cost :  1.35133 , training accuracy :  0.82716\n",
      "i: 4700 , cost :  0.703129 , training accuracy :  0.854097\n",
      "i: 4800 , cost :  6.99786 , training accuracy :  0.718294\n",
      "i: 4900 , cost :  2.311 , training accuracy :  0.790123\n",
      "Final training accuracy :  0.858586\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 5000 ][hidden: 50 ][file: bhavul_tr_acc_0.86_prediction.csv ] ACCURACY :  0.858586\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  2322.75 , training accuracy :  0.362514\n",
      "i: 100 , cost :  5.96825 , training accuracy :  0.691358\n",
      "i: 200 , cost :  3.29475 , training accuracy :  0.742985\n",
      "i: 300 , cost :  3.70513 , training accuracy :  0.746352\n",
      "i: 400 , cost :  2.53107 , training accuracy :  0.800224\n",
      "i: 500 , cost :  1.16169 , training accuracy :  0.823793\n",
      "i: 600 , cost :  8.41287 , training accuracy :  0.519641\n",
      "i: 700 , cost :  1.08136 , training accuracy :  0.837261\n",
      "i: 800 , cost :  3.01655 , training accuracy :  0.774411\n",
      "i: 900 , cost :  1.04004 , training accuracy :  0.85073\n",
      "i: 1000 , cost :  13.5514 , training accuracy :  0.701459\n",
      "i: 1100 , cost :  1.11495 , training accuracy :  0.861953\n",
      "i: 1200 , cost :  2.12865 , training accuracy :  0.815937\n",
      "i: 1300 , cost :  1.25052 , training accuracy :  0.861953\n",
      "i: 1400 , cost :  8.23312 , training accuracy :  0.744108\n",
      "i: 1500 , cost :  1.20933 , training accuracy :  0.854097\n",
      "i: 1600 , cost :  2.95957 , training accuracy :  0.755331\n",
      "i: 1700 , cost :  1.61567 , training accuracy :  0.822671\n",
      "i: 1800 , cost :  0.787251 , training accuracy :  0.870932\n",
      "i: 1900 , cost :  7.94765 , training accuracy :  0.544332\n",
      "i: 2000 , cost :  0.822484 , training accuracy :  0.874299\n",
      "i: 2100 , cost :  2.49211 , training accuracy :  0.787879\n",
      "i: 2200 , cost :  0.87399 , training accuracy :  0.875421\n",
      "i: 2300 , cost :  7.16863 , training accuracy :  0.736251\n",
      "i: 2400 , cost :  1.01725 , training accuracy :  0.874299\n",
      "i: 2500 , cost :  5.37265 , training accuracy :  0.772166\n",
      "i: 2600 , cost :  13.1547 , training accuracy :  0.401796\n",
      "i: 2700 , cost :  1.92195 , training accuracy :  0.841751\n",
      "i: 2800 , cost :  0.810643 , training accuracy :  0.875421\n",
      "i: 2900 , cost :  2.74567 , training accuracy :  0.803591\n",
      "i: 3000 , cost :  4.05406 , training accuracy :  0.768799\n",
      "i: 3100 , cost :  3.03156 , training accuracy :  0.649832\n",
      "i: 3200 , cost :  0.936803 , training accuracy :  0.87991\n",
      "i: 3300 , cost :  1.66821 , training accuracy :  0.800224\n",
      "i: 3400 , cost :  0.88902 , training accuracy :  0.875421\n",
      "i: 3500 , cost :  4.31578 , training accuracy :  0.796857\n",
      "i: 3600 , cost :  1.04845 , training accuracy :  0.858586\n",
      "i: 3700 , cost :  3.44561 , training accuracy :  0.796857\n",
      "i: 3800 , cost :  0.879358 , training accuracy :  0.876543\n",
      "i: 3900 , cost :  5.29872 , training accuracy :  0.781145\n",
      "i: 4000 , cost :  0.759851 , training accuracy :  0.876543\n",
      "i: 4100 , cost :  2.22199 , training accuracy :  0.789001\n",
      "i: 4200 , cost :  0.689832 , training accuracy :  0.87991\n",
      "i: 4300 , cost :  0.912615 , training accuracy :  0.874299\n",
      "i: 4400 , cost :  5.51426 , training accuracy :  0.74523\n",
      "i: 4500 , cost :  1.12249 , training accuracy :  0.869809\n",
      "i: 4600 , cost :  1.36344 , training accuracy :  0.728395\n",
      "i: 4700 , cost :  0.62272 , training accuracy :  0.885522\n",
      "i: 4800 , cost :  7.85261 , training accuracy :  0.510662\n",
      "i: 4900 , cost :  0.797064 , training accuracy :  0.878788\n",
      "Final training accuracy :  0.569024\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 5000 ][hidden: 100 ][file: bhavul_tr_acc_0.57_prediction.csv ] ACCURACY :  0.569024\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  59.8431 , training accuracy :  0.349046\n",
      "i: 100 , cost :  0.803469 , training accuracy :  0.616162\n",
      "i: 200 , cost :  0.709121 , training accuracy :  0.617284\n",
      "i: 300 , cost :  0.69961 , training accuracy :  0.618406\n",
      "i: 400 , cost :  0.695998 , training accuracy :  0.618406\n",
      "i: 500 , cost :  0.693562 , training accuracy :  0.617284\n",
      "i: 600 , cost :  0.690576 , training accuracy :  0.617284\n",
      "i: 700 , cost :  0.686989 , training accuracy :  0.617284\n",
      "i: 800 , cost :  0.685076 , training accuracy :  0.617284\n",
      "i: 900 , cost :  0.683528 , training accuracy :  0.617284\n",
      "i: 1000 , cost :  0.681982 , training accuracy :  0.617284\n",
      "i: 1100 , cost :  0.68044 , training accuracy :  0.617284\n",
      "i: 1200 , cost :  0.678923 , training accuracy :  0.617284\n",
      "i: 1300 , cost :  0.677441 , training accuracy :  0.617284\n",
      "i: 1400 , cost :  0.676006 , training accuracy :  0.617284\n",
      "i: 1500 , cost :  0.674629 , training accuracy :  0.617284\n",
      "i: 1600 , cost :  0.673316 , training accuracy :  0.617284\n",
      "i: 1700 , cost :  0.672075 , training accuracy :  0.617284\n",
      "i: 1800 , cost :  0.670907 , training accuracy :  0.617284\n",
      "i: 1900 , cost :  0.669818 , training accuracy :  0.617284\n",
      "i: 2000 , cost :  0.668817 , training accuracy :  0.617284\n",
      "i: 2100 , cost :  0.667919 , training accuracy :  0.617284\n",
      "i: 2200 , cost :  0.667157 , training accuracy :  0.617284\n",
      "i: 2300 , cost :  0.66658 , training accuracy :  0.615039\n",
      "i: 2400 , cost :  0.666212 , training accuracy :  0.615039\n",
      "i: 2500 , cost :  0.666025 , training accuracy :  0.615039\n",
      "i: 2600 , cost :  0.665947 , training accuracy :  0.615039\n",
      "i: 2700 , cost :  0.665919 , training accuracy :  0.616162\n",
      "i: 2800 , cost :  0.665911 , training accuracy :  0.616162\n",
      "i: 2900 , cost :  0.665908 , training accuracy :  0.616162\n",
      "i: 3000 , cost :  0.665907 , training accuracy :  0.616162\n",
      "i: 3100 , cost :  0.665905 , training accuracy :  0.616162\n",
      "i: 3200 , cost :  0.665903 , training accuracy :  0.616162\n",
      "i: 3300 , cost :  0.665901 , training accuracy :  0.616162\n",
      "i: 3400 , cost :  0.665898 , training accuracy :  0.616162\n",
      "i: 3500 , cost :  0.665888 , training accuracy :  0.616162\n",
      "i: 3600 , cost :  0.665866 , training accuracy :  0.616162\n",
      "i: 3700 , cost :  0.665823 , training accuracy :  0.615039\n",
      "i: 3800 , cost :  0.665796 , training accuracy :  0.615039\n",
      "i: 3900 , cost :  0.665763 , training accuracy :  0.615039\n",
      "i: 4000 , cost :  0.665733 , training accuracy :  0.615039\n",
      "i: 4100 , cost :  0.665704 , training accuracy :  0.617284\n",
      "i: 4200 , cost :  0.665675 , training accuracy :  0.617284\n",
      "i: 4300 , cost :  0.665573 , training accuracy :  0.617284\n",
      "i: 4400 , cost :  0.665515 , training accuracy :  0.617284\n",
      "i: 4500 , cost :  0.665491 , training accuracy :  0.617284\n",
      "i: 4600 , cost :  0.665467 , training accuracy :  0.617284\n",
      "i: 4700 , cost :  0.665444 , training accuracy :  0.618406\n",
      "i: 4800 , cost :  0.66542 , training accuracy :  0.618406\n",
      "i: 4900 , cost :  0.665393 , training accuracy :  0.618406\n",
      "i: 5000 , cost :  0.665366 , training accuracy :  0.618406\n",
      "i: 5100 , cost :  0.665333 , training accuracy :  0.618406\n",
      "i: 5200 , cost :  0.665152 , training accuracy :  0.618406\n",
      "i: 5300 , cost :  0.664968 , training accuracy :  0.619529\n",
      "i: 5400 , cost :  0.66479 , training accuracy :  0.617284\n",
      "i: 5500 , cost :  0.664562 , training accuracy :  0.616162\n",
      "i: 5600 , cost :  0.66279 , training accuracy :  0.619529\n",
      "i: 5700 , cost :  0.654727 , training accuracy :  0.638608\n",
      "i: 5800 , cost :  0.636654 , training accuracy :  0.667789\n",
      "i: 5900 , cost :  0.620544 , training accuracy :  0.684624\n",
      "i: 6000 , cost :  0.61698 , training accuracy :  0.684624\n",
      "i: 6100 , cost :  0.615347 , training accuracy :  0.689113\n",
      "i: 6200 , cost :  0.614753 , training accuracy :  0.686869\n",
      "i: 6300 , cost :  0.614223 , training accuracy :  0.687991\n",
      "i: 6400 , cost :  0.613662 , training accuracy :  0.687991\n",
      "i: 6500 , cost :  0.61307 , training accuracy :  0.689113\n",
      "i: 6600 , cost :  0.612409 , training accuracy :  0.690236\n",
      "i: 6700 , cost :  0.611583 , training accuracy :  0.69248\n",
      "i: 6800 , cost :  0.610574 , training accuracy :  0.695847\n",
      "i: 6900 , cost :  0.609504 , training accuracy :  0.702581\n",
      "i: 7000 , cost :  0.60804 , training accuracy :  0.707071\n",
      "i: 7100 , cost :  0.603104 , training accuracy :  0.718294\n",
      "i: 7200 , cost :  0.562676 , training accuracy :  0.742985\n",
      "i: 7300 , cost :  0.487218 , training accuracy :  0.786756\n",
      "i: 7400 , cost :  0.477475 , training accuracy :  0.799102\n",
      "i: 7500 , cost :  0.459524 , training accuracy :  0.810326\n",
      "i: 7600 , cost :  0.454194 , training accuracy :  0.817059\n",
      "i: 7700 , cost :  0.452713 , training accuracy :  0.81257\n",
      "i: 7800 , cost :  0.451916 , training accuracy :  0.815937\n",
      "i: 7900 , cost :  0.451402 , training accuracy :  0.813693\n",
      "i: 8000 , cost :  0.452472 , training accuracy :  0.81257\n",
      "i: 8100 , cost :  0.45115 , training accuracy :  0.813693\n",
      "i: 8200 , cost :  0.451419 , training accuracy :  0.815937\n",
      "i: 8300 , cost :  0.451011 , training accuracy :  0.813693\n",
      "i: 8400 , cost :  0.452496 , training accuracy :  0.814815\n",
      "i: 8500 , cost :  0.450954 , training accuracy :  0.814815\n",
      "i: 8600 , cost :  0.451297 , training accuracy :  0.818182\n",
      "i: 8700 , cost :  0.451589 , training accuracy :  0.814815\n",
      "i: 8800 , cost :  0.450868 , training accuracy :  0.813693\n",
      "i: 8900 , cost :  0.451269 , training accuracy :  0.814815\n",
      "i: 9000 , cost :  0.450867 , training accuracy :  0.815937\n",
      "i: 9100 , cost :  0.451128 , training accuracy :  0.813693\n",
      "i: 9200 , cost :  0.45106 , training accuracy :  0.813693\n",
      "i: 9300 , cost :  0.451351 , training accuracy :  0.813693\n",
      "i: 9400 , cost :  0.45108 , training accuracy :  0.813693\n",
      "i: 9500 , cost :  0.451143 , training accuracy :  0.81257\n",
      "i: 9600 , cost :  0.451028 , training accuracy :  0.813693\n",
      "i: 9700 , cost :  0.452645 , training accuracy :  0.81257\n",
      "i: 9800 , cost :  0.450757 , training accuracy :  0.815937\n",
      "i: 9900 , cost :  0.450526 , training accuracy :  0.813693\n",
      "Final training accuracy :  0.814815\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 10000 ][hidden: 3 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.814815\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  30.8764 , training accuracy :  0.637486\n",
      "i: 100 , cost :  3.07824 , training accuracy :  0.641975\n",
      "i: 200 , cost :  1.6385 , training accuracy :  0.744108\n",
      "i: 300 , cost :  1.13748 , training accuracy :  0.758698\n",
      "i: 400 , cost :  1.20097 , training accuracy :  0.701459\n",
      "i: 500 , cost :  0.738133 , training accuracy :  0.785634\n",
      "i: 600 , cost :  0.62185 , training accuracy :  0.787879\n",
      "i: 700 , cost :  0.636714 , training accuracy :  0.76431\n",
      "i: 800 , cost :  0.553738 , training accuracy :  0.783389\n",
      "i: 900 , cost :  0.554782 , training accuracy :  0.808081\n",
      "i: 1000 , cost :  1.31672 , training accuracy :  0.717172\n",
      "i: 1100 , cost :  0.48314 , training accuracy :  0.817059\n",
      "i: 1200 , cost :  0.462971 , training accuracy :  0.813693\n",
      "i: 1300 , cost :  0.49483 , training accuracy :  0.809203\n",
      "i: 1400 , cost :  0.476002 , training accuracy :  0.817059\n",
      "i: 1500 , cost :  0.445448 , training accuracy :  0.824916\n",
      "i: 1600 , cost :  0.469668 , training accuracy :  0.817059\n",
      "i: 1700 , cost :  0.542108 , training accuracy :  0.766554\n",
      "i: 1800 , cost :  0.546488 , training accuracy :  0.783389\n",
      "i: 1900 , cost :  0.435434 , training accuracy :  0.818182\n",
      "i: 2000 , cost :  0.426438 , training accuracy :  0.818182\n",
      "i: 2100 , cost :  0.427752 , training accuracy :  0.823793\n",
      "i: 2200 , cost :  0.490899 , training accuracy :  0.775533\n",
      "i: 2300 , cost :  0.420794 , training accuracy :  0.821549\n",
      "i: 2400 , cost :  0.525809 , training accuracy :  0.780022\n",
      "i: 2500 , cost :  0.422786 , training accuracy :  0.809203\n",
      "i: 2600 , cost :  0.481941 , training accuracy :  0.780022\n",
      "i: 2700 , cost :  0.473025 , training accuracy :  0.802469\n",
      "i: 2800 , cost :  0.414221 , training accuracy :  0.815937\n",
      "i: 2900 , cost :  0.514453 , training accuracy :  0.789001\n",
      "i: 3000 , cost :  0.851561 , training accuracy :  0.738496\n",
      "i: 3100 , cost :  0.446686 , training accuracy :  0.802469\n",
      "i: 3200 , cost :  0.412872 , training accuracy :  0.813693\n",
      "i: 3300 , cost :  0.410016 , training accuracy :  0.823793\n",
      "i: 3400 , cost :  0.44651 , training accuracy :  0.787879\n",
      "i: 3500 , cost :  0.410147 , training accuracy :  0.818182\n",
      "i: 3600 , cost :  0.410319 , training accuracy :  0.818182\n",
      "i: 3700 , cost :  0.509089 , training accuracy :  0.786756\n",
      "i: 3800 , cost :  0.488162 , training accuracy :  0.802469\n",
      "i: 3900 , cost :  0.414131 , training accuracy :  0.820426\n",
      "i: 4000 , cost :  0.420336 , training accuracy :  0.81257\n",
      "i: 4100 , cost :  0.493469 , training accuracy :  0.801347\n",
      "i: 4200 , cost :  0.413607 , training accuracy :  0.817059\n",
      "i: 4300 , cost :  0.421581 , training accuracy :  0.806958\n",
      "i: 4400 , cost :  0.424277 , training accuracy :  0.823793\n",
      "i: 4500 , cost :  0.442145 , training accuracy :  0.789001\n",
      "i: 4600 , cost :  0.409128 , training accuracy :  0.828283\n",
      "i: 4700 , cost :  0.660482 , training accuracy :  0.782267\n",
      "i: 4800 , cost :  0.407886 , training accuracy :  0.818182\n",
      "i: 4900 , cost :  0.449568 , training accuracy :  0.811448\n",
      "i: 5000 , cost :  0.551653 , training accuracy :  0.765432\n",
      "i: 5100 , cost :  0.406732 , training accuracy :  0.822671\n",
      "i: 5200 , cost :  0.484062 , training accuracy :  0.780022\n",
      "i: 5300 , cost :  0.42812 , training accuracy :  0.800224\n",
      "i: 5400 , cost :  0.431496 , training accuracy :  0.824916\n",
      "i: 5500 , cost :  0.498654 , training accuracy :  0.790123\n",
      "i: 5600 , cost :  0.491534 , training accuracy :  0.794613\n",
      "i: 5700 , cost :  0.443809 , training accuracy :  0.814815\n",
      "i: 5800 , cost :  0.411097 , training accuracy :  0.823793\n",
      "i: 5900 , cost :  0.47249 , training accuracy :  0.785634\n",
      "i: 6000 , cost :  0.452771 , training accuracy :  0.784512\n",
      "i: 6100 , cost :  0.405808 , training accuracy :  0.822671\n",
      "i: 6200 , cost :  0.436955 , training accuracy :  0.823793\n",
      "i: 6300 , cost :  0.437731 , training accuracy :  0.822671\n",
      "i: 6400 , cost :  0.404815 , training accuracy :  0.824916\n",
      "i: 6500 , cost :  0.408128 , training accuracy :  0.826038\n",
      "i: 6600 , cost :  0.84539 , training accuracy :  0.689113\n",
      "i: 6700 , cost :  0.426579 , training accuracy :  0.815937\n",
      "i: 6800 , cost :  0.402566 , training accuracy :  0.823793\n",
      "i: 6900 , cost :  0.47369 , training accuracy :  0.79798\n",
      "i: 7000 , cost :  0.479507 , training accuracy :  0.800224\n",
      "i: 7100 , cost :  0.471324 , training accuracy :  0.806958\n",
      "i: 7200 , cost :  0.404576 , training accuracy :  0.828283\n",
      "i: 7300 , cost :  0.43042 , training accuracy :  0.801347\n",
      "i: 7400 , cost :  0.47441 , training accuracy :  0.785634\n",
      "i: 7500 , cost :  0.431885 , training accuracy :  0.799102\n",
      "i: 7600 , cost :  0.4074 , training accuracy :  0.828283\n",
      "i: 7700 , cost :  0.462044 , training accuracy :  0.814815\n",
      "i: 7800 , cost :  0.531799 , training accuracy :  0.783389\n",
      "i: 7900 , cost :  0.475215 , training accuracy :  0.805836\n",
      "i: 8000 , cost :  0.424758 , training accuracy :  0.826038\n",
      "i: 8100 , cost :  0.41279 , training accuracy :  0.826038\n",
      "i: 8200 , cost :  0.495434 , training accuracy :  0.803591\n",
      "i: 8300 , cost :  0.41059 , training accuracy :  0.826038\n",
      "i: 8400 , cost :  0.40548 , training accuracy :  0.82716\n",
      "i: 8500 , cost :  0.408647 , training accuracy :  0.82716\n",
      "i: 8600 , cost :  0.439798 , training accuracy :  0.794613\n",
      "i: 8700 , cost :  0.427103 , training accuracy :  0.803591\n",
      "i: 8800 , cost :  0.417693 , training accuracy :  0.814815\n",
      "i: 8900 , cost :  0.448809 , training accuracy :  0.789001\n",
      "i: 9000 , cost :  0.500468 , training accuracy :  0.792368\n",
      "i: 9100 , cost :  0.459136 , training accuracy :  0.813693\n",
      "i: 9200 , cost :  0.513357 , training accuracy :  0.791246\n",
      "i: 9300 , cost :  0.515719 , training accuracy :  0.790123\n",
      "i: 9400 , cost :  0.521003 , training accuracy :  0.787879\n",
      "i: 9500 , cost :  0.490428 , training accuracy :  0.79798\n",
      "i: 9600 , cost :  0.479684 , training accuracy :  0.808081\n",
      "i: 9700 , cost :  0.446714 , training accuracy :  0.817059\n",
      "i: 9800 , cost :  0.4312 , training accuracy :  0.823793\n",
      "i: 9900 , cost :  0.413274 , training accuracy :  0.828283\n",
      "Final training accuracy :  0.828283\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 10000 ][hidden: 10 ][file: bhavul_tr_acc_0.83_prediction.csv ] ACCURACY :  0.828283\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  157.297 , training accuracy :  0.643098\n",
      "i: 100 , cost :  1.15174 , training accuracy :  0.690236\n",
      "i: 200 , cost :  0.572772 , training accuracy :  0.7789\n",
      "i: 300 , cost :  0.477264 , training accuracy :  0.810326\n",
      "i: 400 , cost :  0.468587 , training accuracy :  0.791246\n",
      "i: 500 , cost :  0.488182 , training accuracy :  0.780022\n",
      "i: 600 , cost :  0.469911 , training accuracy :  0.7789\n",
      "i: 700 , cost :  0.447498 , training accuracy :  0.791246\n",
      "i: 800 , cost :  0.433265 , training accuracy :  0.799102\n",
      "i: 900 , cost :  0.427142 , training accuracy :  0.806958\n",
      "i: 1000 , cost :  0.428397 , training accuracy :  0.813693\n",
      "i: 1100 , cost :  0.467974 , training accuracy :  0.802469\n",
      "i: 1200 , cost :  0.451214 , training accuracy :  0.805836\n",
      "i: 1300 , cost :  0.436164 , training accuracy :  0.817059\n",
      "i: 1400 , cost :  0.447378 , training accuracy :  0.786756\n",
      "i: 1500 , cost :  0.419032 , training accuracy :  0.814815\n",
      "i: 1600 , cost :  0.444642 , training accuracy :  0.791246\n",
      "i: 1700 , cost :  0.4467 , training accuracy :  0.810326\n",
      "i: 1800 , cost :  0.465465 , training accuracy :  0.804714\n",
      "i: 1900 , cost :  0.44196 , training accuracy :  0.794613\n",
      "i: 2000 , cost :  0.501155 , training accuracy :  0.786756\n",
      "i: 2100 , cost :  0.433803 , training accuracy :  0.795735\n",
      "i: 2200 , cost :  0.446886 , training accuracy :  0.792368\n",
      "i: 2300 , cost :  0.412241 , training accuracy :  0.810326\n",
      "i: 2400 , cost :  0.475999 , training accuracy :  0.783389\n",
      "i: 2500 , cost :  0.426877 , training accuracy :  0.801347\n",
      "i: 2600 , cost :  0.40858 , training accuracy :  0.81257\n",
      "i: 2700 , cost :  0.44683 , training accuracy :  0.81257\n",
      "i: 2800 , cost :  0.446001 , training accuracy :  0.813693\n",
      "i: 2900 , cost :  0.406538 , training accuracy :  0.823793\n",
      "i: 3000 , cost :  0.441232 , training accuracy :  0.789001\n",
      "i: 3100 , cost :  0.437589 , training accuracy :  0.79349\n",
      "i: 3200 , cost :  0.406192 , training accuracy :  0.815937\n",
      "i: 3300 , cost :  0.463826 , training accuracy :  0.808081\n",
      "i: 3400 , cost :  0.430627 , training accuracy :  0.823793\n",
      "i: 3500 , cost :  0.412988 , training accuracy :  0.809203\n",
      "i: 3600 , cost :  0.44157 , training accuracy :  0.79798\n",
      "i: 3700 , cost :  0.40406 , training accuracy :  0.814815\n",
      "i: 3800 , cost :  0.444384 , training accuracy :  0.823793\n",
      "i: 3900 , cost :  0.4219 , training accuracy :  0.82716\n",
      "i: 4000 , cost :  0.407622 , training accuracy :  0.806958\n",
      "i: 4100 , cost :  0.431575 , training accuracy :  0.800224\n",
      "i: 4200 , cost :  0.40397 , training accuracy :  0.839506\n",
      "i: 4300 , cost :  0.449004 , training accuracy :  0.818182\n",
      "i: 4400 , cost :  0.397921 , training accuracy :  0.83165\n",
      "i: 4500 , cost :  0.436948 , training accuracy :  0.79798\n",
      "i: 4600 , cost :  0.400361 , training accuracy :  0.817059\n",
      "i: 4700 , cost :  0.447023 , training accuracy :  0.819304\n",
      "i: 4800 , cost :  0.408942 , training accuracy :  0.840629\n",
      "i: 4900 , cost :  0.428364 , training accuracy :  0.791246\n",
      "i: 5000 , cost :  0.415987 , training accuracy :  0.809203\n",
      "i: 5100 , cost :  0.423006 , training accuracy :  0.833894\n",
      "i: 5200 , cost :  0.43058 , training accuracy :  0.830527\n",
      "i: 5300 , cost :  0.411519 , training accuracy :  0.808081\n",
      "i: 5400 , cost :  0.415905 , training accuracy :  0.809203\n",
      "i: 5500 , cost :  0.408669 , training accuracy :  0.838384\n",
      "i: 5600 , cost :  0.427699 , training accuracy :  0.830527\n",
      "i: 5700 , cost :  0.414758 , training accuracy :  0.804714\n",
      "i: 5800 , cost :  0.423127 , training accuracy :  0.803591\n",
      "i: 5900 , cost :  0.402465 , training accuracy :  0.837261\n",
      "i: 6000 , cost :  0.442393 , training accuracy :  0.829405\n",
      "i: 6100 , cost :  0.39658 , training accuracy :  0.822671\n",
      "i: 6200 , cost :  0.436034 , training accuracy :  0.79349\n",
      "i: 6300 , cost :  0.397781 , training accuracy :  0.817059\n",
      "i: 6400 , cost :  0.428455 , training accuracy :  0.79349\n",
      "i: 6500 , cost :  0.462393 , training accuracy :  0.808081\n",
      "i: 6600 , cost :  0.414127 , training accuracy :  0.835017\n",
      "i: 6700 , cost :  0.442745 , training accuracy :  0.789001\n",
      "i: 6800 , cost :  0.400987 , training accuracy :  0.817059\n",
      "i: 6900 , cost :  0.44332 , training accuracy :  0.818182\n",
      "i: 7000 , cost :  0.420206 , training accuracy :  0.804714\n",
      "i: 7100 , cost :  0.461951 , training accuracy :  0.805836\n",
      "i: 7200 , cost :  0.400239 , training accuracy :  0.826038\n",
      "i: 7300 , cost :  0.435082 , training accuracy :  0.792368\n",
      "i: 7400 , cost :  0.419843 , training accuracy :  0.806958\n",
      "i: 7500 , cost :  0.425034 , training accuracy :  0.83165\n",
      "i: 7600 , cost :  0.414618 , training accuracy :  0.832772\n",
      "i: 7700 , cost :  0.444109 , training accuracy :  0.792368\n",
      "i: 7800 , cost :  0.40573 , training accuracy :  0.809203\n",
      "i: 7900 , cost :  0.404996 , training accuracy :  0.83165\n",
      "i: 8000 , cost :  0.433771 , training accuracy :  0.792368\n",
      "i: 8100 , cost :  0.401514 , training accuracy :  0.814815\n",
      "i: 8200 , cost :  0.404513 , training accuracy :  0.833894\n",
      "i: 8300 , cost :  0.446653 , training accuracy :  0.813693\n",
      "i: 8400 , cost :  0.429401 , training accuracy :  0.826038\n",
      "i: 8500 , cost :  0.39871 , training accuracy :  0.818182\n",
      "i: 8600 , cost :  0.420135 , training accuracy :  0.829405\n",
      "i: 8700 , cost :  0.451638 , training accuracy :  0.810326\n",
      "i: 8800 , cost :  0.436286 , training accuracy :  0.826038\n",
      "i: 8900 , cost :  0.406247 , training accuracy :  0.830527\n",
      "i: 9000 , cost :  0.400716 , training accuracy :  0.819304\n",
      "i: 9100 , cost :  0.438594 , training accuracy :  0.791246\n",
      "i: 9200 , cost :  0.407961 , training accuracy :  0.832772\n",
      "i: 9300 , cost :  0.402987 , training accuracy :  0.813693\n",
      "i: 9400 , cost :  0.434115 , training accuracy :  0.79349\n",
      "i: 9500 , cost :  0.394892 , training accuracy :  0.821549\n",
      "i: 9600 , cost :  0.418032 , training accuracy :  0.826038\n",
      "i: 9700 , cost :  0.427705 , training accuracy :  0.803591\n",
      "i: 9800 , cost :  0.447443 , training accuracy :  0.810326\n",
      "i: 9900 , cost :  0.435492 , training accuracy :  0.796857\n",
      "Final training accuracy :  0.81257\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 10000 ][hidden: 15 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.81257\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1306.25 , training accuracy :  0.37486\n",
      "i: 100 , cost :  11.8983 , training accuracy :  0.673401\n",
      "i: 200 , cost :  3.25116 , training accuracy :  0.718294\n",
      "i: 300 , cost :  3.05886 , training accuracy :  0.741863\n",
      "i: 400 , cost :  1.71724 , training accuracy :  0.783389\n",
      "i: 500 , cost :  1.63757 , training accuracy :  0.803591\n",
      "i: 600 , cost :  1.28503 , training accuracy :  0.800224\n",
      "i: 700 , cost :  2.64208 , training accuracy :  0.75982\n",
      "i: 800 , cost :  1.07249 , training accuracy :  0.803591\n",
      "i: 900 , cost :  2.60473 , training accuracy :  0.763187\n",
      "i: 1000 , cost :  1.32134 , training accuracy :  0.804714\n",
      "i: 1100 , cost :  0.780395 , training accuracy :  0.822671\n",
      "i: 1200 , cost :  2.09851 , training accuracy :  0.690236\n",
      "i: 1300 , cost :  0.775957 , training accuracy :  0.829405\n",
      "i: 1400 , cost :  1.56324 , training accuracy :  0.801347\n",
      "i: 1500 , cost :  1.05768 , training accuracy :  0.83165\n",
      "i: 1600 , cost :  0.671174 , training accuracy :  0.840629\n",
      "i: 1700 , cost :  4.62551 , training accuracy :  0.519641\n",
      "i: 1800 , cost :  0.659907 , training accuracy :  0.855219\n",
      "i: 1900 , cost :  1.01936 , training accuracy :  0.833894\n",
      "i: 2000 , cost :  0.621904 , training accuracy :  0.858586\n",
      "i: 2100 , cost :  3.08841 , training accuracy :  0.753086\n",
      "i: 2200 , cost :  0.562494 , training accuracy :  0.85073\n",
      "i: 2300 , cost :  2.62174 , training accuracy :  0.780022\n",
      "i: 2400 , cost :  0.579412 , training accuracy :  0.856341\n",
      "i: 2500 , cost :  1.43128 , training accuracy :  0.804714\n",
      "i: 2600 , cost :  1.03662 , training accuracy :  0.82716\n",
      "i: 2700 , cost :  0.531028 , training accuracy :  0.860831\n",
      "i: 2800 , cost :  1.01466 , training accuracy :  0.832772\n",
      "i: 2900 , cost :  0.509166 , training accuracy :  0.854097\n",
      "i: 3000 , cost :  1.14414 , training accuracy :  0.783389\n",
      "i: 3100 , cost :  0.499156 , training accuracy :  0.859708\n",
      "i: 3200 , cost :  3.70057 , training accuracy :  0.727273\n",
      "i: 3300 , cost :  0.561547 , training accuracy :  0.86532\n",
      "i: 3400 , cost :  1.74674 , training accuracy :  0.801347\n",
      "i: 3500 , cost :  0.515339 , training accuracy :  0.868687\n",
      "i: 3600 , cost :  0.894177 , training accuracy :  0.848485\n",
      "i: 3700 , cost :  2.17881 , training accuracy :  0.775533\n",
      "i: 3800 , cost :  0.573593 , training accuracy :  0.873176\n",
      "i: 3900 , cost :  2.41665 , training accuracy :  0.762065\n",
      "i: 4000 , cost :  0.613012 , training accuracy :  0.863075\n",
      "i: 4100 , cost :  0.772312 , training accuracy :  0.826038\n",
      "i: 4200 , cost :  2.22497 , training accuracy :  0.646465\n",
      "i: 4300 , cost :  0.520567 , training accuracy :  0.866442\n",
      "i: 4400 , cost :  2.0161 , training accuracy :  0.619529\n",
      "i: 4500 , cost :  0.644694 , training accuracy :  0.854097\n",
      "i: 4600 , cost :  2.56647 , training accuracy :  0.784512\n",
      "i: 4700 , cost :  0.485555 , training accuracy :  0.870932\n",
      "i: 4800 , cost :  3.04629 , training accuracy :  0.558923\n",
      "i: 4900 , cost :  1.06902 , training accuracy :  0.82716\n",
      "i: 5000 , cost :  0.440361 , training accuracy :  0.872054\n",
      "i: 5100 , cost :  0.456155 , training accuracy :  0.876543\n",
      "i: 5200 , cost :  1.50694 , training accuracy :  0.6633\n",
      "i: 5300 , cost :  0.417525 , training accuracy :  0.86532\n",
      "i: 5400 , cost :  0.483062 , training accuracy :  0.867565\n",
      "i: 5500 , cost :  1.15931 , training accuracy :  0.822671\n",
      "i: 5600 , cost :  0.398419 , training accuracy :  0.877666\n",
      "i: 5700 , cost :  0.836274 , training accuracy :  0.818182\n",
      "i: 5800 , cost :  0.713756 , training accuracy :  0.820426\n",
      "i: 5900 , cost :  0.487889 , training accuracy :  0.867565\n",
      "i: 6000 , cost :  0.692936 , training accuracy :  0.852974\n",
      "i: 6100 , cost :  0.408929 , training accuracy :  0.873176\n",
      "i: 6200 , cost :  1.41775 , training accuracy :  0.818182\n",
      "i: 6300 , cost :  0.395535 , training accuracy :  0.872054\n",
      "i: 6400 , cost :  2.63634 , training accuracy :  0.611672\n",
      "i: 6500 , cost :  0.421524 , training accuracy :  0.876543\n",
      "i: 6600 , cost :  1.02235 , training accuracy :  0.832772\n",
      "i: 6700 , cost :  0.414479 , training accuracy :  0.858586\n",
      "i: 6800 , cost :  0.515162 , training accuracy :  0.881033\n",
      "i: 6900 , cost :  5.55767 , training accuracy :  0.439955\n",
      "i: 7000 , cost :  0.485884 , training accuracy :  0.869809\n",
      "i: 7100 , cost :  0.804493 , training accuracy :  0.829405\n",
      "i: 7200 , cost :  0.442375 , training accuracy :  0.869809\n",
      "i: 7300 , cost :  3.1392 , training accuracy :  0.76431\n",
      "i: 7400 , cost :  0.399472 , training accuracy :  0.882155\n",
      "i: 7500 , cost :  0.51751 , training accuracy :  0.877666\n",
      "i: 7600 , cost :  0.674929 , training accuracy :  0.832772\n",
      "i: 7700 , cost :  0.41326 , training accuracy :  0.8844\n",
      "i: 7800 , cost :  0.654257 , training accuracy :  0.867565\n",
      "i: 7900 , cost :  1.52723 , training accuracy :  0.803591\n",
      "i: 8000 , cost :  0.408075 , training accuracy :  0.883277\n",
      "i: 8100 , cost :  0.754832 , training accuracy :  0.859708\n",
      "i: 8200 , cost :  4.76161 , training accuracy :  0.722783\n",
      "i: 8300 , cost :  0.452767 , training accuracy :  0.882155\n",
      "i: 8400 , cost :  1.45562 , training accuracy :  0.809203\n",
      "i: 8500 , cost :  0.501852 , training accuracy :  0.860831\n",
      "i: 8600 , cost :  3.61428 , training accuracy :  0.748597\n",
      "i: 8700 , cost :  0.389209 , training accuracy :  0.863075\n",
      "i: 8800 , cost :  0.535628 , training accuracy :  0.872054\n",
      "i: 8900 , cost :  2.63097 , training accuracy :  0.777778\n",
      "i: 9000 , cost :  0.37198 , training accuracy :  0.886644\n",
      "i: 9100 , cost :  0.412954 , training accuracy :  0.875421\n",
      "i: 9200 , cost :  0.684707 , training accuracy :  0.856341\n",
      "i: 9300 , cost :  0.950966 , training accuracy :  0.841751\n",
      "i: 9400 , cost :  0.483432 , training accuracy :  0.877666\n",
      "i: 9500 , cost :  3.85477 , training accuracy :  0.542088\n",
      "i: 9600 , cost :  0.386577 , training accuracy :  0.8844\n",
      "i: 9700 , cost :  0.555281 , training accuracy :  0.874299\n",
      "i: 9800 , cost :  1.6171 , training accuracy :  0.813693\n",
      "i: 9900 , cost :  0.442252 , training accuracy :  0.870932\n",
      "Final training accuracy :  0.885522\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 10000 ][hidden: 50 ][file: bhavul_tr_acc_0.89_prediction.csv ] ACCURACY :  0.885522\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1218.48 , training accuracy :  0.383838\n",
      "i: 100 , cost :  4.49229 , training accuracy :  0.750842\n",
      "i: 200 , cost :  2.78548 , training accuracy :  0.780022\n",
      "i: 300 , cost :  3.65658 , training accuracy :  0.756453\n",
      "i: 400 , cost :  18.5005 , training accuracy :  0.69248\n",
      "i: 500 , cost :  4.35742 , training accuracy :  0.721661\n",
      "i: 600 , cost :  2.13825 , training accuracy :  0.765432\n",
      "i: 700 , cost :  5.04548 , training accuracy :  0.749719\n",
      "i: 800 , cost :  8.73712 , training accuracy :  0.749719\n",
      "i: 900 , cost :  2.06905 , training accuracy :  0.832772\n",
      "i: 1000 , cost :  1.37507 , training accuracy :  0.832772\n",
      "i: 1100 , cost :  17.7842 , training accuracy :  0.720539\n",
      "i: 1200 , cost :  4.38729 , training accuracy :  0.762065\n",
      "i: 1300 , cost :  1.37015 , training accuracy :  0.847363\n",
      "i: 1400 , cost :  4.59123 , training accuracy :  0.802469\n",
      "i: 1500 , cost :  1.62041 , training accuracy :  0.826038\n",
      "i: 1600 , cost :  8.96885 , training accuracy :  0.481481\n",
      "i: 1700 , cost :  1.19933 , training accuracy :  0.854097\n",
      "i: 1800 , cost :  8.72839 , training accuracy :  0.546577\n",
      "i: 1900 , cost :  1.15244 , training accuracy :  0.843996\n",
      "i: 2000 , cost :  18.1631 , training accuracy :  0.421998\n",
      "i: 2100 , cost :  1.1555 , training accuracy :  0.860831\n",
      "i: 2200 , cost :  8.72209 , training accuracy :  0.506173\n",
      "i: 2300 , cost :  1.04556 , training accuracy :  0.866442\n",
      "i: 2400 , cost :  7.27886 , training accuracy :  0.765432\n",
      "i: 2500 , cost :  1.00428 , training accuracy :  0.869809\n",
      "i: 2600 , cost :  1.64899 , training accuracy :  0.836139\n",
      "i: 2700 , cost :  4.20582 , training accuracy :  0.754209\n",
      "i: 2800 , cost :  5.76923 , training accuracy :  0.531987\n",
      "i: 2900 , cost :  6.52142 , training accuracy :  0.766554\n",
      "i: 3000 , cost :  1.39887 , training accuracy :  0.845118\n",
      "i: 3100 , cost :  3.76253 , training accuracy :  0.820426\n",
      "i: 3200 , cost :  1.20194 , training accuracy :  0.870932\n",
      "i: 3300 , cost :  2.064 , training accuracy :  0.833894\n",
      "i: 3400 , cost :  1.00787 , training accuracy :  0.87991\n",
      "i: 3500 , cost :  11.2032 , training accuracy :  0.483726\n",
      "i: 3600 , cost :  1.2222 , training accuracy :  0.875421\n",
      "i: 3700 , cost :  1.74758 , training accuracy :  0.824916\n",
      "i: 3800 , cost :  0.913509 , training accuracy :  0.875421\n",
      "i: 3900 , cost :  1.49359 , training accuracy :  0.858586\n",
      "i: 4000 , cost :  3.86677 , training accuracy :  0.809203\n",
      "i: 4100 , cost :  1.33516 , training accuracy :  0.872054\n",
      "i: 4200 , cost :  0.958194 , training accuracy :  0.855219\n",
      "i: 4300 , cost :  1.20052 , training accuracy :  0.870932\n",
      "i: 4400 , cost :  3.44746 , training accuracy :  0.821549\n",
      "i: 4500 , cost :  1.02319 , training accuracy :  0.886644\n",
      "i: 4600 , cost :  19.2644 , training accuracy :  0.42312\n",
      "i: 4700 , cost :  1.00248 , training accuracy :  0.882155\n",
      "i: 4800 , cost :  3.38084 , training accuracy :  0.801347\n",
      "i: 4900 , cost :  1.02126 , training accuracy :  0.8844\n",
      "i: 5000 , cost :  2.69718 , training accuracy :  0.824916\n",
      "i: 5100 , cost :  2.08351 , training accuracy :  0.85073\n",
      "i: 5200 , cost :  1.08727 , training accuracy :  0.881033\n",
      "i: 5300 , cost :  16.1118 , training accuracy :  0.438833\n",
      "i: 5400 , cost :  0.869251 , training accuracy :  0.868687\n",
      "i: 5500 , cost :  0.748125 , training accuracy :  0.882155\n",
      "i: 5600 , cost :  4.7748 , training accuracy :  0.682379\n",
      "i: 5700 , cost :  0.77399 , training accuracy :  0.883277\n",
      "i: 5800 , cost :  1.72872 , training accuracy :  0.85073\n",
      "i: 5900 , cost :  1.1758 , training accuracy :  0.877666\n",
      "i: 6000 , cost :  0.900663 , training accuracy :  0.863075\n",
      "i: 6100 , cost :  4.28619 , training accuracy :  0.828283\n",
      "i: 6200 , cost :  1.03948 , training accuracy :  0.8844\n",
      "i: 6300 , cost :  3.49231 , training accuracy :  0.716049\n",
      "i: 6400 , cost :  0.932459 , training accuracy :  0.885522\n",
      "i: 6500 , cost :  8.77854 , training accuracy :  0.795735\n",
      "i: 6600 , cost :  0.953979 , training accuracy :  0.885522\n",
      "i: 6700 , cost :  4.84331 , training accuracy :  0.783389\n",
      "i: 6800 , cost :  0.98188 , training accuracy :  0.885522\n",
      "i: 6900 , cost :  8.32414 , training accuracy :  0.763187\n",
      "i: 7000 , cost :  3.77107 , training accuracy :  0.830527\n",
      "i: 7100 , cost :  1.57267 , training accuracy :  0.870932\n",
      "i: 7200 , cost :  0.838684 , training accuracy :  0.888889\n",
      "i: 7300 , cost :  1.63168 , training accuracy :  0.84624\n",
      "i: 7400 , cost :  0.992447 , training accuracy :  0.854097\n",
      "i: 7500 , cost :  5.45432 , training accuracy :  0.820426\n",
      "i: 7600 , cost :  0.956819 , training accuracy :  0.897868\n",
      "i: 7700 , cost :  3.52225 , training accuracy :  0.829405\n",
      "i: 7800 , cost :  16.7914 , training accuracy :  0.716049\n",
      "i: 7900 , cost :  1.00467 , training accuracy :  0.881033\n",
      "i: 8000 , cost :  0.711174 , training accuracy :  0.89899\n",
      "i: 8100 , cost :  2.32488 , training accuracy :  0.837261\n",
      "i: 8200 , cost :  2.65133 , training accuracy :  0.79349\n",
      "i: 8300 , cost :  0.989731 , training accuracy :  0.893378\n",
      "i: 8400 , cost :  0.939188 , training accuracy :  0.851852\n",
      "i: 8500 , cost :  5.21183 , training accuracy :  0.783389\n",
      "i: 8600 , cost :  2.39138 , training accuracy :  0.847363\n",
      "i: 8700 , cost :  1.80566 , training accuracy :  0.856341\n",
      "i: 8800 , cost :  1.81872 , training accuracy :  0.861953\n",
      "i: 8900 , cost :  0.935897 , training accuracy :  0.900112\n",
      "i: 9000 , cost :  0.809771 , training accuracy :  0.86532\n",
      "i: 9100 , cost :  4.41448 , training accuracy :  0.82716\n",
      "i: 9200 , cost :  1.14435 , training accuracy :  0.847363\n",
      "i: 9300 , cost :  13.7179 , training accuracy :  0.744108\n",
      "i: 9400 , cost :  0.805934 , training accuracy :  0.904602\n",
      "i: 9500 , cost :  4.70659 , training accuracy :  0.783389\n",
      "i: 9600 , cost :  0.999809 , training accuracy :  0.895623\n",
      "i: 9700 , cost :  21.6762 , training accuracy :  0.427609\n",
      "i: 9800 , cost :  1.03966 , training accuracy :  0.892256\n",
      "i: 9900 , cost :  0.641364 , training accuracy :  0.907969\n",
      "Final training accuracy :  0.878788\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 10000 ][hidden: 100 ][file: bhavul_tr_acc_0.88_prediction.csv ] ACCURACY :  0.878788\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhavul.g/.virtualenvs/ailearn/lib/python3.5/site-packages/ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  710.496 , training accuracy :  0.37486\n",
      "i: 100 , cost :  62.9678 , training accuracy :  0.59596\n",
      "i: 200 , cost :  18.1675 , training accuracy :  0.652076\n",
      "i: 300 , cost :  1.61923 , training accuracy :  0.689113\n",
      "i: 400 , cost :  1.12122 , training accuracy :  0.753086\n",
      "i: 500 , cost :  0.877217 , training accuracy :  0.75982\n",
      "i: 600 , cost :  0.721939 , training accuracy :  0.771044\n",
      "i: 700 , cost :  0.646359 , training accuracy :  0.7789\n",
      "i: 800 , cost :  0.601009 , training accuracy :  0.784512\n",
      "i: 900 , cost :  0.571206 , training accuracy :  0.790123\n",
      "i: 1000 , cost :  0.553248 , training accuracy :  0.781145\n",
      "i: 1100 , cost :  0.535586 , training accuracy :  0.765432\n",
      "i: 1200 , cost :  0.510556 , training accuracy :  0.769921\n",
      "i: 1300 , cost :  0.511724 , training accuracy :  0.765432\n",
      "i: 1400 , cost :  0.498868 , training accuracy :  0.765432\n",
      "i: 1500 , cost :  0.48831 , training accuracy :  0.794613\n",
      "i: 1600 , cost :  0.48656 , training accuracy :  0.791246\n",
      "i: 1700 , cost :  0.498589 , training accuracy :  0.765432\n",
      "i: 1800 , cost :  0.475188 , training accuracy :  0.79349\n",
      "i: 1900 , cost :  0.469451 , training accuracy :  0.795735\n",
      "i: 2000 , cost :  0.491141 , training accuracy :  0.760943\n",
      "i: 2100 , cost :  0.505312 , training accuracy :  0.785634\n",
      "i: 2200 , cost :  0.485327 , training accuracy :  0.765432\n",
      "i: 2300 , cost :  0.477093 , training accuracy :  0.795735\n",
      "i: 2400 , cost :  0.464822 , training accuracy :  0.776655\n",
      "i: 2500 , cost :  0.460297 , training accuracy :  0.799102\n",
      "i: 2600 , cost :  0.457317 , training accuracy :  0.786756\n",
      "i: 2700 , cost :  0.456181 , training accuracy :  0.803591\n",
      "i: 2800 , cost :  0.456211 , training accuracy :  0.786756\n",
      "i: 2900 , cost :  0.457839 , training accuracy :  0.800224\n",
      "i: 3000 , cost :  0.458626 , training accuracy :  0.785634\n",
      "i: 3100 , cost :  0.461579 , training accuracy :  0.805836\n",
      "i: 3200 , cost :  0.463245 , training accuracy :  0.780022\n",
      "i: 3300 , cost :  0.463854 , training accuracy :  0.803591\n",
      "i: 3400 , cost :  0.463654 , training accuracy :  0.783389\n",
      "i: 3500 , cost :  0.469226 , training accuracy :  0.803591\n",
      "i: 3600 , cost :  0.467284 , training accuracy :  0.784512\n",
      "i: 3700 , cost :  0.475422 , training accuracy :  0.802469\n",
      "i: 3800 , cost :  0.473014 , training accuracy :  0.781145\n",
      "i: 3900 , cost :  0.482572 , training accuracy :  0.796857\n",
      "i: 4000 , cost :  0.478582 , training accuracy :  0.780022\n",
      "i: 4100 , cost :  0.497065 , training accuracy :  0.796857\n",
      "i: 4200 , cost :  0.482723 , training accuracy :  0.777778\n",
      "i: 4300 , cost :  0.479963 , training accuracy :  0.802469\n",
      "i: 4400 , cost :  0.459361 , training accuracy :  0.786756\n",
      "i: 4500 , cost :  0.448811 , training accuracy :  0.805836\n",
      "i: 4600 , cost :  0.44873 , training accuracy :  0.811448\n",
      "i: 4700 , cost :  0.456494 , training accuracy :  0.784512\n",
      "i: 4800 , cost :  0.47114 , training accuracy :  0.802469\n",
      "i: 4900 , cost :  0.476255 , training accuracy :  0.781145\n",
      "i: 5000 , cost :  0.495634 , training accuracy :  0.796857\n",
      "i: 5100 , cost :  0.481726 , training accuracy :  0.7789\n",
      "i: 5200 , cost :  0.4737 , training accuracy :  0.804714\n",
      "i: 5300 , cost :  0.451559 , training accuracy :  0.790123\n",
      "i: 5400 , cost :  0.446938 , training accuracy :  0.796857\n",
      "i: 5500 , cost :  0.453672 , training accuracy :  0.806958\n",
      "i: 5600 , cost :  0.463836 , training accuracy :  0.786756\n",
      "i: 5700 , cost :  0.486806 , training accuracy :  0.800224\n",
      "i: 5800 , cost :  0.483008 , training accuracy :  0.777778\n",
      "i: 5900 , cost :  0.487692 , training accuracy :  0.801347\n",
      "i: 6000 , cost :  0.460623 , training accuracy :  0.787879\n",
      "i: 6100 , cost :  0.446957 , training accuracy :  0.799102\n",
      "i: 6200 , cost :  0.450525 , training accuracy :  0.804714\n",
      "i: 6300 , cost :  0.460341 , training accuracy :  0.789001\n",
      "i: 6400 , cost :  0.480173 , training accuracy :  0.800224\n",
      "i: 6500 , cost :  0.48238 , training accuracy :  0.7789\n",
      "i: 6600 , cost :  0.488541 , training accuracy :  0.802469\n",
      "i: 6700 , cost :  0.460939 , training accuracy :  0.789001\n",
      "i: 6800 , cost :  0.446587 , training accuracy :  0.79798\n",
      "i: 6900 , cost :  0.451464 , training accuracy :  0.806958\n",
      "i: 7000 , cost :  0.462357 , training accuracy :  0.787879\n",
      "i: 7100 , cost :  0.486783 , training accuracy :  0.800224\n",
      "i: 7200 , cost :  0.482302 , training accuracy :  0.7789\n",
      "i: 7300 , cost :  0.475605 , training accuracy :  0.803591\n",
      "i: 7400 , cost :  0.449791 , training accuracy :  0.789001\n",
      "i: 7500 , cost :  0.447724 , training accuracy :  0.792368\n",
      "i: 7600 , cost :  0.461836 , training accuracy :  0.805836\n",
      "i: 7700 , cost :  0.475708 , training accuracy :  0.781145\n",
      "i: 7800 , cost :  0.494207 , training accuracy :  0.799102\n",
      "i: 7900 , cost :  0.475159 , training accuracy :  0.780022\n",
      "i: 8000 , cost :  0.4577 , training accuracy :  0.803591\n",
      "i: 8100 , cost :  0.445603 , training accuracy :  0.796857\n",
      "i: 8200 , cost :  0.45007 , training accuracy :  0.791246\n",
      "i: 8300 , cost :  0.463136 , training accuracy :  0.802469\n",
      "i: 8400 , cost :  0.472198 , training accuracy :  0.784512\n",
      "i: 8500 , cost :  0.493235 , training accuracy :  0.800224\n",
      "i: 8600 , cost :  0.476386 , training accuracy :  0.781145\n",
      "i: 8700 , cost :  0.462025 , training accuracy :  0.806958\n",
      "i: 8800 , cost :  0.445972 , training accuracy :  0.796857\n",
      "i: 8900 , cost :  0.447625 , training accuracy :  0.790123\n",
      "i: 9000 , cost :  0.462825 , training accuracy :  0.803591\n",
      "i: 9100 , cost :  0.472719 , training accuracy :  0.784512\n",
      "i: 9200 , cost :  0.492915 , training accuracy :  0.800224\n",
      "i: 9300 , cost :  0.473393 , training accuracy :  0.780022\n",
      "i: 9400 , cost :  0.455422 , training accuracy :  0.803591\n",
      "i: 9500 , cost :  0.444716 , training accuracy :  0.802469\n",
      "i: 9600 , cost :  0.452381 , training accuracy :  0.787879\n",
      "i: 9700 , cost :  0.471416 , training accuracy :  0.805836\n",
      "i: 9800 , cost :  0.477686 , training accuracy :  0.780022\n",
      "i: 9900 , cost :  0.489995 , training accuracy :  0.801347\n",
      "i: 10000 , cost :  0.461276 , training accuracy :  0.789001\n",
      "i: 10100 , cost :  0.445658 , training accuracy :  0.806958\n",
      "i: 10200 , cost :  0.448415 , training accuracy :  0.806958\n",
      "i: 10300 , cost :  0.461556 , training accuracy :  0.787879\n",
      "i: 10400 , cost :  0.485407 , training accuracy :  0.801347\n",
      "i: 10500 , cost :  0.478432 , training accuracy :  0.780022\n",
      "i: 10600 , cost :  0.469798 , training accuracy :  0.805836\n",
      "i: 10700 , cost :  0.446573 , training accuracy :  0.795735\n",
      "i: 10800 , cost :  0.446991 , training accuracy :  0.790123\n",
      "i: 10900 , cost :  0.46317 , training accuracy :  0.803591\n",
      "i: 11000 , cost :  0.474333 , training accuracy :  0.781145\n",
      "i: 11100 , cost :  0.493159 , training accuracy :  0.799102\n",
      "i: 11200 , cost :  0.466363 , training accuracy :  0.787879\n",
      "i: 11300 , cost :  0.449145 , training accuracy :  0.806958\n",
      "i: 11400 , cost :  0.445218 , training accuracy :  0.808081\n",
      "i: 11500 , cost :  0.456257 , training accuracy :  0.791246\n",
      "i: 11600 , cost :  0.476946 , training accuracy :  0.802469\n",
      "i: 11700 , cost :  0.479515 , training accuracy :  0.780022\n",
      "i: 11800 , cost :  0.481917 , training accuracy :  0.803591\n",
      "i: 11900 , cost :  0.453759 , training accuracy :  0.787879\n",
      "i: 12000 , cost :  0.444431 , training accuracy :  0.79798\n",
      "i: 12100 , cost :  0.452955 , training accuracy :  0.809203\n",
      "i: 12200 , cost :  0.466464 , training accuracy :  0.786756\n",
      "i: 12300 , cost :  0.489143 , training accuracy :  0.800224\n",
      "i: 12400 , cost :  0.475827 , training accuracy :  0.780022\n",
      "i: 12500 , cost :  0.460925 , training accuracy :  0.805836\n",
      "i: 12600 , cost :  0.445054 , training accuracy :  0.796857\n",
      "i: 12700 , cost :  0.44831 , training accuracy :  0.791246\n",
      "i: 12800 , cost :  0.464678 , training accuracy :  0.806958\n",
      "i: 12900 , cost :  0.47433 , training accuracy :  0.781145\n",
      "i: 13000 , cost :  0.491702 , training accuracy :  0.799102\n",
      "i: 13100 , cost :  0.467211 , training accuracy :  0.786756\n",
      "i: 13200 , cost :  0.449764 , training accuracy :  0.806958\n",
      "i: 13300 , cost :  0.444686 , training accuracy :  0.802469\n",
      "i: 13400 , cost :  0.452761 , training accuracy :  0.789001\n",
      "i: 13500 , cost :  0.471108 , training accuracy :  0.806958\n",
      "i: 13600 , cost :  0.4765 , training accuracy :  0.780022\n",
      "i: 13700 , cost :  0.489228 , training accuracy :  0.799102\n",
      "i: 13800 , cost :  0.46234 , training accuracy :  0.790123\n",
      "i: 13900 , cost :  0.447244 , training accuracy :  0.808081\n",
      "i: 14000 , cost :  0.445487 , training accuracy :  0.811448\n",
      "i: 14100 , cost :  0.455601 , training accuracy :  0.791246\n",
      "i: 14200 , cost :  0.474215 , training accuracy :  0.802469\n",
      "i: 14300 , cost :  0.477225 , training accuracy :  0.781145\n",
      "i: 14400 , cost :  0.485964 , training accuracy :  0.802469\n",
      "i: 14500 , cost :  0.460549 , training accuracy :  0.789001\n",
      "i: 14600 , cost :  0.447009 , training accuracy :  0.809203\n",
      "i: 14700 , cost :  0.445791 , training accuracy :  0.813693\n",
      "i: 14800 , cost :  0.455093 , training accuracy :  0.791246\n",
      "i: 14900 , cost :  0.471378 , training accuracy :  0.806958\n",
      "i: 15000 , cost :  0.475422 , training accuracy :  0.780022\n",
      "i: 15100 , cost :  0.489133 , training accuracy :  0.799102\n",
      "i: 15200 , cost :  0.464816 , training accuracy :  0.786756\n",
      "i: 15300 , cost :  0.451939 , training accuracy :  0.805836\n",
      "i: 15400 , cost :  0.444521 , training accuracy :  0.795735\n",
      "i: 15500 , cost :  0.446859 , training accuracy :  0.790123\n",
      "i: 15600 , cost :  0.460173 , training accuracy :  0.805836\n",
      "i: 15700 , cost :  0.469062 , training accuracy :  0.783389\n",
      "i: 15800 , cost :  0.487452 , training accuracy :  0.800224\n",
      "i: 15900 , cost :  0.476984 , training accuracy :  0.780022\n",
      "i: 16000 , cost :  0.479243 , training accuracy :  0.803591\n",
      "i: 16100 , cost :  0.459512 , training accuracy :  0.789001\n",
      "i: 16200 , cost :  0.449314 , training accuracy :  0.806958\n",
      "i: 16300 , cost :  0.444149 , training accuracy :  0.801347\n",
      "i: 16400 , cost :  0.448441 , training accuracy :  0.791246\n",
      "i: 16500 , cost :  0.461459 , training accuracy :  0.802469\n",
      "i: 16600 , cost :  0.468437 , training accuracy :  0.783389\n",
      "i: 16700 , cost :  0.4859 , training accuracy :  0.801347\n",
      "i: 16800 , cost :  0.477075 , training accuracy :  0.781145\n",
      "i: 16900 , cost :  0.480112 , training accuracy :  0.802469\n",
      "i: 17000 , cost :  0.460109 , training accuracy :  0.789001\n",
      "i: 17100 , cost :  0.450396 , training accuracy :  0.808081\n",
      "i: 17200 , cost :  0.444568 , training accuracy :  0.796857\n",
      "i: 17300 , cost :  0.445153 , training accuracy :  0.795735\n",
      "i: 17400 , cost :  0.450474 , training accuracy :  0.809203\n",
      "i: 17500 , cost :  0.456459 , training accuracy :  0.791246\n",
      "i: 17600 , cost :  0.468574 , training accuracy :  0.805836\n",
      "i: 17700 , cost :  0.468266 , training accuracy :  0.783389\n",
      "i: 17800 , cost :  0.481122 , training accuracy :  0.801347\n",
      "i: 17900 , cost :  0.475342 , training accuracy :  0.780022\n",
      "i: 18000 , cost :  0.489028 , training accuracy :  0.800224\n",
      "i: 18100 , cost :  0.474325 , training accuracy :  0.781145\n",
      "i: 18200 , cost :  0.478643 , training accuracy :  0.803591\n",
      "i: 18300 , cost :  0.468828 , training accuracy :  0.782267\n",
      "i: 18400 , cost :  0.481284 , training accuracy :  0.803591\n",
      "i: 18500 , cost :  0.47223 , training accuracy :  0.781145\n",
      "i: 18600 , cost :  0.484494 , training accuracy :  0.803591\n",
      "i: 18700 , cost :  0.473016 , training accuracy :  0.782267\n",
      "i: 18800 , cost :  0.483437 , training accuracy :  0.804714\n",
      "i: 18900 , cost :  0.469845 , training accuracy :  0.782267\n",
      "i: 19000 , cost :  0.477455 , training accuracy :  0.805836\n",
      "i: 19100 , cost :  0.466759 , training accuracy :  0.784512\n",
      "i: 19200 , cost :  0.473468 , training accuracy :  0.806958\n",
      "i: 19300 , cost :  0.463717 , training accuracy :  0.787879\n",
      "i: 19400 , cost :  0.46652 , training accuracy :  0.809203\n",
      "i: 19500 , cost :  0.461944 , training accuracy :  0.791246\n",
      "i: 19600 , cost :  0.463032 , training accuracy :  0.809203\n",
      "i: 19700 , cost :  0.461188 , training accuracy :  0.792368\n",
      "i: 19800 , cost :  0.464263 , training accuracy :  0.810326\n",
      "i: 19900 , cost :  0.461116 , training accuracy :  0.792368\n",
      "i: 20000 , cost :  0.465937 , training accuracy :  0.810326\n",
      "i: 20100 , cost :  0.46266 , training accuracy :  0.790123\n",
      "i: 20200 , cost :  0.469407 , training accuracy :  0.805836\n",
      "i: 20300 , cost :  0.465309 , training accuracy :  0.785634\n",
      "i: 20400 , cost :  0.474511 , training accuracy :  0.806958\n",
      "i: 20500 , cost :  0.467675 , training accuracy :  0.784512\n",
      "i: 20600 , cost :  0.479911 , training accuracy :  0.804714\n",
      "i: 20700 , cost :  0.470189 , training accuracy :  0.782267\n",
      "i: 20800 , cost :  0.484203 , training accuracy :  0.805836\n",
      "i: 20900 , cost :  0.472409 , training accuracy :  0.783389\n",
      "i: 21000 , cost :  0.484194 , training accuracy :  0.802469\n",
      "i: 21100 , cost :  0.473798 , training accuracy :  0.784512\n",
      "i: 21200 , cost :  0.485255 , training accuracy :  0.803591\n",
      "i: 21300 , cost :  0.472619 , training accuracy :  0.783389\n",
      "i: 21400 , cost :  0.483717 , training accuracy :  0.803591\n",
      "i: 21500 , cost :  0.471213 , training accuracy :  0.783389\n",
      "i: 21600 , cost :  0.48063 , training accuracy :  0.803591\n",
      "i: 21700 , cost :  0.469656 , training accuracy :  0.783389\n",
      "i: 21800 , cost :  0.476781 , training accuracy :  0.803591\n",
      "i: 21900 , cost :  0.468466 , training accuracy :  0.785634\n",
      "i: 22000 , cost :  0.474466 , training accuracy :  0.803591\n",
      "i: 22100 , cost :  0.466707 , training accuracy :  0.786756\n",
      "i: 22200 , cost :  0.47087 , training accuracy :  0.804714\n",
      "i: 22300 , cost :  0.463492 , training accuracy :  0.790123\n",
      "i: 22400 , cost :  0.46747 , training accuracy :  0.808081\n",
      "i: 22500 , cost :  0.462278 , training accuracy :  0.791246\n",
      "i: 22600 , cost :  0.466283 , training accuracy :  0.808081\n",
      "i: 22700 , cost :  0.460453 , training accuracy :  0.791246\n",
      "i: 22800 , cost :  0.465009 , training accuracy :  0.808081\n",
      "i: 22900 , cost :  0.459252 , training accuracy :  0.791246\n",
      "i: 23000 , cost :  0.461861 , training accuracy :  0.809203\n",
      "i: 23100 , cost :  0.457045 , training accuracy :  0.792368\n",
      "i: 23200 , cost :  0.457469 , training accuracy :  0.806958\n",
      "i: 23300 , cost :  0.453759 , training accuracy :  0.79349\n",
      "i: 23400 , cost :  0.453284 , training accuracy :  0.808081\n",
      "i: 23500 , cost :  0.45064 , training accuracy :  0.79349\n",
      "i: 23600 , cost :  0.450156 , training accuracy :  0.811448\n",
      "i: 23700 , cost :  0.446713 , training accuracy :  0.790123\n",
      "i: 23800 , cost :  0.445701 , training accuracy :  0.809203\n",
      "i: 23900 , cost :  0.443657 , training accuracy :  0.792368\n",
      "i: 24000 , cost :  0.442508 , training accuracy :  0.815937\n",
      "i: 24100 , cost :  0.441522 , training accuracy :  0.800224\n",
      "i: 24200 , cost :  0.440764 , training accuracy :  0.802469\n",
      "i: 24300 , cost :  0.440824 , training accuracy :  0.803591\n",
      "i: 24400 , cost :  0.441201 , training accuracy :  0.799102\n",
      "i: 24500 , cost :  0.442121 , training accuracy :  0.81257\n",
      "i: 24600 , cost :  0.443837 , training accuracy :  0.794613\n",
      "i: 24700 , cost :  0.445517 , training accuracy :  0.811448\n",
      "i: 24800 , cost :  0.448318 , training accuracy :  0.792368\n",
      "i: 24900 , cost :  0.453985 , training accuracy :  0.806958\n",
      "i: 25000 , cost :  0.454937 , training accuracy :  0.792368\n",
      "i: 25100 , cost :  0.464038 , training accuracy :  0.811448\n",
      "i: 25200 , cost :  0.463502 , training accuracy :  0.791246\n",
      "i: 25300 , cost :  0.475085 , training accuracy :  0.808081\n",
      "i: 25400 , cost :  0.469016 , training accuracy :  0.783389\n",
      "i: 25500 , cost :  0.481654 , training accuracy :  0.804714\n",
      "i: 25600 , cost :  0.471432 , training accuracy :  0.784512\n",
      "i: 25700 , cost :  0.481233 , training accuracy :  0.804714\n",
      "i: 25800 , cost :  0.469847 , training accuracy :  0.784512\n",
      "i: 25900 , cost :  0.475274 , training accuracy :  0.804714\n",
      "i: 26000 , cost :  0.464333 , training accuracy :  0.787879\n",
      "i: 26100 , cost :  0.465704 , training accuracy :  0.809203\n",
      "i: 26200 , cost :  0.457046 , training accuracy :  0.79349\n",
      "i: 26300 , cost :  0.455444 , training accuracy :  0.808081\n",
      "i: 26400 , cost :  0.448989 , training accuracy :  0.792368\n",
      "i: 26500 , cost :  0.446018 , training accuracy :  0.81257\n",
      "i: 26600 , cost :  0.442591 , training accuracy :  0.796857\n",
      "i: 26700 , cost :  0.44094 , training accuracy :  0.803591\n",
      "i: 26800 , cost :  0.44073 , training accuracy :  0.803591\n",
      "i: 26900 , cost :  0.44278 , training accuracy :  0.799102\n",
      "i: 27000 , cost :  0.447459 , training accuracy :  0.809203\n",
      "i: 27100 , cost :  0.452553 , training accuracy :  0.794613\n",
      "i: 27200 , cost :  0.463222 , training accuracy :  0.811448\n",
      "i: 27300 , cost :  0.465061 , training accuracy :  0.786756\n",
      "i: 27400 , cost :  0.478787 , training accuracy :  0.806958\n",
      "i: 27500 , cost :  0.470925 , training accuracy :  0.784512\n",
      "i: 27600 , cost :  0.480353 , training accuracy :  0.804714\n",
      "i: 27700 , cost :  0.46673 , training accuracy :  0.786756\n",
      "i: 27800 , cost :  0.469397 , training accuracy :  0.806958\n",
      "i: 27900 , cost :  0.458436 , training accuracy :  0.792368\n",
      "i: 28000 , cost :  0.454464 , training accuracy :  0.809203\n",
      "i: 28100 , cost :  0.447245 , training accuracy :  0.790123\n",
      "i: 28200 , cost :  0.443676 , training accuracy :  0.809203\n",
      "i: 28300 , cost :  0.44082 , training accuracy :  0.801347\n",
      "i: 28400 , cost :  0.441092 , training accuracy :  0.800224\n",
      "i: 28500 , cost :  0.444614 , training accuracy :  0.809203\n",
      "i: 28600 , cost :  0.450452 , training accuracy :  0.79349\n",
      "i: 28700 , cost :  0.462973 , training accuracy :  0.81257\n",
      "i: 28800 , cost :  0.465914 , training accuracy :  0.786756\n",
      "i: 28900 , cost :  0.480463 , training accuracy :  0.806958\n",
      "i: 29000 , cost :  0.47 , training accuracy :  0.784512\n",
      "i: 29100 , cost :  0.476728 , training accuracy :  0.804714\n",
      "i: 29200 , cost :  0.463466 , training accuracy :  0.791246\n",
      "i: 29300 , cost :  0.460132 , training accuracy :  0.810326\n",
      "i: 29400 , cost :  0.450507 , training accuracy :  0.794613\n",
      "i: 29500 , cost :  0.445922 , training accuracy :  0.811448\n",
      "i: 29600 , cost :  0.441565 , training accuracy :  0.800224\n",
      "i: 29700 , cost :  0.440487 , training accuracy :  0.802469\n",
      "i: 29800 , cost :  0.443595 , training accuracy :  0.806958\n",
      "i: 29900 , cost :  0.450901 , training accuracy :  0.792368\n",
      "Final training accuracy :  0.809203\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 30000 ][hidden: 3 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.809203\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  258.257 , training accuracy :  0.343434\n",
      "i: 100 , cost :  5.88612 , training accuracy :  0.619529\n",
      "i: 200 , cost :  0.766626 , training accuracy :  0.73064\n",
      "i: 300 , cost :  0.606956 , training accuracy :  0.774411\n",
      "i: 400 , cost :  0.589524 , training accuracy :  0.760943\n",
      "i: 500 , cost :  0.489582 , training accuracy :  0.792368\n",
      "i: 600 , cost :  3.6964 , training accuracy :  0.664422\n",
      "i: 700 , cost :  0.524748 , training accuracy :  0.79798\n",
      "i: 800 , cost :  0.4765 , training accuracy :  0.800224\n",
      "i: 900 , cost :  0.45661 , training accuracy :  0.800224\n",
      "i: 1000 , cost :  0.447085 , training accuracy :  0.803591\n",
      "i: 1100 , cost :  0.442057 , training accuracy :  0.801347\n",
      "i: 1200 , cost :  0.439031 , training accuracy :  0.806958\n",
      "i: 1300 , cost :  0.439338 , training accuracy :  0.802469\n",
      "i: 1400 , cost :  0.447772 , training accuracy :  0.81257\n",
      "i: 1500 , cost :  0.47678 , training accuracy :  0.785634\n",
      "i: 1600 , cost :  0.553834 , training accuracy :  0.768799\n",
      "i: 1700 , cost :  0.444598 , training accuracy :  0.795735\n",
      "i: 1800 , cost :  0.457062 , training accuracy :  0.783389\n",
      "i: 1900 , cost :  0.550983 , training accuracy :  0.769921\n",
      "i: 2000 , cost :  0.434188 , training accuracy :  0.806958\n",
      "i: 2100 , cost :  0.481044 , training accuracy :  0.790123\n",
      "i: 2200 , cost :  0.486542 , training accuracy :  0.795735\n",
      "i: 2300 , cost :  0.44789 , training accuracy :  0.811448\n",
      "i: 2400 , cost :  0.501874 , training accuracy :  0.782267\n",
      "i: 2500 , cost :  0.430301 , training accuracy :  0.810326\n",
      "i: 2600 , cost :  0.504245 , training accuracy :  0.790123\n",
      "i: 2700 , cost :  0.468628 , training accuracy :  0.782267\n",
      "i: 2800 , cost :  0.431455 , training accuracy :  0.808081\n",
      "i: 2900 , cost :  0.502721 , training accuracy :  0.791246\n",
      "i: 3000 , cost :  0.475784 , training accuracy :  0.784512\n",
      "i: 3100 , cost :  0.427739 , training accuracy :  0.808081\n",
      "i: 3200 , cost :  0.483335 , training accuracy :  0.79349\n",
      "i: 3300 , cost :  0.491474 , training accuracy :  0.782267\n",
      "i: 3400 , cost :  0.437119 , training accuracy :  0.823793\n",
      "i: 3500 , cost :  0.446072 , training accuracy :  0.817059\n",
      "i: 3600 , cost :  0.493701 , training accuracy :  0.785634\n",
      "i: 3700 , cost :  0.46111 , training accuracy :  0.804714\n",
      "i: 3800 , cost :  0.435239 , training accuracy :  0.817059\n",
      "i: 3900 , cost :  0.489302 , training accuracy :  0.786756\n",
      "i: 4000 , cost :  0.46611 , training accuracy :  0.802469\n",
      "i: 4100 , cost :  0.436735 , training accuracy :  0.819304\n",
      "i: 4200 , cost :  0.492063 , training accuracy :  0.784512\n",
      "i: 4300 , cost :  0.450119 , training accuracy :  0.819304\n",
      "i: 4400 , cost :  0.447999 , training accuracy :  0.815937\n",
      "i: 4500 , cost :  0.494861 , training accuracy :  0.781145\n",
      "i: 4600 , cost :  0.430167 , training accuracy :  0.822671\n",
      "i: 4700 , cost :  0.475308 , training accuracy :  0.795735\n",
      "i: 4800 , cost :  0.480537 , training accuracy :  0.785634\n",
      "i: 4900 , cost :  0.428067 , training accuracy :  0.804714\n",
      "i: 5000 , cost :  0.513809 , training accuracy :  0.79349\n",
      "i: 5100 , cost :  0.444155 , training accuracy :  0.789001\n",
      "i: 5200 , cost :  0.450187 , training accuracy :  0.79349\n",
      "i: 5300 , cost :  0.521969 , training accuracy :  0.784512\n",
      "i: 5400 , cost :  0.424563 , training accuracy :  0.811448\n",
      "i: 5500 , cost :  0.475669 , training accuracy :  0.787879\n",
      "i: 5600 , cost :  0.473701 , training accuracy :  0.801347\n",
      "i: 5700 , cost :  0.436416 , training accuracy :  0.818182\n",
      "i: 5800 , cost :  0.490807 , training accuracy :  0.7789\n",
      "i: 5900 , cost :  0.432004 , training accuracy :  0.818182\n",
      "i: 6000 , cost :  0.466241 , training accuracy :  0.800224\n",
      "i: 6100 , cost :  0.481085 , training accuracy :  0.782267\n",
      "i: 6200 , cost :  0.424321 , training accuracy :  0.809203\n",
      "i: 6300 , cost :  0.498175 , training accuracy :  0.795735\n",
      "i: 6400 , cost :  0.455043 , training accuracy :  0.789001\n",
      "i: 6500 , cost :  0.435112 , training accuracy :  0.79349\n",
      "i: 6600 , cost :  0.517736 , training accuracy :  0.786756\n",
      "i: 6700 , cost :  0.433234 , training accuracy :  0.795735\n",
      "i: 6800 , cost :  0.450295 , training accuracy :  0.795735\n",
      "i: 6900 , cost :  0.515296 , training accuracy :  0.786756\n",
      "i: 7000 , cost :  0.423331 , training accuracy :  0.813693\n",
      "i: 7100 , cost :  0.462719 , training accuracy :  0.79349\n",
      "i: 7200 , cost :  0.495631 , training accuracy :  0.794613\n",
      "i: 7300 , cost :  0.42284 , training accuracy :  0.814815\n",
      "i: 7400 , cost :  0.47217 , training accuracy :  0.789001\n",
      "i: 7500 , cost :  0.475911 , training accuracy :  0.802469\n",
      "i: 7600 , cost :  0.425746 , training accuracy :  0.818182\n",
      "i: 7700 , cost :  0.476621 , training accuracy :  0.789001\n",
      "i: 7800 , cost :  0.46446 , training accuracy :  0.803591\n",
      "i: 7900 , cost :  0.427548 , training accuracy :  0.818182\n",
      "i: 8000 , cost :  0.477027 , training accuracy :  0.786756\n",
      "i: 8100 , cost :  0.464776 , training accuracy :  0.804714\n",
      "i: 8200 , cost :  0.426418 , training accuracy :  0.815937\n",
      "i: 8300 , cost :  0.476314 , training accuracy :  0.786756\n",
      "i: 8400 , cost :  0.466365 , training accuracy :  0.805836\n",
      "i: 8500 , cost :  0.425438 , training accuracy :  0.814815\n",
      "i: 8600 , cost :  0.473191 , training accuracy :  0.789001\n",
      "i: 8700 , cost :  0.472494 , training accuracy :  0.803591\n",
      "i: 8800 , cost :  0.422757 , training accuracy :  0.814815\n",
      "i: 8900 , cost :  0.4681 , training accuracy :  0.79349\n",
      "i: 9000 , cost :  0.484326 , training accuracy :  0.796857\n",
      "i: 9100 , cost :  0.420816 , training accuracy :  0.815937\n",
      "i: 9200 , cost :  0.46108 , training accuracy :  0.795735\n",
      "i: 9300 , cost :  0.497037 , training accuracy :  0.79349\n",
      "i: 9400 , cost :  0.421096 , training accuracy :  0.819304\n",
      "i: 9500 , cost :  0.452576 , training accuracy :  0.795735\n",
      "i: 9600 , cost :  0.506052 , training accuracy :  0.79349\n",
      "i: 9700 , cost :  0.425699 , training accuracy :  0.805836\n",
      "i: 9800 , cost :  0.442195 , training accuracy :  0.796857\n",
      "i: 9900 , cost :  0.506738 , training accuracy :  0.791246\n",
      "i: 10000 , cost :  0.437223 , training accuracy :  0.796857\n",
      "i: 10100 , cost :  0.430683 , training accuracy :  0.801347\n",
      "i: 10200 , cost :  0.496029 , training accuracy :  0.79349\n",
      "i: 10300 , cost :  0.454059 , training accuracy :  0.791246\n",
      "i: 10400 , cost :  0.422206 , training accuracy :  0.811448\n",
      "i: 10500 , cost :  0.476349 , training accuracy :  0.802469\n",
      "i: 10600 , cost :  0.470246 , training accuracy :  0.789001\n",
      "i: 10700 , cost :  0.421014 , training accuracy :  0.813693\n",
      "i: 10800 , cost :  0.453458 , training accuracy :  0.817059\n",
      "i: 10900 , cost :  0.478297 , training accuracy :  0.787879\n",
      "i: 11000 , cost :  0.435414 , training accuracy :  0.817059\n",
      "i: 11100 , cost :  0.432069 , training accuracy :  0.822671\n",
      "i: 11200 , cost :  0.472497 , training accuracy :  0.792368\n",
      "i: 11300 , cost :  0.46848 , training accuracy :  0.803591\n",
      "i: 11400 , cost :  0.420055 , training accuracy :  0.817059\n",
      "i: 11500 , cost :  0.454096 , training accuracy :  0.796857\n",
      "i: 11600 , cost :  0.500299 , training accuracy :  0.79349\n",
      "i: 11700 , cost :  0.427684 , training accuracy :  0.803591\n",
      "i: 11800 , cost :  0.433559 , training accuracy :  0.799102\n",
      "i: 11900 , cost :  0.493022 , training accuracy :  0.796857\n",
      "i: 12000 , cost :  0.454471 , training accuracy :  0.790123\n",
      "i: 12100 , cost :  0.419923 , training accuracy :  0.817059\n",
      "i: 12200 , cost :  0.4614 , training accuracy :  0.803591\n",
      "i: 12300 , cost :  0.474854 , training accuracy :  0.790123\n",
      "i: 12400 , cost :  0.432562 , training accuracy :  0.818182\n",
      "i: 12500 , cost :  0.429849 , training accuracy :  0.821549\n",
      "i: 12600 , cost :  0.466811 , training accuracy :  0.79349\n",
      "i: 12700 , cost :  0.480877 , training accuracy :  0.796857\n",
      "i: 12800 , cost :  0.420635 , training accuracy :  0.818182\n",
      "i: 12900 , cost :  0.439755 , training accuracy :  0.799102\n",
      "i: 13000 , cost :  0.494021 , training accuracy :  0.795735\n",
      "i: 13100 , cost :  0.449531 , training accuracy :  0.79349\n",
      "i: 13200 , cost :  0.419589 , training accuracy :  0.817059\n",
      "i: 13300 , cost :  0.456119 , training accuracy :  0.81257\n",
      "i: 13400 , cost :  0.47395 , training accuracy :  0.787879\n",
      "i: 13500 , cost :  0.443184 , training accuracy :  0.821549\n",
      "i: 13600 , cost :  0.421452 , training accuracy :  0.819304\n",
      "i: 13700 , cost :  0.45317 , training accuracy :  0.796857\n",
      "i: 13800 , cost :  0.495642 , training accuracy :  0.794613\n",
      "i: 13900 , cost :  0.435338 , training accuracy :  0.79798\n",
      "i: 14000 , cost :  0.422655 , training accuracy :  0.809203\n",
      "i: 14100 , cost :  0.464846 , training accuracy :  0.802469\n",
      "i: 14200 , cost :  0.472046 , training accuracy :  0.790123\n",
      "i: 14300 , cost :  0.437933 , training accuracy :  0.826038\n",
      "i: 14400 , cost :  0.421366 , training accuracy :  0.819304\n",
      "i: 14500 , cost :  0.45082 , training accuracy :  0.800224\n",
      "i: 14600 , cost :  0.492956 , training accuracy :  0.794613\n",
      "i: 14700 , cost :  0.441581 , training accuracy :  0.796857\n",
      "i: 14800 , cost :  0.419188 , training accuracy :  0.817059\n",
      "i: 14900 , cost :  0.449037 , training accuracy :  0.813693\n",
      "i: 15000 , cost :  0.470228 , training accuracy :  0.791246\n",
      "i: 15100 , cost :  0.462156 , training accuracy :  0.808081\n",
      "i: 15200 , cost :  0.419503 , training accuracy :  0.818182\n",
      "i: 15300 , cost :  0.432873 , training accuracy :  0.806958\n",
      "i: 15400 , cost :  0.477103 , training accuracy :  0.799102\n",
      "i: 15500 , cost :  0.464921 , training accuracy :  0.79349\n",
      "i: 15600 , cost :  0.431041 , training accuracy :  0.822671\n",
      "i: 15700 , cost :  0.421674 , training accuracy :  0.817059\n",
      "i: 15800 , cost :  0.447309 , training accuracy :  0.802469\n",
      "i: 15900 , cost :  0.489348 , training accuracy :  0.796857\n",
      "i: 16000 , cost :  0.451723 , training accuracy :  0.792368\n",
      "i: 16100 , cost :  0.420306 , training accuracy :  0.822671\n",
      "i: 16200 , cost :  0.429937 , training accuracy :  0.822671\n",
      "i: 16300 , cost :  0.454005 , training accuracy :  0.799102\n",
      "i: 16400 , cost :  0.489156 , training accuracy :  0.795735\n",
      "i: 16500 , cost :  0.440906 , training accuracy :  0.794613\n",
      "i: 16600 , cost :  0.418126 , training accuracy :  0.82716\n",
      "i: 16700 , cost :  0.434851 , training accuracy :  0.822671\n",
      "i: 16800 , cost :  0.45877 , training accuracy :  0.792368\n",
      "i: 16900 , cost :  0.485947 , training accuracy :  0.79798\n",
      "i: 17000 , cost :  0.440228 , training accuracy :  0.796857\n",
      "i: 17100 , cost :  0.418035 , training accuracy :  0.82716\n",
      "i: 17200 , cost :  0.432328 , training accuracy :  0.821549\n",
      "i: 17300 , cost :  0.454936 , training accuracy :  0.796857\n",
      "i: 17400 , cost :  0.486282 , training accuracy :  0.796857\n",
      "i: 17500 , cost :  0.446859 , training accuracy :  0.794613\n",
      "i: 17600 , cost :  0.420239 , training accuracy :  0.823793\n",
      "i: 17700 , cost :  0.424186 , training accuracy :  0.818182\n",
      "i: 17800 , cost :  0.445826 , training accuracy :  0.803591\n",
      "i: 17900 , cost :  0.480568 , training accuracy :  0.796857\n",
      "i: 18000 , cost :  0.459252 , training accuracy :  0.791246\n",
      "i: 18100 , cost :  0.433079 , training accuracy :  0.822671\n",
      "i: 18200 , cost :  0.417603 , training accuracy :  0.826038\n",
      "i: 18300 , cost :  0.431384 , training accuracy :  0.804714\n",
      "i: 18400 , cost :  0.463721 , training accuracy :  0.803591\n",
      "i: 18500 , cost :  0.465205 , training accuracy :  0.794613\n",
      "i: 18600 , cost :  0.456016 , training accuracy :  0.814815\n",
      "i: 18700 , cost :  0.422382 , training accuracy :  0.811448\n",
      "i: 18800 , cost :  0.419785 , training accuracy :  0.815937\n",
      "i: 18900 , cost :  0.440526 , training accuracy :  0.819304\n",
      "i: 19000 , cost :  0.457186 , training accuracy :  0.792368\n",
      "i: 19100 , cost :  0.481431 , training accuracy :  0.79798\n",
      "i: 19200 , cost :  0.446883 , training accuracy :  0.796857\n",
      "i: 19300 , cost :  0.423189 , training accuracy :  0.821549\n",
      "i: 19400 , cost :  0.418207 , training accuracy :  0.821549\n",
      "i: 19500 , cost :  0.432173 , training accuracy :  0.803591\n",
      "i: 19600 , cost :  0.460724 , training accuracy :  0.804714\n",
      "i: 19700 , cost :  0.462796 , training accuracy :  0.794613\n",
      "i: 19800 , cost :  0.468786 , training accuracy :  0.806958\n",
      "i: 19900 , cost :  0.432883 , training accuracy :  0.806958\n",
      "i: 20000 , cost :  0.417508 , training accuracy :  0.826038\n",
      "i: 20100 , cost :  0.422619 , training accuracy :  0.818182\n",
      "i: 20200 , cost :  0.43736 , training accuracy :  0.804714\n",
      "i: 20300 , cost :  0.46457 , training accuracy :  0.802469\n",
      "i: 20400 , cost :  0.462192 , training accuracy :  0.796857\n",
      "i: 20500 , cost :  0.465563 , training accuracy :  0.809203\n",
      "i: 20600 , cost :  0.432253 , training accuracy :  0.808081\n",
      "i: 20700 , cost :  0.417675 , training accuracy :  0.828283\n",
      "i: 20800 , cost :  0.420099 , training accuracy :  0.821549\n",
      "i: 20900 , cost :  0.432858 , training accuracy :  0.805836\n",
      "i: 21000 , cost :  0.456051 , training accuracy :  0.811448\n",
      "i: 21100 , cost :  0.459351 , training accuracy :  0.79349\n",
      "i: 21200 , cost :  0.475032 , training accuracy :  0.803591\n",
      "i: 21300 , cost :  0.446254 , training accuracy :  0.799102\n",
      "i: 21400 , cost :  0.428791 , training accuracy :  0.823793\n",
      "i: 21500 , cost :  0.417264 , training accuracy :  0.826038\n",
      "i: 21600 , cost :  0.418514 , training accuracy :  0.817059\n",
      "i: 21700 , cost :  0.428674 , training accuracy :  0.824916\n",
      "i: 21800 , cost :  0.438774 , training accuracy :  0.803591\n",
      "i: 21900 , cost :  0.458507 , training accuracy :  0.810326\n",
      "i: 22000 , cost :  0.457625 , training accuracy :  0.799102\n",
      "i: 22100 , cost :  0.473929 , training accuracy :  0.803591\n",
      "i: 22200 , cost :  0.452024 , training accuracy :  0.799102\n",
      "i: 22300 , cost :  0.444486 , training accuracy :  0.823793\n",
      "i: 22400 , cost :  0.428336 , training accuracy :  0.810326\n",
      "i: 22500 , cost :  0.419523 , training accuracy :  0.82716\n",
      "i: 22600 , cost :  0.415956 , training accuracy :  0.82716\n",
      "i: 22700 , cost :  0.418157 , training accuracy :  0.815937\n",
      "i: 22800 , cost :  0.421767 , training accuracy :  0.818182\n",
      "i: 22900 , cost :  0.422851 , training accuracy :  0.815937\n",
      "i: 23000 , cost :  0.425795 , training accuracy :  0.826038\n",
      "i: 23100 , cost :  0.427838 , training accuracy :  0.806958\n",
      "i: 23200 , cost :  0.452728 , training accuracy :  0.818182\n",
      "i: 23300 , cost :  0.44978 , training accuracy :  0.818182\n",
      "i: 23400 , cost :  0.417819 , training accuracy :  0.814815\n",
      "i: 23500 , cost :  0.431006 , training accuracy :  0.828283\n",
      "i: 23600 , cost :  0.425153 , training accuracy :  0.81257\n",
      "i: 23700 , cost :  0.417744 , training accuracy :  0.813693\n",
      "i: 23800 , cost :  0.477003 , training accuracy :  0.808081\n",
      "i: 23900 , cost :  0.416075 , training accuracy :  0.819304\n",
      "i: 24000 , cost :  0.426703 , training accuracy :  0.805836\n",
      "i: 24100 , cost :  0.437044 , training accuracy :  0.802469\n",
      "i: 24200 , cost :  0.426888 , training accuracy :  0.809203\n",
      "i: 24300 , cost :  0.440912 , training accuracy :  0.805836\n",
      "i: 24400 , cost :  0.478434 , training accuracy :  0.802469\n",
      "i: 24500 , cost :  0.475246 , training accuracy :  0.802469\n",
      "i: 24600 , cost :  0.433878 , training accuracy :  0.826038\n",
      "i: 24700 , cost :  0.465712 , training accuracy :  0.809203\n",
      "i: 24800 , cost :  0.416807 , training accuracy :  0.828283\n",
      "i: 24900 , cost :  0.437461 , training accuracy :  0.805836\n",
      "i: 25000 , cost :  0.442228 , training accuracy :  0.808081\n",
      "i: 25100 , cost :  0.529732 , training accuracy :  0.781145\n",
      "i: 25200 , cost :  0.427625 , training accuracy :  0.808081\n",
      "i: 25300 , cost :  0.428925 , training accuracy :  0.822671\n",
      "i: 25400 , cost :  0.447133 , training accuracy :  0.826038\n",
      "i: 25500 , cost :  0.47449 , training accuracy :  0.792368\n",
      "i: 25600 , cost :  0.429117 , training accuracy :  0.805836\n",
      "i: 25700 , cost :  0.426153 , training accuracy :  0.809203\n",
      "i: 25800 , cost :  0.425597 , training accuracy :  0.810326\n",
      "i: 25900 , cost :  0.426499 , training accuracy :  0.809203\n",
      "i: 26000 , cost :  0.426216 , training accuracy :  0.809203\n",
      "i: 26100 , cost :  0.424463 , training accuracy :  0.810326\n",
      "i: 26200 , cost :  0.423757 , training accuracy :  0.809203\n",
      "i: 26300 , cost :  0.425917 , training accuracy :  0.809203\n",
      "i: 26400 , cost :  0.43303 , training accuracy :  0.804714\n",
      "i: 26500 , cost :  0.43692 , training accuracy :  0.804714\n",
      "i: 26600 , cost :  0.439316 , training accuracy :  0.806958\n",
      "i: 26700 , cost :  0.440928 , training accuracy :  0.805836\n",
      "i: 26800 , cost :  0.439666 , training accuracy :  0.806958\n",
      "i: 26900 , cost :  0.441917 , training accuracy :  0.806958\n",
      "i: 27000 , cost :  0.440375 , training accuracy :  0.805836\n",
      "i: 27100 , cost :  0.439027 , training accuracy :  0.806958\n",
      "i: 27200 , cost :  0.437236 , training accuracy :  0.805836\n",
      "i: 27300 , cost :  0.432009 , training accuracy :  0.808081\n",
      "i: 27400 , cost :  0.427215 , training accuracy :  0.811448\n",
      "i: 27500 , cost :  0.433473 , training accuracy :  0.804714\n",
      "i: 27600 , cost :  0.435779 , training accuracy :  0.804714\n",
      "i: 27700 , cost :  0.431991 , training accuracy :  0.805836\n",
      "i: 27800 , cost :  0.428357 , training accuracy :  0.809203\n",
      "i: 27900 , cost :  0.430309 , training accuracy :  0.808081\n",
      "i: 28000 , cost :  0.435555 , training accuracy :  0.803591\n",
      "i: 28100 , cost :  0.435818 , training accuracy :  0.803591\n",
      "i: 28200 , cost :  0.430571 , training accuracy :  0.808081\n",
      "i: 28300 , cost :  0.429801 , training accuracy :  0.808081\n",
      "i: 28400 , cost :  0.435042 , training accuracy :  0.803591\n",
      "i: 28500 , cost :  0.435178 , training accuracy :  0.803591\n",
      "i: 28600 , cost :  0.42938 , training accuracy :  0.810326\n",
      "i: 28700 , cost :  0.431288 , training accuracy :  0.804714\n",
      "i: 28800 , cost :  0.436107 , training accuracy :  0.803591\n",
      "i: 28900 , cost :  0.430019 , training accuracy :  0.809203\n",
      "i: 29000 , cost :  0.429312 , training accuracy :  0.810326\n",
      "i: 29100 , cost :  0.433465 , training accuracy :  0.803591\n",
      "i: 29200 , cost :  0.427128 , training accuracy :  0.810326\n",
      "i: 29300 , cost :  0.435859 , training accuracy :  0.804714\n",
      "i: 29400 , cost :  0.427949 , training accuracy :  0.811448\n",
      "i: 29500 , cost :  0.436801 , training accuracy :  0.805836\n",
      "i: 29600 , cost :  0.425813 , training accuracy :  0.81257\n",
      "i: 29700 , cost :  0.435677 , training accuracy :  0.804714\n",
      "i: 29800 , cost :  0.429074 , training accuracy :  0.810326\n",
      "i: 29900 , cost :  0.429437 , training accuracy :  0.808081\n",
      "Final training accuracy :  0.805836\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 30000 ][hidden: 10 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.805836\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  87.8622 , training accuracy :  0.40853\n",
      "i: 100 , cost :  1.72201 , training accuracy :  0.67789\n",
      "i: 200 , cost :  1.08125 , training accuracy :  0.729517\n",
      "i: 300 , cost :  0.789548 , training accuracy :  0.75982\n",
      "i: 400 , cost :  3.34364 , training accuracy :  0.38945\n",
      "i: 500 , cost :  0.629398 , training accuracy :  0.783389\n",
      "i: 600 , cost :  1.01471 , training accuracy :  0.683502\n",
      "i: 700 , cost :  0.718955 , training accuracy :  0.775533\n",
      "i: 800 , cost :  0.588169 , training accuracy :  0.799102\n",
      "i: 900 , cost :  0.498076 , training accuracy :  0.794613\n",
      "i: 1000 , cost :  1.48061 , training accuracy :  0.716049\n",
      "i: 1100 , cost :  0.491156 , training accuracy :  0.800224\n",
      "i: 1200 , cost :  0.892393 , training accuracy :  0.773288\n",
      "i: 1300 , cost :  0.489446 , training accuracy :  0.806958\n",
      "i: 1400 , cost :  0.631992 , training accuracy :  0.76431\n",
      "i: 1500 , cost :  0.637996 , training accuracy :  0.786756\n",
      "i: 1600 , cost :  0.524868 , training accuracy :  0.803591\n",
      "i: 1700 , cost :  0.455625 , training accuracy :  0.79798\n",
      "i: 1800 , cost :  0.97002 , training accuracy :  0.736251\n",
      "i: 1900 , cost :  0.475161 , training accuracy :  0.803591\n",
      "i: 2000 , cost :  0.442685 , training accuracy :  0.803591\n",
      "i: 2100 , cost :  1.90604 , training accuracy :  0.708193\n",
      "i: 2200 , cost :  1.2517 , training accuracy :  0.741863\n",
      "i: 2300 , cost :  0.482245 , training accuracy :  0.803591\n",
      "i: 2400 , cost :  0.43716 , training accuracy :  0.806958\n",
      "i: 2500 , cost :  0.458959 , training accuracy :  0.79349\n",
      "i: 2600 , cost :  0.844592 , training accuracy :  0.783389\n",
      "i: 2700 , cost :  0.635727 , training accuracy :  0.796857\n",
      "i: 2800 , cost :  0.446156 , training accuracy :  0.809203\n",
      "i: 2900 , cost :  0.434164 , training accuracy :  0.802469\n",
      "i: 3000 , cost :  0.871428 , training accuracy :  0.771044\n",
      "i: 3100 , cost :  0.46175 , training accuracy :  0.806958\n",
      "i: 3200 , cost :  0.427112 , training accuracy :  0.808081\n",
      "i: 3300 , cost :  0.510082 , training accuracy :  0.79798\n",
      "i: 3400 , cost :  0.478331 , training accuracy :  0.81257\n",
      "i: 3500 , cost :  0.426443 , training accuracy :  0.806958\n",
      "i: 3600 , cost :  0.927184 , training accuracy :  0.760943\n",
      "i: 3700 , cost :  0.621537 , training accuracy :  0.799102\n",
      "i: 3800 , cost :  0.448868 , training accuracy :  0.811448\n",
      "i: 3900 , cost :  0.421461 , training accuracy :  0.811448\n",
      "i: 4000 , cost :  0.503482 , training accuracy :  0.79349\n",
      "i: 4100 , cost :  0.565413 , training accuracy :  0.790123\n",
      "i: 4200 , cost :  0.510004 , training accuracy :  0.795735\n",
      "i: 4300 , cost :  0.682382 , training accuracy :  0.755331\n",
      "i: 4400 , cost :  0.537719 , training accuracy :  0.79349\n",
      "i: 4500 , cost :  1.26301 , training accuracy :  0.690236\n",
      "i: 4600 , cost :  0.455225 , training accuracy :  0.808081\n",
      "i: 4700 , cost :  0.579269 , training accuracy :  0.772166\n",
      "i: 4800 , cost :  0.715028 , training accuracy :  0.782267\n",
      "i: 4900 , cost :  0.427359 , training accuracy :  0.808081\n",
      "i: 5000 , cost :  1.01261 , training accuracy :  0.75982\n",
      "i: 5100 , cost :  0.439909 , training accuracy :  0.810326\n",
      "i: 5200 , cost :  0.417053 , training accuracy :  0.810326\n",
      "i: 5300 , cost :  0.547239 , training accuracy :  0.802469\n",
      "i: 5400 , cost :  0.413416 , training accuracy :  0.81257\n",
      "i: 5500 , cost :  1.22231 , training accuracy :  0.750842\n",
      "i: 5600 , cost :  0.431559 , training accuracy :  0.815937\n",
      "i: 5700 , cost :  0.40664 , training accuracy :  0.818182\n",
      "i: 5800 , cost :  0.7761 , training accuracy :  0.7789\n",
      "i: 5900 , cost :  0.590785 , training accuracy :  0.794613\n",
      "i: 6000 , cost :  0.422232 , training accuracy :  0.81257\n",
      "i: 6100 , cost :  0.40368 , training accuracy :  0.819304\n",
      "i: 6200 , cost :  0.582321 , training accuracy :  0.808081\n",
      "i: 6300 , cost :  0.424831 , training accuracy :  0.817059\n",
      "i: 6400 , cost :  0.403329 , training accuracy :  0.820426\n",
      "i: 6500 , cost :  0.973284 , training accuracy :  0.769921\n",
      "i: 6600 , cost :  2.03107 , training accuracy :  0.433221\n",
      "i: 6700 , cost :  0.433401 , training accuracy :  0.81257\n",
      "i: 6800 , cost :  0.403954 , training accuracy :  0.822671\n",
      "i: 6900 , cost :  1.90257 , training accuracy :  0.675645\n",
      "i: 7000 , cost :  0.657389 , training accuracy :  0.794613\n",
      "i: 7100 , cost :  0.426127 , training accuracy :  0.815937\n",
      "i: 7200 , cost :  0.401884 , training accuracy :  0.822671\n",
      "i: 7300 , cost :  0.499266 , training accuracy :  0.792368\n",
      "i: 7400 , cost :  0.469182 , training accuracy :  0.811448\n",
      "i: 7500 , cost :  0.401918 , training accuracy :  0.826038\n",
      "i: 7600 , cost :  1.2079 , training accuracy :  0.548822\n",
      "i: 7700 , cost :  0.471828 , training accuracy :  0.802469\n",
      "i: 7800 , cost :  0.404833 , training accuracy :  0.823793\n",
      "i: 7900 , cost :  0.455537 , training accuracy :  0.801347\n",
      "i: 8000 , cost :  0.440236 , training accuracy :  0.821549\n",
      "i: 8100 , cost :  0.399835 , training accuracy :  0.822671\n",
      "i: 8200 , cost :  0.443097 , training accuracy :  0.813693\n",
      "i: 8300 , cost :  0.499629 , training accuracy :  0.789001\n",
      "i: 8400 , cost :  0.429927 , training accuracy :  0.808081\n",
      "i: 8500 , cost :  0.468446 , training accuracy :  0.796857\n",
      "i: 8600 , cost :  0.527338 , training accuracy :  0.785634\n",
      "i: 8700 , cost :  1.23578 , training accuracy :  0.721661\n",
      "i: 8800 , cost :  0.621098 , training accuracy :  0.789001\n",
      "i: 8900 , cost :  0.414545 , training accuracy :  0.821549\n",
      "i: 9000 , cost :  0.397466 , training accuracy :  0.823793\n",
      "i: 9100 , cost :  0.559846 , training accuracy :  0.787879\n",
      "i: 9200 , cost :  0.412985 , training accuracy :  0.819304\n",
      "i: 9300 , cost :  0.664707 , training accuracy :  0.76431\n",
      "i: 9400 , cost :  0.441383 , training accuracy :  0.813693\n",
      "i: 9500 , cost :  0.445114 , training accuracy :  0.809203\n",
      "i: 9600 , cost :  0.417614 , training accuracy :  0.811448\n",
      "i: 9700 , cost :  0.548109 , training accuracy :  0.804714\n",
      "i: 9800 , cost :  0.485641 , training accuracy :  0.808081\n",
      "i: 9900 , cost :  0.402608 , training accuracy :  0.826038\n",
      "i: 10000 , cost :  0.433969 , training accuracy :  0.81257\n",
      "i: 10100 , cost :  0.457314 , training accuracy :  0.823793\n",
      "i: 10200 , cost :  0.394592 , training accuracy :  0.82716\n",
      "i: 10300 , cost :  2.77181 , training accuracy :  0.679012\n",
      "i: 10400 , cost :  0.43348 , training accuracy :  0.829405\n",
      "i: 10500 , cost :  0.392379 , training accuracy :  0.826038\n",
      "i: 10600 , cost :  1.37238 , training accuracy :  0.727273\n",
      "i: 10700 , cost :  0.429419 , training accuracy :  0.819304\n",
      "i: 10800 , cost :  0.393307 , training accuracy :  0.82716\n",
      "i: 10900 , cost :  0.396391 , training accuracy :  0.824916\n",
      "i: 11000 , cost :  0.567865 , training accuracy :  0.784512\n",
      "i: 11100 , cost :  0.495167 , training accuracy :  0.79798\n",
      "i: 11200 , cost :  0.404127 , training accuracy :  0.823793\n",
      "i: 11300 , cost :  0.461782 , training accuracy :  0.803591\n",
      "i: 11400 , cost :  0.52442 , training accuracy :  0.792368\n",
      "i: 11500 , cost :  0.515318 , training accuracy :  0.803591\n",
      "i: 11600 , cost :  0.400711 , training accuracy :  0.826038\n",
      "i: 11700 , cost :  1.18021 , training accuracy :  0.736251\n",
      "i: 11800 , cost :  0.599622 , training accuracy :  0.808081\n",
      "i: 11900 , cost :  0.403275 , training accuracy :  0.82716\n",
      "i: 12000 , cost :  0.386816 , training accuracy :  0.826038\n",
      "i: 12100 , cost :  0.714059 , training accuracy :  0.773288\n",
      "i: 12200 , cost :  0.427983 , training accuracy :  0.818182\n",
      "i: 12300 , cost :  0.390526 , training accuracy :  0.83165\n",
      "i: 12400 , cost :  0.400814 , training accuracy :  0.821549\n",
      "i: 12500 , cost :  1.01823 , training accuracy :  0.717172\n",
      "i: 12600 , cost :  0.39623 , training accuracy :  0.823793\n",
      "i: 12700 , cost :  0.395361 , training accuracy :  0.817059\n",
      "i: 12800 , cost :  1.64075 , training accuracy :  0.714927\n",
      "i: 12900 , cost :  1.46729 , training accuracy :  0.731762\n",
      "i: 13000 , cost :  0.414483 , training accuracy :  0.823793\n",
      "i: 13100 , cost :  0.388608 , training accuracy :  0.828283\n",
      "i: 13200 , cost :  0.404508 , training accuracy :  0.819304\n",
      "i: 13300 , cost :  0.409169 , training accuracy :  0.817059\n",
      "i: 13400 , cost :  0.501907 , training accuracy :  0.79798\n",
      "i: 13500 , cost :  0.392689 , training accuracy :  0.82716\n",
      "i: 13600 , cost :  0.387415 , training accuracy :  0.829405\n",
      "i: 13700 , cost :  0.479924 , training accuracy :  0.795735\n",
      "i: 13800 , cost :  0.434258 , training accuracy :  0.808081\n",
      "i: 13900 , cost :  1.50786 , training accuracy :  0.507295\n",
      "i: 14000 , cost :  0.433698 , training accuracy :  0.819304\n",
      "i: 14100 , cost :  0.390666 , training accuracy :  0.833894\n",
      "i: 14200 , cost :  0.386693 , training accuracy :  0.83165\n",
      "i: 14300 , cost :  0.61918 , training accuracy :  0.79349\n",
      "i: 14400 , cost :  0.391561 , training accuracy :  0.832772\n",
      "i: 14500 , cost :  0.390688 , training accuracy :  0.82716\n",
      "i: 14600 , cost :  1.88211 , training accuracy :  0.709315\n",
      "i: 14700 , cost :  0.620948 , training accuracy :  0.814815\n",
      "i: 14800 , cost :  0.401136 , training accuracy :  0.828283\n",
      "i: 14900 , cost :  0.385353 , training accuracy :  0.830527\n",
      "i: 15000 , cost :  0.394211 , training accuracy :  0.821549\n",
      "i: 15100 , cost :  1.23699 , training accuracy :  0.749719\n",
      "i: 15200 , cost :  0.401806 , training accuracy :  0.82716\n",
      "i: 15300 , cost :  0.384487 , training accuracy :  0.829405\n",
      "i: 15400 , cost :  2.67322 , training accuracy :  0.674523\n",
      "i: 15500 , cost :  0.720703 , training accuracy :  0.800224\n",
      "i: 15600 , cost :  0.401423 , training accuracy :  0.828283\n",
      "i: 15700 , cost :  0.384612 , training accuracy :  0.830527\n",
      "i: 15800 , cost :  0.462063 , training accuracy :  0.803591\n",
      "i: 15900 , cost :  0.561225 , training accuracy :  0.810326\n",
      "i: 16000 , cost :  0.389034 , training accuracy :  0.830527\n",
      "i: 16100 , cost :  0.424029 , training accuracy :  0.811448\n",
      "i: 16200 , cost :  0.420905 , training accuracy :  0.813693\n",
      "i: 16300 , cost :  0.389988 , training accuracy :  0.829405\n",
      "i: 16400 , cost :  0.874318 , training accuracy :  0.784512\n",
      "i: 16500 , cost :  0.446127 , training accuracy :  0.824916\n",
      "i: 16600 , cost :  0.392217 , training accuracy :  0.83165\n",
      "i: 16700 , cost :  0.382433 , training accuracy :  0.830527\n",
      "i: 16800 , cost :  0.399269 , training accuracy :  0.826038\n",
      "i: 16900 , cost :  0.776839 , training accuracy :  0.751964\n",
      "i: 17000 , cost :  0.393373 , training accuracy :  0.829405\n",
      "i: 17100 , cost :  0.381609 , training accuracy :  0.830527\n",
      "i: 17200 , cost :  0.455809 , training accuracy :  0.803591\n",
      "i: 17300 , cost :  0.470869 , training accuracy :  0.815937\n",
      "i: 17400 , cost :  0.385665 , training accuracy :  0.835017\n",
      "i: 17500 , cost :  0.405716 , training accuracy :  0.819304\n",
      "i: 17600 , cost :  0.733788 , training accuracy :  0.758698\n",
      "i: 17700 , cost :  0.388466 , training accuracy :  0.830527\n",
      "i: 17800 , cost :  0.388168 , training accuracy :  0.820426\n",
      "i: 17900 , cost :  0.64035 , training accuracy :  0.772166\n",
      "i: 18000 , cost :  0.433215 , training accuracy :  0.830527\n",
      "i: 18100 , cost :  0.38172 , training accuracy :  0.835017\n",
      "i: 18200 , cost :  0.445324 , training accuracy :  0.809203\n",
      "i: 18300 , cost :  1.6327 , training accuracy :  0.461279\n",
      "i: 18400 , cost :  0.391568 , training accuracy :  0.829405\n",
      "i: 18500 , cost :  0.378758 , training accuracy :  0.833894\n",
      "i: 18600 , cost :  0.672969 , training accuracy :  0.757576\n",
      "i: 18700 , cost :  2.54555 , training accuracy :  0.691358\n",
      "i: 18800 , cost :  0.387261 , training accuracy :  0.835017\n",
      "i: 18900 , cost :  0.387002 , training accuracy :  0.826038\n",
      "i: 19000 , cost :  0.394424 , training accuracy :  0.828283\n",
      "i: 19100 , cost :  0.391377 , training accuracy :  0.828283\n",
      "i: 19200 , cost :  0.383162 , training accuracy :  0.832772\n",
      "i: 19300 , cost :  0.390651 , training accuracy :  0.83165\n",
      "i: 19400 , cost :  0.429224 , training accuracy :  0.821549\n",
      "i: 19500 , cost :  0.748448 , training accuracy :  0.789001\n",
      "i: 19600 , cost :  0.383927 , training accuracy :  0.836139\n",
      "i: 19700 , cost :  0.430305 , training accuracy :  0.814815\n",
      "i: 19800 , cost :  0.621736 , training accuracy :  0.802469\n",
      "i: 19900 , cost :  0.475372 , training accuracy :  0.82716\n",
      "i: 20000 , cost :  0.393513 , training accuracy :  0.833894\n",
      "i: 20100 , cost :  0.37666 , training accuracy :  0.837261\n",
      "i: 20200 , cost :  0.378295 , training accuracy :  0.826038\n",
      "i: 20300 , cost :  0.45775 , training accuracy :  0.810326\n",
      "i: 20400 , cost :  1.05654 , training accuracy :  0.630752\n",
      "i: 20500 , cost :  0.41525 , training accuracy :  0.82716\n",
      "i: 20600 , cost :  0.378535 , training accuracy :  0.838384\n",
      "i: 20700 , cost :  0.372206 , training accuracy :  0.836139\n",
      "i: 20800 , cost :  0.395587 , training accuracy :  0.822671\n",
      "i: 20900 , cost :  0.387976 , training accuracy :  0.821549\n",
      "i: 21000 , cost :  0.388489 , training accuracy :  0.83165\n",
      "i: 21100 , cost :  0.371107 , training accuracy :  0.837261\n",
      "i: 21200 , cost :  0.888488 , training accuracy :  0.784512\n",
      "i: 21300 , cost :  0.523115 , training accuracy :  0.818182\n",
      "i: 21400 , cost :  0.38288 , training accuracy :  0.837261\n",
      "i: 21500 , cost :  0.370994 , training accuracy :  0.838384\n",
      "i: 21600 , cost :  0.396025 , training accuracy :  0.830527\n",
      "i: 21700 , cost :  0.42321 , training accuracy :  0.81257\n",
      "i: 21800 , cost :  0.375089 , training accuracy :  0.833894\n",
      "i: 21900 , cost :  0.377643 , training accuracy :  0.835017\n",
      "i: 22000 , cost :  0.386499 , training accuracy :  0.83165\n",
      "i: 22100 , cost :  0.396312 , training accuracy :  0.823793\n",
      "i: 22200 , cost :  0.463374 , training accuracy :  0.808081\n",
      "i: 22300 , cost :  0.552372 , training accuracy :  0.794613\n",
      "i: 22400 , cost :  0.385643 , training accuracy :  0.826038\n",
      "i: 22500 , cost :  0.39712 , training accuracy :  0.823793\n",
      "i: 22600 , cost :  0.37894 , training accuracy :  0.83165\n",
      "i: 22700 , cost :  0.392171 , training accuracy :  0.828283\n",
      "i: 22800 , cost :  0.395616 , training accuracy :  0.82716\n",
      "i: 22900 , cost :  0.376182 , training accuracy :  0.83165\n",
      "i: 23000 , cost :  0.396977 , training accuracy :  0.823793\n",
      "i: 23100 , cost :  0.924294 , training accuracy :  0.744108\n",
      "i: 23200 , cost :  0.43815 , training accuracy :  0.837261\n",
      "i: 23300 , cost :  0.368495 , training accuracy :  0.839506\n",
      "i: 23400 , cost :  2.37957 , training accuracy :  0.699214\n",
      "i: 23500 , cost :  0.377317 , training accuracy :  0.836139\n",
      "i: 23600 , cost :  0.364139 , training accuracy :  0.840629\n",
      "i: 23700 , cost :  0.403509 , training accuracy :  0.81257\n",
      "i: 23800 , cost :  0.682066 , training accuracy :  0.772166\n",
      "i: 23900 , cost :  0.366599 , training accuracy :  0.838384\n",
      "i: 24000 , cost :  0.389741 , training accuracy :  0.82716\n",
      "i: 24100 , cost :  1.33898 , training accuracy :  0.73064\n",
      "i: 24200 , cost :  0.375913 , training accuracy :  0.836139\n",
      "i: 24300 , cost :  0.362805 , training accuracy :  0.842873\n",
      "i: 24400 , cost :  0.370301 , training accuracy :  0.830527\n",
      "i: 24500 , cost :  3.3519 , training accuracy :  0.410774\n",
      "i: 24600 , cost :  0.386054 , training accuracy :  0.83165\n",
      "i: 24700 , cost :  0.363791 , training accuracy :  0.842873\n",
      "i: 24800 , cost :  0.369018 , training accuracy :  0.837261\n",
      "i: 24900 , cost :  0.367878 , training accuracy :  0.840629\n",
      "i: 25000 , cost :  0.619398 , training accuracy :  0.806958\n",
      "i: 25100 , cost :  0.423498 , training accuracy :  0.82716\n",
      "i: 25200 , cost :  0.371322 , training accuracy :  0.838384\n",
      "i: 25300 , cost :  0.361552 , training accuracy :  0.840629\n",
      "i: 25400 , cost :  0.365042 , training accuracy :  0.836139\n",
      "i: 25500 , cost :  0.39114 , training accuracy :  0.826038\n",
      "i: 25600 , cost :  0.419327 , training accuracy :  0.814815\n",
      "i: 25700 , cost :  0.487247 , training accuracy :  0.814815\n",
      "i: 25800 , cost :  0.375491 , training accuracy :  0.838384\n",
      "i: 25900 , cost :  0.361181 , training accuracy :  0.842873\n",
      "i: 26000 , cost :  0.398729 , training accuracy :  0.821549\n",
      "i: 26100 , cost :  0.366266 , training accuracy :  0.838384\n",
      "i: 26200 , cost :  0.375906 , training accuracy :  0.832772\n",
      "i: 26300 , cost :  0.408138 , training accuracy :  0.817059\n",
      "i: 26400 , cost :  0.466379 , training accuracy :  0.820426\n",
      "i: 26500 , cost :  0.3657 , training accuracy :  0.842873\n",
      "i: 26600 , cost :  0.358665 , training accuracy :  0.838384\n",
      "i: 26700 , cost :  0.389087 , training accuracy :  0.822671\n",
      "i: 26800 , cost :  0.54825 , training accuracy :  0.794613\n",
      "i: 26900 , cost :  0.392266 , training accuracy :  0.83165\n",
      "i: 27000 , cost :  0.35998 , training accuracy :  0.839506\n",
      "i: 27100 , cost :  0.372244 , training accuracy :  0.835017\n",
      "i: 27200 , cost :  0.430913 , training accuracy :  0.813693\n",
      "i: 27300 , cost :  0.431475 , training accuracy :  0.814815\n",
      "i: 27400 , cost :  0.434297 , training accuracy :  0.814815\n",
      "i: 27500 , cost :  0.366085 , training accuracy :  0.836139\n",
      "i: 27600 , cost :  0.393698 , training accuracy :  0.819304\n",
      "i: 27700 , cost :  0.431979 , training accuracy :  0.815937\n",
      "i: 27800 , cost :  0.487918 , training accuracy :  0.804714\n",
      "i: 27900 , cost :  0.360311 , training accuracy :  0.832772\n",
      "i: 28000 , cost :  0.360491 , training accuracy :  0.83165\n",
      "i: 28100 , cost :  0.451901 , training accuracy :  0.820426\n",
      "i: 28200 , cost :  0.361834 , training accuracy :  0.838384\n",
      "i: 28300 , cost :  0.38716 , training accuracy :  0.824916\n",
      "i: 28400 , cost :  0.415823 , training accuracy :  0.813693\n",
      "i: 28500 , cost :  0.453686 , training accuracy :  0.810326\n",
      "i: 28600 , cost :  0.474273 , training accuracy :  0.83165\n",
      "i: 28700 , cost :  0.382944 , training accuracy :  0.83165\n",
      "i: 28800 , cost :  0.357026 , training accuracy :  0.839506\n",
      "i: 28900 , cost :  0.372457 , training accuracy :  0.833894\n",
      "i: 29000 , cost :  0.377678 , training accuracy :  0.832772\n",
      "i: 29100 , cost :  0.401247 , training accuracy :  0.819304\n",
      "i: 29200 , cost :  0.3834 , training accuracy :  0.829405\n",
      "i: 29300 , cost :  0.356603 , training accuracy :  0.833894\n",
      "i: 29400 , cost :  0.371191 , training accuracy :  0.835017\n",
      "i: 29500 , cost :  0.362428 , training accuracy :  0.830527\n",
      "i: 29600 , cost :  0.442143 , training accuracy :  0.814815\n",
      "i: 29700 , cost :  0.383931 , training accuracy :  0.839506\n",
      "i: 29800 , cost :  0.356558 , training accuracy :  0.835017\n",
      "i: 29900 , cost :  0.374398 , training accuracy :  0.826038\n",
      "Final training accuracy :  0.830527\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 30000 ][hidden: 15 ][file: bhavul_tr_acc_0.83_prediction.csv ] ACCURACY :  0.830527\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  534.048 , training accuracy :  0.619529\n",
      "i: 100 , cost :  3.52847 , training accuracy :  0.645342\n",
      "i: 200 , cost :  2.02194 , training accuracy :  0.762065\n",
      "i: 300 , cost :  2.26143 , training accuracy :  0.762065\n",
      "i: 400 , cost :  6.70357 , training accuracy :  0.500561\n",
      "i: 500 , cost :  1.57964 , training accuracy :  0.809203\n",
      "i: 600 , cost :  1.31534 , training accuracy :  0.780022\n",
      "i: 700 , cost :  5.18195 , training accuracy :  0.716049\n",
      "i: 800 , cost :  1.49362 , training accuracy :  0.780022\n",
      "i: 900 , cost :  1.73623 , training accuracy :  0.802469\n",
      "i: 1000 , cost :  1.58326 , training accuracy :  0.810326\n",
      "i: 1100 , cost :  3.17508 , training accuracy :  0.760943\n",
      "i: 1200 , cost :  1.81283 , training accuracy :  0.794613\n",
      "i: 1300 , cost :  1.58892 , training accuracy :  0.773288\n",
      "i: 1400 , cost :  1.80898 , training accuracy :  0.800224\n",
      "i: 1500 , cost :  2.91557 , training accuracy :  0.641975\n",
      "i: 1600 , cost :  1.44909 , training accuracy :  0.809203\n",
      "i: 1700 , cost :  3.02218 , training accuracy :  0.768799\n",
      "i: 1800 , cost :  0.919845 , training accuracy :  0.830527\n",
      "i: 1900 , cost :  1.80591 , training accuracy :  0.747475\n",
      "i: 2000 , cost :  2.98959 , training accuracy :  0.762065\n",
      "i: 2100 , cost :  2.80115 , training accuracy :  0.637486\n",
      "i: 2200 , cost :  1.00387 , training accuracy :  0.837261\n",
      "i: 2300 , cost :  2.97733 , training accuracy :  0.736251\n",
      "i: 2400 , cost :  1.09879 , training accuracy :  0.826038\n",
      "i: 2500 , cost :  2.59225 , training accuracy :  0.781145\n",
      "i: 2600 , cost :  2.35124 , training accuracy :  0.666667\n",
      "i: 2700 , cost :  0.990418 , training accuracy :  0.842873\n",
      "i: 2800 , cost :  4.75035 , training accuracy :  0.721661\n",
      "i: 2900 , cost :  1.08547 , training accuracy :  0.826038\n",
      "i: 3000 , cost :  3.37864 , training accuracy :  0.746352\n",
      "i: 3100 , cost :  0.775964 , training accuracy :  0.828283\n",
      "i: 3200 , cost :  1.29224 , training accuracy :  0.7789\n",
      "i: 3300 , cost :  0.849666 , training accuracy :  0.840629\n",
      "i: 3400 , cost :  0.828315 , training accuracy :  0.841751\n",
      "i: 3500 , cost :  1.79041 , training accuracy :  0.813693\n",
      "i: 3600 , cost :  0.744766 , training accuracy :  0.849607\n",
      "i: 3700 , cost :  6.16357 , training accuracy :  0.69248\n",
      "i: 3800 , cost :  0.91547 , training accuracy :  0.823793\n",
      "i: 3900 , cost :  1.29131 , training accuracy :  0.822671\n",
      "i: 4000 , cost :  0.801864 , training accuracy :  0.847363\n",
      "i: 4100 , cost :  1.75071 , training accuracy :  0.809203\n",
      "i: 4200 , cost :  1.1496 , training accuracy :  0.843996\n",
      "i: 4300 , cost :  6.77515 , training accuracy :  0.410774\n",
      "i: 4400 , cost :  0.843105 , training accuracy :  0.852974\n",
      "i: 4500 , cost :  1.48768 , training accuracy :  0.817059\n",
      "i: 4600 , cost :  0.964209 , training accuracy :  0.837261\n",
      "i: 4700 , cost :  7.92878 , training accuracy :  0.717172\n",
      "i: 4800 , cost :  2.50536 , training accuracy :  0.659933\n",
      "i: 4900 , cost :  0.686861 , training accuracy :  0.860831\n",
      "i: 5000 , cost :  1.50337 , training accuracy :  0.756453\n",
      "i: 5100 , cost :  0.685647 , training accuracy :  0.860831\n",
      "i: 5200 , cost :  2.52566 , training accuracy :  0.659933\n",
      "i: 5300 , cost :  6.93303 , training accuracy :  0.401796\n",
      "i: 5400 , cost :  7.00799 , training accuracy :  0.741863\n",
      "i: 5500 , cost :  0.977383 , training accuracy :  0.839506\n",
      "i: 5600 , cost :  2.47903 , training accuracy :  0.794613\n",
      "i: 5700 , cost :  0.838447 , training accuracy :  0.859708\n",
      "i: 5800 , cost :  3.33716 , training accuracy :  0.754209\n",
      "i: 5900 , cost :  2.0367 , training accuracy :  0.809203\n",
      "i: 6000 , cost :  0.841583 , training accuracy :  0.796857\n",
      "i: 6100 , cost :  1.34151 , training accuracy :  0.802469\n",
      "i: 6200 , cost :  0.947954 , training accuracy :  0.75982\n",
      "i: 6300 , cost :  5.52394 , training accuracy :  0.736251\n",
      "i: 6400 , cost :  0.922447 , training accuracy :  0.860831\n",
      "i: 6500 , cost :  9.80795 , training accuracy :  0.402918\n",
      "i: 6600 , cost :  0.861512 , training accuracy :  0.858586\n",
      "i: 6700 , cost :  1.87929 , training accuracy :  0.814815\n",
      "i: 6800 , cost :  0.82395 , training accuracy :  0.864198\n",
      "i: 6900 , cost :  2.64204 , training accuracy :  0.792368\n",
      "i: 7000 , cost :  0.843059 , training accuracy :  0.866442\n",
      "i: 7100 , cost :  4.90791 , training accuracy :  0.460157\n",
      "i: 7200 , cost :  0.886795 , training accuracy :  0.854097\n",
      "i: 7300 , cost :  0.782107 , training accuracy :  0.839506\n",
      "i: 7400 , cost :  4.57184 , training accuracy :  0.747475\n",
      "i: 7500 , cost :  0.79175 , training accuracy :  0.867565\n",
      "i: 7600 , cost :  7.27987 , training accuracy :  0.713805\n",
      "i: 7700 , cost :  0.83241 , training accuracy :  0.859708\n",
      "i: 7800 , cost :  0.865921 , training accuracy :  0.82716\n",
      "i: 7900 , cost :  0.897057 , training accuracy :  0.859708\n",
      "i: 8000 , cost :  0.964321 , training accuracy :  0.835017\n",
      "i: 8100 , cost :  4.35157 , training accuracy :  0.731762\n",
      "i: 8200 , cost :  0.674436 , training accuracy :  0.863075\n",
      "i: 8300 , cost :  1.33158 , training accuracy :  0.824916\n",
      "i: 8400 , cost :  1.36456 , training accuracy :  0.832772\n",
      "i: 8500 , cost :  2.57353 , training accuracy :  0.780022\n",
      "i: 8600 , cost :  0.915077 , training accuracy :  0.857464\n",
      "i: 8700 , cost :  4.07847 , training accuracy :  0.75982\n",
      "i: 8800 , cost :  1.29836 , training accuracy :  0.785634\n",
      "i: 8900 , cost :  1.54122 , training accuracy :  0.802469\n",
      "i: 9000 , cost :  3.00978 , training accuracy :  0.801347\n",
      "i: 9100 , cost :  0.730027 , training accuracy :  0.861953\n",
      "i: 9200 , cost :  2.49274 , training accuracy :  0.802469\n",
      "i: 9300 , cost :  0.735057 , training accuracy :  0.864198\n",
      "i: 9400 , cost :  2.26691 , training accuracy :  0.794613\n",
      "i: 9500 , cost :  3.36164 , training accuracy :  0.638608\n",
      "i: 9600 , cost :  3.93772 , training accuracy :  0.613917\n",
      "i: 9700 , cost :  1.30735 , training accuracy :  0.847363\n",
      "i: 9800 , cost :  0.956881 , training accuracy :  0.861953\n",
      "i: 9900 , cost :  0.660883 , training accuracy :  0.857464\n",
      "i: 10000 , cost :  1.44288 , training accuracy :  0.839506\n",
      "i: 10100 , cost :  0.77912 , training accuracy :  0.859708\n",
      "i: 10200 , cost :  3.73205 , training accuracy :  0.783389\n",
      "i: 10300 , cost :  0.830747 , training accuracy :  0.79349\n",
      "i: 10400 , cost :  0.654084 , training accuracy :  0.866442\n",
      "i: 10500 , cost :  2.54341 , training accuracy :  0.787879\n",
      "i: 10600 , cost :  2.04959 , training accuracy :  0.769921\n",
      "i: 10700 , cost :  1.01318 , training accuracy :  0.854097\n",
      "i: 10800 , cost :  0.675206 , training accuracy :  0.840629\n",
      "i: 10900 , cost :  1.64041 , training accuracy :  0.791246\n",
      "i: 11000 , cost :  0.735911 , training accuracy :  0.864198\n",
      "i: 11100 , cost :  5.84043 , training accuracy :  0.481481\n",
      "i: 11200 , cost :  0.928562 , training accuracy :  0.860831\n",
      "i: 11300 , cost :  0.685781 , training accuracy :  0.815937\n",
      "i: 11400 , cost :  0.869321 , training accuracy :  0.859708\n",
      "i: 11500 , cost :  3.82017 , training accuracy :  0.699214\n",
      "i: 11600 , cost :  0.829752 , training accuracy :  0.85073\n",
      "i: 11700 , cost :  7.31606 , training accuracy :  0.690236\n",
      "i: 11800 , cost :  0.626437 , training accuracy :  0.863075\n",
      "i: 11900 , cost :  0.715333 , training accuracy :  0.814815\n",
      "i: 12000 , cost :  3.24652 , training accuracy :  0.647587\n",
      "i: 12100 , cost :  0.678579 , training accuracy :  0.866442\n",
      "i: 12200 , cost :  0.819654 , training accuracy :  0.835017\n",
      "i: 12300 , cost :  2.98256 , training accuracy :  0.796857\n",
      "i: 12400 , cost :  0.676784 , training accuracy :  0.864198\n",
      "i: 12500 , cost :  0.750299 , training accuracy :  0.826038\n",
      "i: 12600 , cost :  11.1872 , training accuracy :  0.716049\n",
      "i: 12700 , cost :  1.62199 , training accuracy :  0.765432\n",
      "i: 12800 , cost :  0.780561 , training accuracy :  0.866442\n",
      "i: 12900 , cost :  1.09096 , training accuracy :  0.736251\n",
      "i: 13000 , cost :  1.07431 , training accuracy :  0.839506\n",
      "i: 13100 , cost :  0.604134 , training accuracy :  0.863075\n",
      "i: 13200 , cost :  2.21382 , training accuracy :  0.723906\n",
      "i: 13300 , cost :  0.62167 , training accuracy :  0.866442\n",
      "i: 13400 , cost :  2.85158 , training accuracy :  0.806958\n",
      "i: 13500 , cost :  0.694728 , training accuracy :  0.86532\n",
      "i: 13600 , cost :  3.05566 , training accuracy :  0.749719\n",
      "i: 13700 , cost :  0.774042 , training accuracy :  0.854097\n",
      "i: 13800 , cost :  2.8144 , training accuracy :  0.794613\n",
      "i: 13900 , cost :  0.74672 , training accuracy :  0.86532\n",
      "i: 14000 , cost :  1.9413 , training accuracy :  0.791246\n",
      "i: 14100 , cost :  0.971961 , training accuracy :  0.858586\n",
      "i: 14200 , cost :  0.628972 , training accuracy :  0.860831\n",
      "i: 14300 , cost :  0.652876 , training accuracy :  0.854097\n",
      "i: 14400 , cost :  1.34752 , training accuracy :  0.830527\n",
      "i: 14500 , cost :  0.590802 , training accuracy :  0.86532\n",
      "i: 14600 , cost :  2.33703 , training accuracy :  0.675645\n",
      "i: 14700 , cost :  0.691064 , training accuracy :  0.868687\n",
      "i: 14800 , cost :  1.03695 , training accuracy :  0.838384\n",
      "i: 14900 , cost :  0.624596 , training accuracy :  0.861953\n",
      "i: 15000 , cost :  3.37511 , training accuracy :  0.79349\n",
      "i: 15100 , cost :  0.725711 , training accuracy :  0.868687\n",
      "i: 15200 , cost :  10.8199 , training accuracy :  0.396184\n",
      "i: 15300 , cost :  0.656464 , training accuracy :  0.867565\n",
      "i: 15400 , cost :  1.03182 , training accuracy :  0.83165\n",
      "i: 15500 , cost :  6.19298 , training accuracy :  0.703704\n",
      "i: 15600 , cost :  0.683137 , training accuracy :  0.868687\n",
      "i: 15700 , cost :  1.59607 , training accuracy :  0.814815\n",
      "i: 15800 , cost :  0.65484 , training accuracy :  0.868687\n",
      "i: 15900 , cost :  1.43551 , training accuracy :  0.771044\n",
      "i: 16000 , cost :  0.644388 , training accuracy :  0.870932\n",
      "i: 16100 , cost :  1.23741 , training accuracy :  0.833894\n",
      "i: 16200 , cost :  0.618785 , training accuracy :  0.868687\n",
      "i: 16300 , cost :  0.632132 , training accuracy :  0.826038\n",
      "i: 16400 , cost :  0.823261 , training accuracy :  0.854097\n",
      "i: 16500 , cost :  4.29189 , training accuracy :  0.7789\n",
      "i: 16600 , cost :  1.164 , training accuracy :  0.845118\n",
      "i: 16700 , cost :  0.5921 , training accuracy :  0.867565\n",
      "i: 16800 , cost :  2.54642 , training accuracy :  0.811448\n",
      "i: 16900 , cost :  0.597668 , training accuracy :  0.86532\n",
      "i: 17000 , cost :  1.66731 , training accuracy :  0.806958\n",
      "i: 17100 , cost :  0.665138 , training accuracy :  0.873176\n",
      "i: 17200 , cost :  4.00288 , training accuracy :  0.782267\n",
      "i: 17300 , cost :  0.618596 , training accuracy :  0.870932\n",
      "i: 17400 , cost :  3.30021 , training accuracy :  0.744108\n",
      "i: 17500 , cost :  0.567111 , training accuracy :  0.864198\n",
      "i: 17600 , cost :  0.961575 , training accuracy :  0.754209\n",
      "i: 17700 , cost :  1.64269 , training accuracy :  0.819304\n",
      "i: 17800 , cost :  0.699733 , training accuracy :  0.868687\n",
      "i: 17900 , cost :  3.03591 , training accuracy :  0.773288\n",
      "i: 18000 , cost :  0.625057 , training accuracy :  0.874299\n",
      "i: 18100 , cost :  1.17242 , training accuracy :  0.830527\n",
      "i: 18200 , cost :  0.935329 , training accuracy :  0.762065\n",
      "i: 18300 , cost :  0.846197 , training accuracy :  0.857464\n",
      "i: 18400 , cost :  0.56372 , training accuracy :  0.851852\n",
      "i: 18500 , cost :  0.794199 , training accuracy :  0.86532\n",
      "i: 18600 , cost :  3.6776 , training accuracy :  0.552189\n",
      "i: 18700 , cost :  1.60289 , training accuracy :  0.828283\n",
      "i: 18800 , cost :  1.66189 , training accuracy :  0.83165\n",
      "i: 18900 , cost :  0.788016 , training accuracy :  0.878788\n",
      "i: 19000 , cost :  1.35687 , training accuracy :  0.766554\n",
      "i: 19100 , cost :  0.692021 , training accuracy :  0.869809\n",
      "i: 19200 , cost :  5.33945 , training accuracy :  0.729517\n",
      "i: 19300 , cost :  0.76163 , training accuracy :  0.87991\n",
      "i: 19400 , cost :  0.883025 , training accuracy :  0.766554\n",
      "i: 19500 , cost :  3.49929 , training accuracy :  0.777778\n",
      "i: 19600 , cost :  0.687122 , training accuracy :  0.882155\n",
      "i: 19700 , cost :  9.03741 , training accuracy :  0.748597\n",
      "i: 19800 , cost :  0.928578 , training accuracy :  0.859708\n",
      "i: 19900 , cost :  2.13675 , training accuracy :  0.781145\n",
      "i: 20000 , cost :  0.893321 , training accuracy :  0.858586\n",
      "i: 20100 , cost :  9.55521 , training accuracy :  0.666667\n",
      "i: 20200 , cost :  6.85276 , training accuracy :  0.739618\n",
      "i: 20300 , cost :  0.723841 , training accuracy :  0.876543\n",
      "i: 20400 , cost :  7.36526 , training accuracy :  0.42312\n",
      "i: 20500 , cost :  0.617902 , training accuracy :  0.882155\n",
      "i: 20600 , cost :  4.6125 , training accuracy :  0.738496\n",
      "i: 20700 , cost :  0.607612 , training accuracy :  0.876543\n",
      "i: 20800 , cost :  0.998757 , training accuracy :  0.856341\n",
      "i: 20900 , cost :  0.670176 , training accuracy :  0.876543\n",
      "i: 21000 , cost :  6.02523 , training accuracy :  0.4422\n",
      "i: 21100 , cost :  0.605074 , training accuracy :  0.874299\n",
      "i: 21200 , cost :  3.97059 , training accuracy :  0.717172\n",
      "i: 21300 , cost :  0.605887 , training accuracy :  0.87991\n",
      "i: 21400 , cost :  4.84839 , training accuracy :  0.525253\n",
      "i: 21500 , cost :  0.634263 , training accuracy :  0.876543\n",
      "i: 21600 , cost :  1.32321 , training accuracy :  0.84624\n",
      "i: 21700 , cost :  0.638285 , training accuracy :  0.882155\n",
      "i: 21800 , cost :  1.45845 , training accuracy :  0.820426\n",
      "i: 21900 , cost :  0.602675 , training accuracy :  0.876543\n",
      "i: 22000 , cost :  9.66691 , training accuracy :  0.40404\n",
      "i: 22100 , cost :  0.730007 , training accuracy :  0.864198\n",
      "i: 22200 , cost :  5.62755 , training accuracy :  0.708193\n",
      "i: 22300 , cost :  1.08252 , training accuracy :  0.860831\n",
      "i: 22400 , cost :  0.566726 , training accuracy :  0.873176\n",
      "i: 22500 , cost :  7.27676 , training accuracy :  0.525253\n",
      "i: 22600 , cost :  1.54347 , training accuracy :  0.773288\n",
      "i: 22700 , cost :  3.42177 , training accuracy :  0.624018\n",
      "i: 22800 , cost :  1.04486 , training accuracy :  0.868687\n",
      "i: 22900 , cost :  0.528336 , training accuracy :  0.869809\n",
      "i: 23000 , cost :  0.702574 , training accuracy :  0.863075\n",
      "i: 23100 , cost :  4.8024 , training accuracy :  0.71156\n",
      "i: 23200 , cost :  0.632444 , training accuracy :  0.878788\n",
      "i: 23300 , cost :  0.963514 , training accuracy :  0.841751\n",
      "i: 23400 , cost :  0.645329 , training accuracy :  0.881033\n",
      "i: 23500 , cost :  1.77063 , training accuracy :  0.818182\n",
      "i: 23600 , cost :  0.582506 , training accuracy :  0.882155\n",
      "i: 23700 , cost :  0.858825 , training accuracy :  0.843996\n",
      "i: 23800 , cost :  8.40152 , training accuracy :  0.666667\n",
      "i: 23900 , cost :  0.748991 , training accuracy :  0.869809\n",
      "i: 24000 , cost :  10.5878 , training accuracy :  0.417508\n",
      "i: 24100 , cost :  0.589886 , training accuracy :  0.8844\n",
      "i: 24200 , cost :  4.76405 , training accuracy :  0.750842\n",
      "i: 24300 , cost :  0.653982 , training accuracy :  0.877666\n",
      "i: 24400 , cost :  4.85429 , training accuracy :  0.760943\n",
      "i: 24500 , cost :  0.521674 , training accuracy :  0.874299\n",
      "i: 24600 , cost :  0.914247 , training accuracy :  0.837261\n",
      "i: 24700 , cost :  1.52321 , training accuracy :  0.810326\n",
      "i: 24800 , cost :  0.71019 , training accuracy :  0.867565\n",
      "i: 24900 , cost :  0.998196 , training accuracy :  0.817059\n",
      "i: 25000 , cost :  1.03478 , training accuracy :  0.85073\n",
      "i: 25100 , cost :  0.651104 , training accuracy :  0.873176\n",
      "i: 25200 , cost :  5.9462 , training accuracy :  0.741863\n",
      "i: 25300 , cost :  0.581671 , training accuracy :  0.878788\n",
      "i: 25400 , cost :  0.540821 , training accuracy :  0.857464\n",
      "i: 25500 , cost :  3.78226 , training accuracy :  0.720539\n",
      "i: 25600 , cost :  0.530337 , training accuracy :  0.878788\n",
      "i: 25700 , cost :  1.81711 , training accuracy :  0.822671\n",
      "i: 25800 , cost :  2.36205 , training accuracy :  0.683502\n",
      "i: 25900 , cost :  1.01074 , training accuracy :  0.870932\n",
      "i: 26000 , cost :  0.526641 , training accuracy :  0.869809\n",
      "i: 26100 , cost :  6.31605 , training accuracy :  0.432099\n",
      "i: 26200 , cost :  2.506 , training accuracy :  0.796857\n",
      "i: 26300 , cost :  0.918699 , training accuracy :  0.869809\n",
      "i: 26400 , cost :  0.48757 , training accuracy :  0.870932\n",
      "i: 26500 , cost :  0.693325 , training accuracy :  0.869809\n",
      "i: 26600 , cost :  2.8376 , training accuracy :  0.517396\n",
      "i: 26700 , cost :  0.551356 , training accuracy :  0.877666\n",
      "i: 26800 , cost :  0.907715 , training accuracy :  0.836139\n",
      "i: 26900 , cost :  4.00741 , training accuracy :  0.777778\n",
      "i: 27000 , cost :  0.76587 , training accuracy :  0.876543\n",
      "i: 27100 , cost :  0.867958 , training accuracy :  0.836139\n",
      "i: 27200 , cost :  0.620789 , training accuracy :  0.881033\n",
      "i: 27300 , cost :  2.18526 , training accuracy :  0.800224\n",
      "i: 27400 , cost :  0.505748 , training accuracy :  0.876543\n",
      "i: 27500 , cost :  2.50243 , training accuracy :  0.767677\n",
      "i: 27600 , cost :  0.493524 , training accuracy :  0.877666\n",
      "i: 27700 , cost :  4.90643 , training accuracy :  0.763187\n",
      "i: 27800 , cost :  0.51996 , training accuracy :  0.87991\n",
      "i: 27900 , cost :  2.2728 , training accuracy :  0.710438\n",
      "i: 28000 , cost :  0.479749 , training accuracy :  0.878788\n",
      "i: 28100 , cost :  1.65644 , training accuracy :  0.818182\n",
      "i: 28200 , cost :  0.452035 , training accuracy :  0.872054\n",
      "i: 28300 , cost :  0.760908 , training accuracy :  0.864198\n",
      "i: 28400 , cost :  9.71338 , training accuracy :  0.393939\n",
      "i: 28500 , cost :  0.818408 , training accuracy :  0.866442\n",
      "i: 28600 , cost :  0.443085 , training accuracy :  0.874299\n",
      "i: 28700 , cost :  0.543945 , training accuracy :  0.881033\n",
      "i: 28800 , cost :  3.98862 , training accuracy :  0.781145\n",
      "i: 28900 , cost :  0.69124 , training accuracy :  0.882155\n",
      "i: 29000 , cost :  3.12418 , training accuracy :  0.775533\n",
      "i: 29100 , cost :  0.496398 , training accuracy :  0.87991\n",
      "i: 29200 , cost :  1.18151 , training accuracy :  0.840629\n",
      "i: 29300 , cost :  0.444524 , training accuracy :  0.874299\n",
      "i: 29400 , cost :  0.605104 , training accuracy :  0.87991\n",
      "i: 29500 , cost :  1.71925 , training accuracy :  0.702581\n",
      "i: 29600 , cost :  0.571408 , training accuracy :  0.885522\n",
      "i: 29700 , cost :  1.33036 , training accuracy :  0.823793\n",
      "i: 29800 , cost :  0.475479 , training accuracy :  0.876543\n",
      "i: 29900 , cost :  1.40637 , training accuracy :  0.753086\n",
      "Final training accuracy :  0.803591\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 30000 ][hidden: 50 ][file: bhavul_tr_acc_0.80_prediction.csv ] ACCURACY :  0.803591\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1208.3 , training accuracy :  0.616162\n",
      "i: 100 , cost :  4.70732 , training accuracy :  0.710438\n",
      "i: 200 , cost :  4.00343 , training accuracy :  0.746352\n",
      "i: 300 , cost :  1.95355 , training accuracy :  0.810326\n",
      "i: 400 , cost :  1.73549 , training accuracy :  0.801347\n",
      "i: 500 , cost :  1.57882 , training accuracy :  0.822671\n",
      "i: 600 , cost :  1.92847 , training accuracy :  0.826038\n",
      "i: 700 , cost :  1.44392 , training accuracy :  0.840629\n",
      "i: 800 , cost :  1.37697 , training accuracy :  0.836139\n",
      "i: 900 , cost :  1.19753 , training accuracy :  0.847363\n",
      "i: 1000 , cost :  4.57876 , training accuracy :  0.698092\n",
      "i: 1100 , cost :  1.04744 , training accuracy :  0.863075\n",
      "i: 1200 , cost :  2.79197 , training accuracy :  0.817059\n",
      "i: 1300 , cost :  0.890788 , training accuracy :  0.866442\n",
      "i: 1400 , cost :  3.77576 , training accuracy :  0.799102\n",
      "i: 1500 , cost :  1.45421 , training accuracy :  0.76431\n",
      "i: 1600 , cost :  0.987208 , training accuracy :  0.869809\n",
      "i: 1700 , cost :  1.06271 , training accuracy :  0.861953\n",
      "i: 1800 , cost :  13.0622 , training accuracy :  0.698092\n",
      "i: 1900 , cost :  0.86281 , training accuracy :  0.867565\n",
      "i: 2000 , cost :  6.72024 , training accuracy :  0.551066\n",
      "i: 2100 , cost :  0.834567 , training accuracy :  0.869809\n",
      "i: 2200 , cost :  1.92141 , training accuracy :  0.790123\n",
      "i: 2300 , cost :  2.98936 , training accuracy :  0.804714\n",
      "i: 2400 , cost :  1.22684 , training accuracy :  0.866442\n",
      "i: 2500 , cost :  4.6852 , training accuracy :  0.721661\n",
      "i: 2600 , cost :  1.08337 , training accuracy :  0.868687\n",
      "i: 2700 , cost :  0.876102 , training accuracy :  0.843996\n",
      "i: 2800 , cost :  0.999802 , training accuracy :  0.86532\n",
      "i: 2900 , cost :  6.57377 , training accuracy :  0.758698\n",
      "i: 3000 , cost :  0.951755 , training accuracy :  0.860831\n",
      "i: 3100 , cost :  6.1544 , training accuracy :  0.608305\n",
      "i: 3200 , cost :  0.75368 , training accuracy :  0.877666\n",
      "i: 3300 , cost :  1.30987 , training accuracy :  0.858586\n",
      "i: 3400 , cost :  7.54109 , training accuracy :  0.762065\n",
      "i: 3500 , cost :  1.25091 , training accuracy :  0.869809\n",
      "i: 3600 , cost :  1.44621 , training accuracy :  0.830527\n",
      "i: 3700 , cost :  0.930315 , training accuracy :  0.874299\n",
      "i: 3800 , cost :  6.41485 , training accuracy :  0.600449\n",
      "i: 3900 , cost :  0.87796 , training accuracy :  0.8844\n",
      "i: 4000 , cost :  4.42944 , training accuracy :  0.813693\n",
      "i: 4100 , cost :  0.7289 , training accuracy :  0.883277\n",
      "i: 4200 , cost :  0.868395 , training accuracy :  0.882155\n",
      "i: 4300 , cost :  3.79335 , training accuracy :  0.708193\n",
      "i: 4400 , cost :  0.940127 , training accuracy :  0.878788\n",
      "i: 4500 , cost :  4.64315 , training accuracy :  0.811448\n",
      "i: 4600 , cost :  0.687684 , training accuracy :  0.87991\n",
      "i: 4700 , cost :  0.692753 , training accuracy :  0.868687\n",
      "i: 4800 , cost :  1.53466 , training accuracy :  0.828283\n",
      "i: 4900 , cost :  2.12125 , training accuracy :  0.848485\n",
      "i: 5000 , cost :  0.672479 , training accuracy :  0.886644\n",
      "i: 5100 , cost :  0.667526 , training accuracy :  0.882155\n",
      "i: 5200 , cost :  2.28425 , training accuracy :  0.787879\n",
      "i: 5300 , cost :  7.02552 , training accuracy :  0.735129\n",
      "i: 5400 , cost :  4.14976 , training accuracy :  0.762065\n",
      "i: 5500 , cost :  1.02162 , training accuracy :  0.882155\n",
      "i: 5600 , cost :  14.3916 , training accuracy :  0.693603\n",
      "i: 5700 , cost :  0.63702 , training accuracy :  0.873176\n",
      "i: 5800 , cost :  0.9757 , training accuracy :  0.870932\n",
      "i: 5900 , cost :  18.2582 , training accuracy :  0.433221\n",
      "i: 6000 , cost :  0.785389 , training accuracy :  0.8844\n",
      "i: 6100 , cost :  0.945831 , training accuracy :  0.866442\n",
      "i: 6200 , cost :  0.817578 , training accuracy :  0.86532\n",
      "i: 6300 , cost :  5.77816 , training accuracy :  0.768799\n",
      "i: 6400 , cost :  0.677921 , training accuracy :  0.885522\n",
      "i: 6500 , cost :  4.6174 , training accuracy :  0.546577\n",
      "i: 6600 , cost :  0.700256 , training accuracy :  0.885522\n",
      "i: 6700 , cost :  6.88929 , training accuracy :  0.573513\n",
      "i: 6800 , cost :  0.648677 , training accuracy :  0.886644\n",
      "i: 6900 , cost :  3.54372 , training accuracy :  0.808081\n",
      "i: 7000 , cost :  0.664204 , training accuracy :  0.888889\n",
      "i: 7100 , cost :  0.845476 , training accuracy :  0.881033\n",
      "i: 7200 , cost :  1.85479 , training accuracy :  0.836139\n",
      "i: 7300 , cost :  0.896616 , training accuracy :  0.890011\n",
      "i: 7400 , cost :  4.98358 , training accuracy :  0.791246\n",
      "i: 7500 , cost :  3.40362 , training accuracy :  0.838384\n",
      "i: 7600 , cost :  0.705671 , training accuracy :  0.882155\n",
      "i: 7700 , cost :  4.7303 , training accuracy :  0.795735\n",
      "i: 7800 , cost :  0.606425 , training accuracy :  0.892256\n",
      "i: 7900 , cost :  0.710857 , training accuracy :  0.8844\n",
      "i: 8000 , cost :  0.644691 , training accuracy :  0.877666\n",
      "i: 8100 , cost :  0.665349 , training accuracy :  0.885522\n",
      "i: 8200 , cost :  1.72471 , training accuracy :  0.842873\n",
      "i: 8300 , cost :  4.56078 , training accuracy :  0.79349\n",
      "i: 8400 , cost :  0.637716 , training accuracy :  0.895623\n",
      "i: 8500 , cost :  2.83071 , training accuracy :  0.822671\n",
      "i: 8600 , cost :  3.29202 , training accuracy :  0.823793\n",
      "i: 8700 , cost :  0.907518 , training accuracy :  0.886644\n",
      "i: 8800 , cost :  13.6858 , training accuracy :  0.468013\n",
      "i: 8900 , cost :  0.569414 , training accuracy :  0.89899\n",
      "i: 9000 , cost :  0.59559 , training accuracy :  0.89899\n",
      "i: 9100 , cost :  3.55618 , training accuracy :  0.818182\n",
      "i: 9200 , cost :  1.14943 , training accuracy :  0.792368\n",
      "i: 9300 , cost :  0.805408 , training accuracy :  0.883277\n",
      "i: 9400 , cost :  2.71349 , training accuracy :  0.832772\n",
      "i: 9500 , cost :  5.89293 , training accuracy :  0.585859\n",
      "i: 9600 , cost :  0.582313 , training accuracy :  0.896745\n",
      "i: 9700 , cost :  0.722041 , training accuracy :  0.890011\n",
      "i: 9800 , cost :  0.694753 , training accuracy :  0.882155\n",
      "i: 9900 , cost :  8.69954 , training accuracy :  0.531987\n",
      "i: 10000 , cost :  8.23433 , training accuracy :  0.744108\n",
      "i: 10100 , cost :  4.39772 , training accuracy :  0.802469\n",
      "i: 10200 , cost :  0.676788 , training accuracy :  0.891134\n",
      "i: 10300 , cost :  1.50546 , training accuracy :  0.82716\n",
      "i: 10400 , cost :  0.578328 , training accuracy :  0.897868\n",
      "i: 10500 , cost :  9.38536 , training accuracy :  0.736251\n",
      "i: 10600 , cost :  0.551023 , training accuracy :  0.89899\n",
      "i: 10700 , cost :  1.79325 , training accuracy :  0.859708\n",
      "i: 10800 , cost :  0.606269 , training accuracy :  0.897868\n",
      "i: 10900 , cost :  1.26222 , training accuracy :  0.818182\n",
      "i: 11000 , cost :  1.01955 , training accuracy :  0.872054\n",
      "i: 11100 , cost :  6.28179 , training accuracy :  0.771044\n",
      "i: 11200 , cost :  0.583198 , training accuracy :  0.902357\n",
      "i: 11300 , cost :  3.48394 , training accuracy :  0.818182\n",
      "i: 11400 , cost :  0.543404 , training accuracy :  0.902357\n",
      "i: 11500 , cost :  2.92644 , training accuracy :  0.835017\n",
      "i: 11600 , cost :  0.557405 , training accuracy :  0.902357\n",
      "i: 11700 , cost :  0.867629 , training accuracy :  0.866442\n",
      "i: 11800 , cost :  15.4133 , training accuracy :  0.700337\n",
      "i: 11900 , cost :  0.596042 , training accuracy :  0.890011\n",
      "i: 12000 , cost :  2.82818 , training accuracy :  0.824916\n",
      "i: 12100 , cost :  0.545934 , training accuracy :  0.896745\n",
      "i: 12200 , cost :  1.52679 , training accuracy :  0.837261\n",
      "i: 12300 , cost :  0.574589 , training accuracy :  0.900112\n",
      "i: 12400 , cost :  2.72341 , training accuracy :  0.604938\n",
      "i: 12500 , cost :  0.510429 , training accuracy :  0.900112\n",
      "i: 12600 , cost :  0.967551 , training accuracy :  0.886644\n",
      "i: 12700 , cost :  1.51792 , training accuracy :  0.843996\n",
      "i: 12800 , cost :  1.34923 , training accuracy :  0.839506\n",
      "i: 12900 , cost :  0.523047 , training accuracy :  0.887767\n",
      "i: 13000 , cost :  1.05038 , training accuracy :  0.886644\n",
      "i: 13100 , cost :  11.7954 , training accuracy :  0.479237\n",
      "i: 13200 , cost :  1.0908 , training accuracy :  0.863075\n",
      "i: 13300 , cost :  0.540646 , training accuracy :  0.874299\n",
      "i: 13400 , cost :  1.21219 , training accuracy :  0.84624\n",
      "i: 13500 , cost :  1.67535 , training accuracy :  0.787879\n",
      "i: 13600 , cost :  0.504048 , training accuracy :  0.891134\n",
      "i: 13700 , cost :  0.991695 , training accuracy :  0.790123\n",
      "i: 13800 , cost :  4.34713 , training accuracy :  0.517396\n",
      "i: 13900 , cost :  0.630354 , training accuracy :  0.895623\n",
      "i: 14000 , cost :  0.545147 , training accuracy :  0.895623\n",
      "i: 14100 , cost :  6.24175 , training accuracy :  0.506173\n",
      "i: 14200 , cost :  0.509312 , training accuracy :  0.902357\n",
      "i: 14300 , cost :  5.15542 , training accuracy :  0.600449\n",
      "i: 14400 , cost :  0.487681 , training accuracy :  0.901235\n",
      "i: 14500 , cost :  7.1962 , training accuracy :  0.776655\n",
      "i: 14600 , cost :  0.921463 , training accuracy :  0.897868\n",
      "i: 14700 , cost :  0.566138 , training accuracy :  0.902357\n",
      "i: 14800 , cost :  3.70589 , training accuracy :  0.810326\n",
      "i: 14900 , cost :  0.498527 , training accuracy :  0.903479\n",
      "i: 15000 , cost :  2.04746 , training accuracy :  0.784512\n",
      "i: 15100 , cost :  0.726729 , training accuracy :  0.897868\n",
      "i: 15200 , cost :  3.30622 , training accuracy :  0.814815\n",
      "i: 15300 , cost :  1.20513 , training accuracy :  0.874299\n",
      "i: 15400 , cost :  0.593362 , training accuracy :  0.870932\n",
      "i: 15500 , cost :  2.72395 , training accuracy :  0.82716\n",
      "i: 15600 , cost :  0.485641 , training accuracy :  0.894501\n",
      "i: 15700 , cost :  7.20686 , training accuracy :  0.769921\n",
      "i: 15800 , cost :  0.497625 , training accuracy :  0.905724\n",
      "i: 15900 , cost :  3.50094 , training accuracy :  0.839506\n",
      "i: 16000 , cost :  2.18301 , training accuracy :  0.809203\n",
      "i: 16100 , cost :  5.9214 , training accuracy :  0.531987\n",
      "i: 16200 , cost :  0.677665 , training accuracy :  0.901235\n",
      "i: 16300 , cost :  0.526212 , training accuracy :  0.903479\n",
      "i: 16400 , cost :  4.17016 , training accuracy :  0.811448\n",
      "i: 16500 , cost :  0.54574 , training accuracy :  0.905724\n",
      "i: 16600 , cost :  2.92065 , training accuracy :  0.847363\n",
      "i: 16700 , cost :  1.90687 , training accuracy :  0.842873\n",
      "i: 16800 , cost :  0.502623 , training accuracy :  0.902357\n",
      "i: 16900 , cost :  16.3472 , training accuracy :  0.433221\n",
      "i: 17000 , cost :  12.7854 , training accuracy :  0.500561\n",
      "i: 17100 , cost :  1.10974 , training accuracy :  0.882155\n",
      "i: 17200 , cost :  0.600181 , training accuracy :  0.903479\n",
      "i: 17300 , cost :  0.498086 , training accuracy :  0.891134\n",
      "i: 17400 , cost :  0.5191 , training accuracy :  0.896745\n",
      "i: 17500 , cost :  6.68437 , training accuracy :  0.499439\n",
      "i: 17600 , cost :  0.722789 , training accuracy :  0.885522\n",
      "i: 17700 , cost :  5.11289 , training accuracy :  0.781145\n",
      "i: 17800 , cost :  0.643938 , training accuracy :  0.894501\n",
      "i: 17900 , cost :  2.85563 , training accuracy :  0.803591\n",
      "i: 18000 , cost :  0.605934 , training accuracy :  0.897868\n",
      "i: 18100 , cost :  8.76736 , training accuracy :  0.769921\n",
      "i: 18200 , cost :  0.550187 , training accuracy :  0.906846\n",
      "i: 18300 , cost :  2.24364 , training accuracy :  0.718294\n",
      "i: 18400 , cost :  4.07289 , training accuracy :  0.828283\n",
      "i: 18500 , cost :  1.2705 , training accuracy :  0.885522\n",
      "i: 18600 , cost :  0.6648 , training accuracy :  0.904602\n",
      "i: 18700 , cost :  0.462612 , training accuracy :  0.903479\n",
      "i: 18800 , cost :  0.59013 , training accuracy :  0.892256\n",
      "i: 18900 , cost :  2.38237 , training accuracy :  0.845118\n",
      "i: 19000 , cost :  4.27429 , training accuracy :  0.62514\n",
      "i: 19100 , cost :  0.544288 , training accuracy :  0.906846\n",
      "i: 19200 , cost :  1.1884 , training accuracy :  0.838384\n",
      "i: 19300 , cost :  0.720892 , training accuracy :  0.896745\n",
      "i: 19400 , cost :  0.581609 , training accuracy :  0.904602\n",
      "i: 19500 , cost :  2.36536 , training accuracy :  0.775533\n",
      "i: 19600 , cost :  0.453615 , training accuracy :  0.905724\n",
      "i: 19700 , cost :  4.66438 , training accuracy :  0.800224\n",
      "i: 19800 , cost :  0.669499 , training accuracy :  0.903479\n",
      "i: 19900 , cost :  0.645789 , training accuracy :  0.897868\n",
      "i: 20000 , cost :  1.48187 , training accuracy :  0.802469\n",
      "i: 20100 , cost :  0.443397 , training accuracy :  0.910213\n",
      "i: 20200 , cost :  1.20977 , training accuracy :  0.868687\n",
      "i: 20300 , cost :  0.799046 , training accuracy :  0.900112\n",
      "i: 20400 , cost :  0.432871 , training accuracy :  0.909091\n",
      "i: 20500 , cost :  4.23288 , training accuracy :  0.811448\n",
      "i: 20600 , cost :  0.431924 , training accuracy :  0.909091\n",
      "i: 20700 , cost :  14.0049 , training accuracy :  0.725028\n",
      "i: 20800 , cost :  1.24108 , training accuracy :  0.828283\n",
      "i: 20900 , cost :  0.455261 , training accuracy :  0.907969\n",
      "i: 21000 , cost :  2.98124 , training accuracy :  0.849607\n",
      "i: 21100 , cost :  8.1677 , training accuracy :  0.760943\n",
      "i: 21200 , cost :  0.899377 , training accuracy :  0.891134\n",
      "i: 21300 , cost :  0.520343 , training accuracy :  0.911336\n",
      "i: 21400 , cost :  10.8654 , training accuracy :  0.468013\n",
      "i: 21500 , cost :  0.729258 , training accuracy :  0.903479\n",
      "i: 21600 , cost :  0.436318 , training accuracy :  0.895623\n",
      "i: 21700 , cost :  6.59144 , training accuracy :  0.771044\n",
      "i: 21800 , cost :  0.50265 , training accuracy :  0.874299\n",
      "i: 21900 , cost :  2.62817 , training accuracy :  0.849607\n",
      "i: 22000 , cost :  0.449368 , training accuracy :  0.911336\n",
      "i: 22100 , cost :  4.44309 , training accuracy :  0.546577\n",
      "i: 22200 , cost :  10.2077 , training accuracy :  0.737374\n",
      "i: 22300 , cost :  1.16455 , training accuracy :  0.894501\n",
      "i: 22400 , cost :  0.755158 , training accuracy :  0.89899\n",
      "i: 22500 , cost :  0.482549 , training accuracy :  0.906846\n",
      "i: 22600 , cost :  3.20957 , training accuracy :  0.725028\n",
      "i: 22700 , cost :  0.589271 , training accuracy :  0.901235\n",
      "i: 22800 , cost :  0.411923 , training accuracy :  0.900112\n",
      "i: 22900 , cost :  0.407205 , training accuracy :  0.911336\n",
      "i: 23000 , cost :  1.20098 , training accuracy :  0.877666\n",
      "i: 23100 , cost :  0.437703 , training accuracy :  0.912458\n",
      "i: 23200 , cost :  2.05504 , training accuracy :  0.805836\n",
      "i: 23300 , cost :  1.47342 , training accuracy :  0.847363\n",
      "i: 23400 , cost :  0.515206 , training accuracy :  0.909091\n",
      "i: 23500 , cost :  0.471466 , training accuracy :  0.878788\n",
      "i: 23600 , cost :  0.827291 , training accuracy :  0.905724\n",
      "i: 23700 , cost :  0.463273 , training accuracy :  0.882155\n",
      "i: 23800 , cost :  3.91275 , training accuracy :  0.719416\n",
      "i: 23900 , cost :  0.438268 , training accuracy :  0.906846\n",
      "i: 24000 , cost :  0.756943 , training accuracy :  0.903479\n",
      "i: 24100 , cost :  0.779346 , training accuracy :  0.905724\n",
      "i: 24200 , cost :  0.41123 , training accuracy :  0.902357\n",
      "i: 24300 , cost :  4.26699 , training accuracy :  0.794613\n",
      "i: 24400 , cost :  0.458039 , training accuracy :  0.910213\n",
      "i: 24500 , cost :  0.477225 , training accuracy :  0.878788\n",
      "i: 24600 , cost :  7.05225 , training accuracy :  0.804714\n",
      "i: 24700 , cost :  0.83351 , training accuracy :  0.896745\n",
      "i: 24800 , cost :  0.480018 , training accuracy :  0.910213\n",
      "i: 24900 , cost :  0.487383 , training accuracy :  0.910213\n",
      "i: 25000 , cost :  7.51909 , training accuracy :  0.515152\n",
      "i: 25100 , cost :  0.898849 , training accuracy :  0.905724\n",
      "i: 25200 , cost :  0.478494 , training accuracy :  0.911336\n",
      "i: 25300 , cost :  0.506998 , training accuracy :  0.912458\n",
      "i: 25400 , cost :  8.25872 , training accuracy :  0.776655\n",
      "i: 25500 , cost :  1.6556 , training accuracy :  0.87991\n",
      "i: 25600 , cost :  0.571395 , training accuracy :  0.912458\n",
      "i: 25700 , cost :  0.406949 , training accuracy :  0.91358\n",
      "i: 25800 , cost :  0.924927 , training accuracy :  0.860831\n",
      "i: 25900 , cost :  8.00014 , training accuracy :  0.780022\n",
      "i: 26000 , cost :  0.880539 , training accuracy :  0.901235\n",
      "i: 26100 , cost :  0.481582 , training accuracy :  0.912458\n",
      "i: 26200 , cost :  0.47259 , training accuracy :  0.911336\n",
      "i: 26300 , cost :  4.06698 , training accuracy :  0.776655\n",
      "i: 26400 , cost :  0.706313 , training accuracy :  0.903479\n",
      "i: 26500 , cost :  0.431684 , training accuracy :  0.912458\n",
      "i: 26600 , cost :  0.404369 , training accuracy :  0.888889\n",
      "i: 26700 , cost :  4.58497 , training accuracy :  0.810326\n",
      "i: 26800 , cost :  0.602741 , training accuracy :  0.911336\n",
      "i: 26900 , cost :  0.398254 , training accuracy :  0.912458\n",
      "i: 27000 , cost :  1.43449 , training accuracy :  0.843996\n",
      "i: 27100 , cost :  1.45941 , training accuracy :  0.868687\n",
      "i: 27200 , cost :  0.402711 , training accuracy :  0.911336\n",
      "i: 27300 , cost :  0.413918 , training accuracy :  0.910213\n",
      "i: 27400 , cost :  4.63774 , training accuracy :  0.606061\n",
      "i: 27500 , cost :  0.66666 , training accuracy :  0.907969\n",
      "i: 27600 , cost :  0.526151 , training accuracy :  0.907969\n",
      "i: 27700 , cost :  1.90322 , training accuracy :  0.860831\n",
      "i: 27800 , cost :  0.380806 , training accuracy :  0.911336\n",
      "i: 27900 , cost :  2.3604 , training accuracy :  0.845118\n",
      "i: 28000 , cost :  0.417606 , training accuracy :  0.91807\n",
      "i: 28100 , cost :  0.465786 , training accuracy :  0.878788\n",
      "i: 28200 , cost :  2.14093 , training accuracy :  0.86532\n",
      "i: 28300 , cost :  1.1901 , training accuracy :  0.859708\n",
      "i: 28400 , cost :  0.46333 , training accuracy :  0.915825\n",
      "i: 28500 , cost :  0.372364 , training accuracy :  0.914703\n",
      "i: 28600 , cost :  3.84678 , training accuracy :  0.843996\n",
      "i: 28700 , cost :  4.61156 , training accuracy :  0.699214\n",
      "i: 28800 , cost :  0.577496 , training accuracy :  0.911336\n",
      "i: 28900 , cost :  0.378545 , training accuracy :  0.914703\n",
      "i: 29000 , cost :  0.587105 , training accuracy :  0.842873\n",
      "i: 29100 , cost :  1.0624 , training accuracy :  0.885522\n",
      "i: 29200 , cost :  0.369004 , training accuracy :  0.919192\n",
      "i: 29300 , cost :  2.04304 , training accuracy :  0.646465\n",
      "i: 29400 , cost :  0.370729 , training accuracy :  0.914703\n",
      "i: 29500 , cost :  3.78972 , training accuracy :  0.848485\n",
      "i: 29600 , cost :  1.23934 , training accuracy :  0.888889\n",
      "i: 29700 , cost :  0.712605 , training accuracy :  0.905724\n",
      "i: 29800 , cost :  0.415822 , training accuracy :  0.915825\n",
      "i: 29900 , cost :  0.481306 , training accuracy :  0.907969\n",
      "Final training accuracy :  0.906846\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.01 ][epoch: 30000 ][hidden: 100 ][file: bhavul_tr_acc_0.91_prediction.csv ] ACCURACY :  0.906846\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  401.859 , training accuracy :  0.624018\n",
      "i: 100 , cost :  5.61887 , training accuracy :  0.64422\n",
      "i: 200 , cost :  0.649333 , training accuracy :  0.734007\n",
      "i: 300 , cost :  0.577837 , training accuracy :  0.75982\n",
      "i: 400 , cost :  0.542989 , training accuracy :  0.769921\n",
      "i: 500 , cost :  0.536023 , training accuracy :  0.766554\n",
      "i: 600 , cost :  0.534071 , training accuracy :  0.765432\n",
      "i: 700 , cost :  0.532689 , training accuracy :  0.765432\n",
      "i: 800 , cost :  0.531155 , training accuracy :  0.76431\n",
      "i: 900 , cost :  0.529319 , training accuracy :  0.763187\n",
      "Final training accuracy :  0.760943\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 1000 ][hidden: 3 ][file: bhavul_tr_acc_0.76_prediction.csv ] ACCURACY :  0.760943\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  400.805 , training accuracy :  0.373737\n",
      "i: 100 , cost :  12.0434 , training accuracy :  0.59147\n",
      "i: 200 , cost :  3.40839 , training accuracy :  0.382716\n",
      "i: 300 , cost :  0.633472 , training accuracy :  0.691358\n",
      "i: 400 , cost :  0.603431 , training accuracy :  0.684624\n",
      "i: 500 , cost :  0.5944 , training accuracy :  0.686869\n",
      "i: 600 , cost :  0.618028 , training accuracy :  0.693603\n",
      "i: 700 , cost :  0.612584 , training accuracy :  0.702581\n",
      "i: 800 , cost :  0.604747 , training accuracy :  0.720539\n",
      "i: 900 , cost :  0.594939 , training accuracy :  0.736251\n",
      "Final training accuracy :  0.741863\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 1000 ][hidden: 10 ][file: bhavul_tr_acc_0.74_prediction.csv ] ACCURACY :  0.741863\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  458.584 , training accuracy :  0.380471\n",
      "i: 100 , cost :  3.91341 , training accuracy :  0.693603\n",
      "i: 200 , cost :  5.36521 , training accuracy :  0.409652\n",
      "i: 300 , cost :  0.654531 , training accuracy :  0.789001\n",
      "i: 400 , cost :  0.944572 , training accuracy :  0.775533\n",
      "i: 500 , cost :  0.558464 , training accuracy :  0.791246\n",
      "i: 600 , cost :  3.65841 , training accuracy :  0.681257\n",
      "i: 700 , cost :  0.616201 , training accuracy :  0.796857\n",
      "i: 800 , cost :  1.62162 , training accuracy :  0.746352\n",
      "i: 900 , cost :  0.895827 , training accuracy :  0.774411\n",
      "Final training accuracy :  0.775533\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 1000 ][hidden: 15 ][file: bhavul_tr_acc_0.78_prediction.csv ] ACCURACY :  0.775533\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  340.536 , training accuracy :  0.459035\n",
      "i: 100 , cost :  3.43496 , training accuracy :  0.772166\n",
      "i: 200 , cost :  3.91735 , training accuracy :  0.771044\n",
      "i: 300 , cost :  1.93955 , training accuracy :  0.794613\n",
      "i: 400 , cost :  1.85423 , training accuracy :  0.777778\n",
      "i: 500 , cost :  6.27841 , training accuracy :  0.516274\n",
      "i: 600 , cost :  5.19087 , training accuracy :  0.758698\n",
      "i: 700 , cost :  1.74485 , training accuracy :  0.837261\n",
      "i: 800 , cost :  3.72875 , training accuracy :  0.75982\n",
      "i: 900 , cost :  1.23264 , training accuracy :  0.837261\n",
      "Final training accuracy :  0.738496\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 1000 ][hidden: 50 ][file: bhavul_tr_acc_0.74_prediction.csv ] ACCURACY :  0.738496\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  854.089 , training accuracy :  0.383838\n",
      "i: 100 , cost :  21.699 , training accuracy :  0.4422\n",
      "i: 200 , cost :  4.35847 , training accuracy :  0.786756\n",
      "i: 300 , cost :  3.1861 , training accuracy :  0.820426\n",
      "i: 400 , cost :  14.8555 , training accuracy :  0.717172\n",
      "i: 500 , cost :  11.3397 , training accuracy :  0.517396\n",
      "i: 600 , cost :  23.5444 , training accuracy :  0.425365\n",
      "i: 700 , cost :  5.69038 , training accuracy :  0.789001\n",
      "i: 800 , cost :  14.2029 , training accuracy :  0.718294\n",
      "i: 900 , cost :  2.53366 , training accuracy :  0.843996\n",
      "Final training accuracy :  0.465769\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 1000 ][hidden: 100 ][file: bhavul_tr_acc_0.47_prediction.csv ] ACCURACY :  0.465769\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  404.077 , training accuracy :  0.616162\n",
      "i: 100 , cost :  4.42501 , training accuracy :  0.466891\n",
      "i: 200 , cost :  0.688664 , training accuracy :  0.657688\n",
      "i: 300 , cost :  0.631631 , training accuracy :  0.666667\n",
      "i: 400 , cost :  0.629503 , training accuracy :  0.668911\n",
      "i: 500 , cost :  0.62813 , training accuracy :  0.672278\n",
      "i: 600 , cost :  0.627016 , training accuracy :  0.674523\n",
      "i: 700 , cost :  0.626068 , training accuracy :  0.674523\n",
      "i: 800 , cost :  0.625117 , training accuracy :  0.673401\n",
      "i: 900 , cost :  0.624096 , training accuracy :  0.672278\n",
      "i: 1000 , cost :  0.623136 , training accuracy :  0.672278\n",
      "i: 1100 , cost :  0.622164 , training accuracy :  0.672278\n",
      "i: 1200 , cost :  0.62126 , training accuracy :  0.673401\n",
      "i: 1300 , cost :  0.620326 , training accuracy :  0.675645\n",
      "i: 1400 , cost :  0.619487 , training accuracy :  0.673401\n",
      "i: 1500 , cost :  0.618785 , training accuracy :  0.676768\n",
      "i: 1600 , cost :  0.618176 , training accuracy :  0.679012\n",
      "i: 1700 , cost :  0.617547 , training accuracy :  0.680135\n",
      "i: 1800 , cost :  0.616958 , training accuracy :  0.680135\n",
      "i: 1900 , cost :  0.61612 , training accuracy :  0.687991\n",
      "i: 2000 , cost :  0.615127 , training accuracy :  0.686869\n",
      "i: 2100 , cost :  0.61397 , training accuracy :  0.684624\n",
      "i: 2200 , cost :  0.612917 , training accuracy :  0.687991\n",
      "i: 2300 , cost :  0.612107 , training accuracy :  0.690236\n",
      "i: 2400 , cost :  0.611938 , training accuracy :  0.686869\n",
      "i: 2500 , cost :  0.610878 , training accuracy :  0.69248\n",
      "i: 2600 , cost :  0.610315 , training accuracy :  0.69248\n",
      "i: 2700 , cost :  0.609758 , training accuracy :  0.69697\n",
      "i: 2800 , cost :  0.609201 , training accuracy :  0.69697\n",
      "i: 2900 , cost :  0.608619 , training accuracy :  0.69697\n",
      "i: 3000 , cost :  0.607997 , training accuracy :  0.69697\n",
      "i: 3100 , cost :  0.607245 , training accuracy :  0.698092\n",
      "i: 3200 , cost :  0.606413 , training accuracy :  0.703704\n",
      "i: 3300 , cost :  0.605335 , training accuracy :  0.703704\n",
      "i: 3400 , cost :  0.604998 , training accuracy :  0.695847\n",
      "i: 3500 , cost :  0.599298 , training accuracy :  0.712682\n",
      "i: 3600 , cost :  0.59502 , training accuracy :  0.713805\n",
      "i: 3700 , cost :  0.590535 , training accuracy :  0.718294\n",
      "i: 3800 , cost :  0.583915 , training accuracy :  0.725028\n",
      "i: 3900 , cost :  0.571619 , training accuracy :  0.731762\n",
      "i: 4000 , cost :  0.523635 , training accuracy :  0.768799\n",
      "i: 4100 , cost :  0.47166 , training accuracy :  0.800224\n",
      "i: 4200 , cost :  0.477365 , training accuracy :  0.792368\n",
      "i: 4300 , cost :  0.447507 , training accuracy :  0.813693\n",
      "i: 4400 , cost :  0.443965 , training accuracy :  0.818182\n",
      "i: 4500 , cost :  0.4426 , training accuracy :  0.818182\n",
      "i: 4600 , cost :  0.44334 , training accuracy :  0.821549\n",
      "i: 4700 , cost :  0.441823 , training accuracy :  0.820426\n",
      "i: 4800 , cost :  0.442107 , training accuracy :  0.819304\n",
      "i: 4900 , cost :  0.439968 , training accuracy :  0.819304\n",
      "Final training accuracy :  0.818182\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 5000 ][hidden: 3 ][file: bhavul_tr_acc_0.82_prediction.csv ] ACCURACY :  0.818182\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  80.58 , training accuracy :  0.613917\n",
      "i: 100 , cost :  6.85711 , training accuracy :  0.384961\n",
      "i: 200 , cost :  0.51825 , training accuracy :  0.791246\n",
      "i: 300 , cost :  0.561113 , training accuracy :  0.754209\n",
      "i: 400 , cost :  0.551777 , training accuracy :  0.749719\n",
      "i: 500 , cost :  0.547985 , training accuracy :  0.753086\n",
      "i: 600 , cost :  7.45437 , training accuracy :  0.384961\n",
      "i: 700 , cost :  1.75764 , training accuracy :  0.716049\n",
      "i: 800 , cost :  0.748957 , training accuracy :  0.728395\n",
      "i: 900 , cost :  0.507684 , training accuracy :  0.792368\n",
      "i: 1000 , cost :  0.479049 , training accuracy :  0.787879\n",
      "i: 1100 , cost :  0.463402 , training accuracy :  0.79349\n",
      "i: 1200 , cost :  0.454962 , training accuracy :  0.794613\n",
      "i: 1300 , cost :  0.450205 , training accuracy :  0.795735\n",
      "i: 1400 , cost :  0.447398 , training accuracy :  0.795735\n",
      "i: 1500 , cost :  0.520838 , training accuracy :  0.776655\n",
      "i: 1600 , cost :  0.445751 , training accuracy :  0.800224\n",
      "i: 1700 , cost :  0.746376 , training accuracy :  0.717172\n",
      "i: 1800 , cost :  0.44538 , training accuracy :  0.800224\n",
      "i: 1900 , cost :  0.44492 , training accuracy :  0.79798\n",
      "i: 2000 , cost :  0.520647 , training accuracy :  0.7789\n",
      "i: 2100 , cost :  0.52032 , training accuracy :  0.783389\n",
      "i: 2200 , cost :  0.52088 , training accuracy :  0.787879\n",
      "i: 2300 , cost :  0.521179 , training accuracy :  0.787879\n",
      "i: 2400 , cost :  0.521455 , training accuracy :  0.787879\n",
      "i: 2500 , cost :  0.52208 , training accuracy :  0.789001\n",
      "i: 2600 , cost :  0.520481 , training accuracy :  0.789001\n",
      "i: 2700 , cost :  0.517514 , training accuracy :  0.790123\n",
      "i: 2800 , cost :  0.520534 , training accuracy :  0.789001\n",
      "i: 2900 , cost :  0.514903 , training accuracy :  0.792368\n",
      "i: 3000 , cost :  0.51779 , training accuracy :  0.79349\n",
      "i: 3100 , cost :  0.513368 , training accuracy :  0.794613\n",
      "i: 3200 , cost :  0.512386 , training accuracy :  0.792368\n",
      "i: 3300 , cost :  0.512278 , training accuracy :  0.791246\n",
      "i: 3400 , cost :  0.512063 , training accuracy :  0.791246\n",
      "i: 3500 , cost :  0.511036 , training accuracy :  0.791246\n",
      "i: 3600 , cost :  0.510213 , training accuracy :  0.792368\n",
      "i: 3700 , cost :  0.508369 , training accuracy :  0.79349\n",
      "i: 3800 , cost :  0.506881 , training accuracy :  0.79349\n",
      "i: 3900 , cost :  0.506835 , training accuracy :  0.794613\n",
      "i: 4000 , cost :  0.505529 , training accuracy :  0.795735\n",
      "i: 4100 , cost :  0.504903 , training accuracy :  0.795735\n",
      "i: 4200 , cost :  0.504081 , training accuracy :  0.796857\n",
      "i: 4300 , cost :  0.503415 , training accuracy :  0.795735\n",
      "i: 4400 , cost :  0.502446 , training accuracy :  0.795735\n",
      "i: 4500 , cost :  0.50212 , training accuracy :  0.796857\n",
      "i: 4600 , cost :  0.50053 , training accuracy :  0.796857\n",
      "i: 4700 , cost :  0.499494 , training accuracy :  0.79798\n",
      "i: 4800 , cost :  0.50068 , training accuracy :  0.79798\n",
      "i: 4900 , cost :  0.498516 , training accuracy :  0.79798\n",
      "Final training accuracy :  0.799102\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 5000 ][hidden: 10 ][file: bhavul_tr_acc_0.80_prediction.csv ] ACCURACY :  0.799102\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  806.562 , training accuracy :  0.383838\n",
      "i: 100 , cost :  3.55027 , training accuracy :  0.700337\n",
      "i: 200 , cost :  2.44963 , training accuracy :  0.685746\n",
      "i: 300 , cost :  1.88882 , training accuracy :  0.704826\n",
      "i: 400 , cost :  0.844201 , training accuracy :  0.79349\n",
      "i: 500 , cost :  0.606747 , training accuracy :  0.795735\n",
      "i: 600 , cost :  1.11413 , training accuracy :  0.751964\n",
      "i: 700 , cost :  4.69679 , training accuracy :  0.650954\n",
      "i: 800 , cost :  2.34371 , training accuracy :  0.71156\n",
      "i: 900 , cost :  1.00854 , training accuracy :  0.784512\n",
      "i: 1000 , cost :  0.73134 , training accuracy :  0.799102\n",
      "i: 1100 , cost :  0.54564 , training accuracy :  0.806958\n",
      "i: 1200 , cost :  0.736786 , training accuracy :  0.780022\n",
      "i: 1300 , cost :  0.87502 , training accuracy :  0.784512\n",
      "i: 1400 , cost :  0.691974 , training accuracy :  0.802469\n",
      "i: 1500 , cost :  0.519305 , training accuracy :  0.817059\n",
      "i: 1600 , cost :  3.58291 , training accuracy :  0.679012\n",
      "i: 1700 , cost :  0.748851 , training accuracy :  0.791246\n",
      "i: 1800 , cost :  0.477879 , training accuracy :  0.814815\n",
      "i: 1900 , cost :  1.40406 , training accuracy :  0.747475\n",
      "i: 2000 , cost :  0.745191 , training accuracy :  0.75982\n",
      "i: 2100 , cost :  0.440886 , training accuracy :  0.821549\n",
      "i: 2200 , cost :  1.03027 , training accuracy :  0.771044\n",
      "i: 2300 , cost :  0.460738 , training accuracy :  0.813693\n",
      "i: 2400 , cost :  1.27144 , training accuracy :  0.74523\n",
      "i: 2500 , cost :  1.11217 , training accuracy :  0.771044\n",
      "i: 2600 , cost :  0.515366 , training accuracy :  0.82716\n",
      "i: 2700 , cost :  0.429777 , training accuracy :  0.822671\n",
      "i: 2800 , cost :  0.815917 , training accuracy :  0.780022\n",
      "i: 2900 , cost :  0.493018 , training accuracy :  0.829405\n",
      "i: 3000 , cost :  0.647586 , training accuracy :  0.74523\n",
      "i: 3100 , cost :  1.18647 , training accuracy :  0.771044\n",
      "i: 3200 , cost :  0.491347 , training accuracy :  0.824916\n",
      "i: 3300 , cost :  0.42498 , training accuracy :  0.819304\n",
      "i: 3400 , cost :  1.27317 , training accuracy :  0.535354\n",
      "i: 3500 , cost :  0.655519 , training accuracy :  0.766554\n",
      "i: 3600 , cost :  0.427505 , training accuracy :  0.823793\n",
      "i: 3700 , cost :  4.1653 , training accuracy :  0.397306\n",
      "i: 3800 , cost :  0.490141 , training accuracy :  0.835017\n",
      "i: 3900 , cost :  0.427664 , training accuracy :  0.836139\n",
      "i: 4000 , cost :  0.65524 , training accuracy :  0.801347\n",
      "i: 4100 , cost :  0.441298 , training accuracy :  0.822671\n",
      "i: 4200 , cost :  1.35461 , training accuracy :  0.775533\n",
      "i: 4300 , cost :  0.54057 , training accuracy :  0.832772\n",
      "i: 4400 , cost :  0.431397 , training accuracy :  0.828283\n",
      "i: 4500 , cost :  0.461999 , training accuracy :  0.813693\n",
      "i: 4600 , cost :  0.68038 , training accuracy :  0.794613\n",
      "i: 4700 , cost :  0.612074 , training accuracy :  0.820426\n",
      "i: 4800 , cost :  0.433757 , training accuracy :  0.833894\n",
      "i: 4900 , cost :  0.56619 , training accuracy :  0.800224\n",
      "Final training accuracy :  0.708193\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 5000 ][hidden: 15 ][file: bhavul_tr_acc_0.71_prediction.csv ] ACCURACY :  0.708193\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1707.59 , training accuracy :  0.380471\n",
      "i: 100 , cost :  8.17891 , training accuracy :  0.603816\n",
      "i: 200 , cost :  5.36638 , training accuracy :  0.755331\n",
      "i: 300 , cost :  3.1878 , training accuracy :  0.794613\n",
      "i: 400 , cost :  2.95343 , training accuracy :  0.803591\n",
      "i: 500 , cost :  2.60963 , training accuracy :  0.815937\n",
      "i: 600 , cost :  11.9189 , training accuracy :  0.736251\n",
      "i: 700 , cost :  12.6771 , training accuracy :  0.700337\n",
      "i: 800 , cost :  5.23863 , training accuracy :  0.780022\n",
      "i: 900 , cost :  2.15119 , training accuracy :  0.824916\n",
      "i: 1000 , cost :  4.52928 , training accuracy :  0.769921\n",
      "i: 1100 , cost :  2.60473 , training accuracy :  0.791246\n",
      "i: 1200 , cost :  5.75163 , training accuracy :  0.782267\n",
      "i: 1300 , cost :  2.31682 , training accuracy :  0.810326\n",
      "i: 1400 , cost :  1.6912 , training accuracy :  0.839506\n",
      "i: 1500 , cost :  9.12672 , training accuracy :  0.721661\n",
      "i: 1600 , cost :  3.11822 , training accuracy :  0.823793\n",
      "i: 1700 , cost :  1.25903 , training accuracy :  0.843996\n",
      "i: 1800 , cost :  5.12426 , training accuracy :  0.775533\n",
      "i: 1900 , cost :  1.28042 , training accuracy :  0.84624\n",
      "i: 2000 , cost :  5.85392 , training accuracy :  0.622896\n",
      "i: 2100 , cost :  1.55321 , training accuracy :  0.840629\n",
      "i: 2200 , cost :  8.75018 , training accuracy :  0.731762\n",
      "i: 2300 , cost :  6.09523 , training accuracy :  0.608305\n",
      "i: 2400 , cost :  1.5863 , training accuracy :  0.84624\n",
      "i: 2500 , cost :  1.38512 , training accuracy :  0.783389\n",
      "i: 2600 , cost :  7.51869 , training accuracy :  0.754209\n",
      "i: 2700 , cost :  1.57655 , training accuracy :  0.848485\n",
      "i: 2800 , cost :  3.46201 , training accuracy :  0.783389\n",
      "i: 2900 , cost :  2.82262 , training accuracy :  0.81257\n",
      "i: 3000 , cost :  1.20218 , training accuracy :  0.852974\n",
      "i: 3100 , cost :  16.3455 , training accuracy :  0.673401\n",
      "i: 3200 , cost :  1.56921 , training accuracy :  0.855219\n",
      "i: 3300 , cost :  0.966311 , training accuracy :  0.832772\n",
      "i: 3400 , cost :  1.75254 , training accuracy :  0.814815\n",
      "i: 3500 , cost :  2.27447 , training accuracy :  0.791246\n",
      "i: 3600 , cost :  1.52376 , training accuracy :  0.851852\n",
      "i: 3700 , cost :  6.22025 , training accuracy :  0.75982\n",
      "i: 3800 , cost :  0.997789 , training accuracy :  0.86532\n",
      "i: 3900 , cost :  9.8104 , training accuracy :  0.536476\n",
      "i: 4000 , cost :  1.1897 , training accuracy :  0.860831\n",
      "i: 4100 , cost :  2.68046 , training accuracy :  0.719416\n",
      "i: 4200 , cost :  0.910453 , training accuracy :  0.842873\n",
      "i: 4300 , cost :  2.37499 , training accuracy :  0.636364\n",
      "i: 4400 , cost :  1.4358 , training accuracy :  0.838384\n",
      "i: 4500 , cost :  16.0195 , training accuracy :  0.401796\n",
      "i: 4600 , cost :  1.81111 , training accuracy :  0.820426\n",
      "i: 4700 , cost :  2.82659 , training accuracy :  0.74523\n",
      "i: 4800 , cost :  2.50797 , training accuracy :  0.774411\n",
      "i: 4900 , cost :  0.860381 , training accuracy :  0.85073\n",
      "Final training accuracy :  0.499439\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 5000 ][hidden: 50 ][file: bhavul_tr_acc_0.50_prediction.csv ] ACCURACY :  0.499439\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1316.45 , training accuracy :  0.616162\n",
      "i: 100 , cost :  14.5714 , training accuracy :  0.725028\n",
      "i: 200 , cost :  4.58395 , training accuracy :  0.810326\n",
      "i: 300 , cost :  3.93758 , training accuracy :  0.814815\n",
      "i: 400 , cost :  6.78374 , training accuracy :  0.776655\n",
      "i: 500 , cost :  4.99581 , training accuracy :  0.811448\n",
      "i: 600 , cost :  13.7156 , training accuracy :  0.754209\n",
      "i: 700 , cost :  7.05103 , training accuracy :  0.780022\n",
      "i: 800 , cost :  4.82961 , training accuracy :  0.784512\n",
      "i: 900 , cost :  11.1049 , training accuracy :  0.540965\n",
      "i: 1000 , cost :  2.99313 , training accuracy :  0.85073\n",
      "i: 1100 , cost :  9.69698 , training accuracy :  0.748597\n",
      "i: 1200 , cost :  4.12137 , training accuracy :  0.824916\n",
      "i: 1300 , cost :  20.9293 , training accuracy :  0.690236\n",
      "i: 1400 , cost :  11.7252 , training accuracy :  0.719416\n",
      "i: 1500 , cost :  2.44492 , training accuracy :  0.852974\n",
      "i: 1600 , cost :  23.5296 , training accuracy :  0.483726\n",
      "i: 1700 , cost :  2.29044 , training accuracy :  0.858586\n",
      "i: 1800 , cost :  6.54868 , training accuracy :  0.723906\n",
      "i: 1900 , cost :  2.12639 , training accuracy :  0.84624\n",
      "i: 2000 , cost :  3.7344 , training accuracy :  0.826038\n",
      "i: 2100 , cost :  3.66825 , training accuracy :  0.81257\n",
      "i: 2200 , cost :  1.84686 , training accuracy :  0.858586\n",
      "i: 2300 , cost :  6.41068 , training accuracy :  0.720539\n",
      "i: 2400 , cost :  3.64669 , training accuracy :  0.819304\n",
      "i: 2500 , cost :  2.87447 , training accuracy :  0.753086\n",
      "i: 2600 , cost :  2.22384 , training accuracy :  0.851852\n",
      "i: 2700 , cost :  15.0342 , training accuracy :  0.731762\n",
      "i: 2800 , cost :  3.11627 , training accuracy :  0.829405\n",
      "i: 2900 , cost :  2.88355 , training accuracy :  0.818182\n",
      "i: 3000 , cost :  2.01906 , training accuracy :  0.864198\n",
      "i: 3100 , cost :  5.46461 , training accuracy :  0.818182\n",
      "i: 3200 , cost :  1.73188 , training accuracy :  0.868687\n",
      "i: 3300 , cost :  3.16498 , training accuracy :  0.83165\n",
      "i: 3400 , cost :  1.44387 , training accuracy :  0.873176\n",
      "i: 3500 , cost :  1.6487 , training accuracy :  0.867565\n",
      "i: 3600 , cost :  13.2459 , training accuracy :  0.760943\n",
      "i: 3700 , cost :  1.39665 , training accuracy :  0.868687\n",
      "i: 3800 , cost :  3.44874 , training accuracy :  0.763187\n",
      "i: 3900 , cost :  2.57825 , training accuracy :  0.859708\n",
      "i: 4000 , cost :  1.8438 , training accuracy :  0.854097\n",
      "i: 4100 , cost :  3.98337 , training accuracy :  0.822671\n",
      "i: 4200 , cost :  1.84607 , training accuracy :  0.872054\n",
      "i: 4300 , cost :  9.70783 , training accuracy :  0.781145\n",
      "i: 4400 , cost :  1.42761 , training accuracy :  0.875421\n",
      "i: 4500 , cost :  4.24502 , training accuracy :  0.734007\n",
      "i: 4600 , cost :  1.91727 , training accuracy :  0.873176\n",
      "i: 4700 , cost :  11.977 , training accuracy :  0.735129\n",
      "i: 4800 , cost :  1.49773 , training accuracy :  0.876543\n",
      "i: 4900 , cost :  8.49624 , training accuracy :  0.783389\n",
      "Final training accuracy :  0.881033\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 5000 ][hidden: 100 ][file: bhavul_tr_acc_0.88_prediction.csv ] ACCURACY :  0.881033\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  12.1342 , training accuracy :  0.451178\n",
      "i: 100 , cost :  0.68438 , training accuracy :  0.612795\n",
      "i: 200 , cost :  0.669294 , training accuracy :  0.615039\n",
      "i: 300 , cost :  0.666901 , training accuracy :  0.616162\n",
      "i: 400 , cost :  0.666202 , training accuracy :  0.616162\n",
      "i: 500 , cost :  0.665758 , training accuracy :  0.617284\n",
      "i: 600 , cost :  0.665558 , training accuracy :  0.616162\n",
      "i: 700 , cost :  0.665497 , training accuracy :  0.616162\n",
      "i: 800 , cost :  0.665476 , training accuracy :  0.616162\n",
      "i: 900 , cost :  0.665456 , training accuracy :  0.616162\n",
      "i: 1000 , cost :  0.665437 , training accuracy :  0.616162\n",
      "i: 1100 , cost :  0.665415 , training accuracy :  0.616162\n",
      "i: 1200 , cost :  0.665393 , training accuracy :  0.616162\n",
      "i: 1300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 1400 , cost :  0.665339 , training accuracy :  0.616162\n",
      "i: 1500 , cost :  0.665305 , training accuracy :  0.616162\n",
      "i: 1600 , cost :  0.665267 , training accuracy :  0.616162\n",
      "i: 1700 , cost :  0.665224 , training accuracy :  0.617284\n",
      "i: 1800 , cost :  0.665172 , training accuracy :  0.617284\n",
      "i: 1900 , cost :  0.665112 , training accuracy :  0.617284\n",
      "i: 2000 , cost :  0.66504 , training accuracy :  0.618406\n",
      "i: 2100 , cost :  0.664966 , training accuracy :  0.618406\n",
      "i: 2200 , cost :  0.664908 , training accuracy :  0.618406\n",
      "i: 2300 , cost :  0.664849 , training accuracy :  0.618406\n",
      "i: 2400 , cost :  0.664778 , training accuracy :  0.618406\n",
      "i: 2500 , cost :  0.664724 , training accuracy :  0.618406\n",
      "i: 2600 , cost :  0.664659 , training accuracy :  0.618406\n",
      "i: 2700 , cost :  0.664582 , training accuracy :  0.618406\n",
      "i: 2800 , cost :  0.664497 , training accuracy :  0.618406\n",
      "i: 2900 , cost :  0.664405 , training accuracy :  0.618406\n",
      "i: 3000 , cost :  0.664306 , training accuracy :  0.618406\n",
      "i: 3100 , cost :  0.664198 , training accuracy :  0.618406\n",
      "i: 3200 , cost :  0.664069 , training accuracy :  0.618406\n",
      "i: 3300 , cost :  0.663912 , training accuracy :  0.618406\n",
      "i: 3400 , cost :  0.663736 , training accuracy :  0.619529\n",
      "i: 3500 , cost :  0.663575 , training accuracy :  0.619529\n",
      "i: 3600 , cost :  0.663465 , training accuracy :  0.619529\n",
      "i: 3700 , cost :  0.663355 , training accuracy :  0.619529\n",
      "i: 3800 , cost :  0.663245 , training accuracy :  0.619529\n",
      "i: 3900 , cost :  0.663144 , training accuracy :  0.619529\n",
      "i: 4000 , cost :  0.66306 , training accuracy :  0.619529\n",
      "i: 4100 , cost :  0.662992 , training accuracy :  0.619529\n",
      "i: 4200 , cost :  0.662944 , training accuracy :  0.619529\n",
      "i: 4300 , cost :  0.662896 , training accuracy :  0.619529\n",
      "i: 4400 , cost :  0.662868 , training accuracy :  0.619529\n",
      "i: 4500 , cost :  0.662837 , training accuracy :  0.619529\n",
      "i: 4600 , cost :  0.66281 , training accuracy :  0.619529\n",
      "i: 4700 , cost :  0.662798 , training accuracy :  0.619529\n",
      "i: 4800 , cost :  0.662778 , training accuracy :  0.619529\n",
      "i: 4900 , cost :  0.662766 , training accuracy :  0.619529\n",
      "i: 5000 , cost :  0.662751 , training accuracy :  0.619529\n",
      "i: 5100 , cost :  0.662746 , training accuracy :  0.619529\n",
      "i: 5200 , cost :  0.662738 , training accuracy :  0.619529\n",
      "i: 5300 , cost :  0.662729 , training accuracy :  0.619529\n",
      "i: 5400 , cost :  0.662726 , training accuracy :  0.619529\n",
      "i: 5500 , cost :  0.662718 , training accuracy :  0.619529\n",
      "i: 5600 , cost :  0.662713 , training accuracy :  0.619529\n",
      "i: 5700 , cost :  0.662712 , training accuracy :  0.619529\n",
      "i: 5800 , cost :  0.662715 , training accuracy :  0.619529\n",
      "i: 5900 , cost :  0.662723 , training accuracy :  0.619529\n",
      "i: 6000 , cost :  0.662701 , training accuracy :  0.619529\n",
      "i: 6100 , cost :  0.662699 , training accuracy :  0.619529\n",
      "i: 6200 , cost :  0.662701 , training accuracy :  0.619529\n",
      "i: 6300 , cost :  0.6627 , training accuracy :  0.619529\n",
      "i: 6400 , cost :  0.662696 , training accuracy :  0.619529\n",
      "i: 6500 , cost :  0.662696 , training accuracy :  0.619529\n",
      "i: 6600 , cost :  0.662693 , training accuracy :  0.619529\n",
      "i: 6700 , cost :  0.662692 , training accuracy :  0.619529\n",
      "i: 6800 , cost :  0.662691 , training accuracy :  0.619529\n",
      "i: 6900 , cost :  0.662696 , training accuracy :  0.619529\n",
      "i: 7000 , cost :  0.662688 , training accuracy :  0.619529\n",
      "i: 7100 , cost :  0.662692 , training accuracy :  0.619529\n",
      "i: 7200 , cost :  0.662689 , training accuracy :  0.619529\n",
      "i: 7300 , cost :  0.662693 , training accuracy :  0.619529\n",
      "i: 7400 , cost :  0.662686 , training accuracy :  0.619529\n",
      "i: 7500 , cost :  0.662687 , training accuracy :  0.619529\n",
      "i: 7600 , cost :  0.662694 , training accuracy :  0.619529\n",
      "i: 7700 , cost :  0.662694 , training accuracy :  0.619529\n",
      "i: 7800 , cost :  0.662694 , training accuracy :  0.619529\n",
      "i: 7900 , cost :  0.662683 , training accuracy :  0.619529\n",
      "i: 8000 , cost :  0.662684 , training accuracy :  0.619529\n",
      "i: 8100 , cost :  0.662696 , training accuracy :  0.619529\n",
      "i: 8200 , cost :  0.662682 , training accuracy :  0.619529\n",
      "i: 8300 , cost :  0.662694 , training accuracy :  0.619529\n",
      "i: 8400 , cost :  0.662682 , training accuracy :  0.619529\n",
      "i: 8500 , cost :  0.662682 , training accuracy :  0.619529\n",
      "i: 8600 , cost :  0.662693 , training accuracy :  0.619529\n",
      "i: 8700 , cost :  0.662701 , training accuracy :  0.619529\n",
      "i: 8800 , cost :  0.662684 , training accuracy :  0.619529\n",
      "i: 8900 , cost :  0.662685 , training accuracy :  0.619529\n",
      "i: 9000 , cost :  0.662685 , training accuracy :  0.619529\n",
      "i: 9100 , cost :  0.662692 , training accuracy :  0.619529\n",
      "i: 9200 , cost :  0.662683 , training accuracy :  0.619529\n",
      "i: 9300 , cost :  0.662686 , training accuracy :  0.619529\n",
      "i: 9400 , cost :  0.662683 , training accuracy :  0.619529\n",
      "i: 9500 , cost :  0.662718 , training accuracy :  0.619529\n",
      "i: 9600 , cost :  0.662683 , training accuracy :  0.619529\n",
      "i: 9700 , cost :  0.662679 , training accuracy :  0.619529\n",
      "i: 9800 , cost :  0.662679 , training accuracy :  0.619529\n",
      "i: 9900 , cost :  0.662682 , training accuracy :  0.619529\n",
      "Final training accuracy :  0.619529\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 10000 ][hidden: 3 ][file: bhavul_tr_acc_0.62_prediction.csv ] ACCURACY :  0.619529\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  429.307 , training accuracy :  0.616162\n",
      "i: 100 , cost :  2.00957 , training accuracy :  0.708193\n",
      "i: 200 , cost :  0.819699 , training accuracy :  0.765432\n",
      "i: 300 , cost :  4.62228 , training accuracy :  0.636364\n",
      "i: 400 , cost :  1.82892 , training accuracy :  0.703704\n",
      "i: 500 , cost :  0.64599 , training accuracy :  0.792368\n",
      "i: 600 , cost :  0.514427 , training accuracy :  0.790123\n",
      "i: 700 , cost :  0.718327 , training accuracy :  0.755331\n",
      "i: 800 , cost :  0.499135 , training accuracy :  0.805836\n",
      "i: 900 , cost :  0.679886 , training accuracy :  0.783389\n",
      "i: 1000 , cost :  0.618108 , training accuracy :  0.810326\n",
      "i: 1100 , cost :  0.610886 , training accuracy :  0.810326\n",
      "i: 1200 , cost :  0.468382 , training accuracy :  0.813693\n",
      "i: 1300 , cost :  0.516661 , training accuracy :  0.774411\n",
      "i: 1400 , cost :  0.621227 , training accuracy :  0.813693\n",
      "i: 1500 , cost :  0.442098 , training accuracy :  0.817059\n",
      "i: 1600 , cost :  1.45234 , training accuracy :  0.578002\n",
      "i: 1700 , cost :  0.53406 , training accuracy :  0.792368\n",
      "i: 1800 , cost :  0.4397 , training accuracy :  0.819304\n",
      "i: 1900 , cost :  2.21784 , training accuracy :  0.450056\n",
      "i: 2000 , cost :  0.473933 , training accuracy :  0.811448\n",
      "i: 2100 , cost :  0.51478 , training accuracy :  0.776655\n",
      "i: 2200 , cost :  0.43623 , training accuracy :  0.815937\n",
      "i: 2300 , cost :  2.19692 , training accuracy :  0.676768\n",
      "i: 2400 , cost :  1.27538 , training accuracy :  0.729517\n",
      "i: 2500 , cost :  0.474183 , training accuracy :  0.806958\n",
      "i: 2600 , cost :  0.432914 , training accuracy :  0.822671\n",
      "i: 2700 , cost :  0.557058 , training accuracy :  0.774411\n",
      "i: 2800 , cost :  0.471251 , training accuracy :  0.785634\n",
      "i: 2900 , cost :  1.16087 , training accuracy :  0.736251\n",
      "i: 3000 , cost :  0.456917 , training accuracy :  0.821549\n",
      "i: 3100 , cost :  0.42969 , training accuracy :  0.81257\n",
      "i: 3200 , cost :  0.687719 , training accuracy :  0.786756\n",
      "i: 3300 , cost :  0.499058 , training accuracy :  0.810326\n",
      "i: 3400 , cost :  0.439128 , training accuracy :  0.813693\n",
      "i: 3500 , cost :  0.472459 , training accuracy :  0.813693\n",
      "i: 3600 , cost :  1.269 , training accuracy :  0.608305\n",
      "i: 3700 , cost :  0.802141 , training accuracy :  0.774411\n",
      "i: 3800 , cost :  0.450834 , training accuracy :  0.822671\n",
      "i: 3900 , cost :  0.427901 , training accuracy :  0.824916\n",
      "i: 4000 , cost :  0.88249 , training accuracy :  0.749719\n",
      "i: 4100 , cost :  0.61025 , training accuracy :  0.757576\n",
      "i: 4200 , cost :  0.436631 , training accuracy :  0.815937\n",
      "i: 4300 , cost :  0.465542 , training accuracy :  0.785634\n",
      "i: 4400 , cost :  0.687132 , training accuracy :  0.746352\n",
      "i: 4500 , cost :  0.656292 , training accuracy :  0.791246\n",
      "i: 4600 , cost :  0.434003 , training accuracy :  0.819304\n",
      "i: 4700 , cost :  0.483426 , training accuracy :  0.7789\n",
      "i: 4800 , cost :  0.427571 , training accuracy :  0.826038\n",
      "i: 4900 , cost :  0.438888 , training accuracy :  0.824916\n",
      "i: 5000 , cost :  0.551582 , training accuracy :  0.75982\n",
      "i: 5100 , cost :  0.867795 , training accuracy :  0.713805\n",
      "i: 5200 , cost :  0.660374 , training accuracy :  0.796857\n",
      "i: 5300 , cost :  0.437543 , training accuracy :  0.811448\n",
      "i: 5400 , cost :  0.425917 , training accuracy :  0.815937\n",
      "i: 5500 , cost :  0.755465 , training accuracy :  0.781145\n",
      "i: 5600 , cost :  0.450726 , training accuracy :  0.810326\n",
      "i: 5700 , cost :  0.426986 , training accuracy :  0.828283\n",
      "i: 5800 , cost :  0.84564 , training accuracy :  0.618406\n",
      "i: 5900 , cost :  0.46612 , training accuracy :  0.826038\n",
      "i: 6000 , cost :  0.4276 , training accuracy :  0.823793\n",
      "i: 6100 , cost :  0.427171 , training accuracy :  0.814815\n",
      "i: 6200 , cost :  0.636338 , training accuracy :  0.787879\n",
      "i: 6300 , cost :  0.679112 , training accuracy :  0.774411\n",
      "i: 6400 , cost :  0.436142 , training accuracy :  0.818182\n",
      "i: 6500 , cost :  0.424685 , training accuracy :  0.821549\n",
      "i: 6600 , cost :  0.430705 , training accuracy :  0.805836\n",
      "i: 6700 , cost :  0.792001 , training accuracy :  0.74523\n",
      "i: 6800 , cost :  1.32779 , training accuracy :  0.723906\n",
      "i: 6900 , cost :  0.448069 , training accuracy :  0.814815\n",
      "i: 7000 , cost :  0.428116 , training accuracy :  0.820426\n",
      "i: 7100 , cost :  0.423374 , training accuracy :  0.818182\n",
      "i: 7200 , cost :  0.458402 , training accuracy :  0.813693\n",
      "i: 7300 , cost :  0.423417 , training accuracy :  0.821549\n",
      "i: 7400 , cost :  0.558199 , training accuracy :  0.735129\n",
      "i: 7500 , cost :  0.636045 , training accuracy :  0.789001\n",
      "i: 7600 , cost :  0.848158 , training accuracy :  0.747475\n",
      "i: 7700 , cost :  0.438292 , training accuracy :  0.826038\n",
      "i: 7800 , cost :  0.422415 , training accuracy :  0.821549\n",
      "i: 7900 , cost :  0.497841 , training accuracy :  0.802469\n",
      "i: 8000 , cost :  0.444843 , training accuracy :  0.820426\n",
      "i: 8100 , cost :  0.564413 , training accuracy :  0.781145\n",
      "i: 8200 , cost :  0.407722 , training accuracy :  0.824916\n",
      "i: 8300 , cost :  0.447084 , training accuracy :  0.806958\n",
      "i: 8400 , cost :  0.441345 , training accuracy :  0.835017\n",
      "i: 8500 , cost :  0.401833 , training accuracy :  0.841751\n",
      "i: 8600 , cost :  0.396971 , training accuracy :  0.841751\n",
      "i: 8700 , cost :  0.402258 , training accuracy :  0.824916\n",
      "i: 8800 , cost :  0.398112 , training accuracy :  0.838384\n",
      "i: 8900 , cost :  0.403185 , training accuracy :  0.829405\n",
      "i: 9000 , cost :  0.415892 , training accuracy :  0.829405\n",
      "i: 9100 , cost :  0.426029 , training accuracy :  0.832772\n",
      "i: 9200 , cost :  0.410721 , training accuracy :  0.82716\n",
      "i: 9300 , cost :  0.469131 , training accuracy :  0.808081\n",
      "i: 9400 , cost :  0.567721 , training accuracy :  0.783389\n",
      "i: 9500 , cost :  0.5707 , training accuracy :  0.787879\n",
      "i: 9600 , cost :  0.426786 , training accuracy :  0.823793\n",
      "i: 9700 , cost :  0.394948 , training accuracy :  0.84624\n",
      "i: 9800 , cost :  0.392197 , training accuracy :  0.840629\n",
      "i: 9900 , cost :  0.394103 , training accuracy :  0.838384\n",
      "Final training accuracy :  0.837261\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 10000 ][hidden: 10 ][file: bhavul_tr_acc_0.84_prediction.csv ] ACCURACY :  0.837261\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  51.9588 , training accuracy :  0.391695\n",
      "i: 100 , cost :  4.877 , training accuracy :  0.661055\n",
      "i: 200 , cost :  1.73689 , training accuracy :  0.601571\n",
      "i: 300 , cost :  0.797781 , training accuracy :  0.784512\n",
      "i: 400 , cost :  1.74052 , training accuracy :  0.746352\n",
      "i: 500 , cost :  0.735472 , training accuracy :  0.803591\n",
      "i: 600 , cost :  0.700374 , training accuracy :  0.776655\n",
      "i: 700 , cost :  0.704266 , training accuracy :  0.801347\n",
      "i: 800 , cost :  0.582947 , training accuracy :  0.806958\n",
      "i: 900 , cost :  0.924104 , training accuracy :  0.79349\n",
      "i: 1000 , cost :  0.58637 , training accuracy :  0.805836\n",
      "i: 1100 , cost :  2.24577 , training accuracy :  0.470258\n",
      "i: 1200 , cost :  0.601742 , training accuracy :  0.815937\n",
      "i: 1300 , cost :  0.517166 , training accuracy :  0.804714\n",
      "i: 1400 , cost :  0.589343 , training accuracy :  0.814815\n",
      "i: 1500 , cost :  0.545759 , training accuracy :  0.803591\n",
      "i: 1600 , cost :  0.507441 , training accuracy :  0.814815\n",
      "i: 1700 , cost :  1.36654 , training accuracy :  0.722783\n",
      "i: 1800 , cost :  0.499819 , training accuracy :  0.824916\n",
      "i: 1900 , cost :  2.52091 , training accuracy :  0.672278\n",
      "i: 2000 , cost :  0.611001 , training accuracy :  0.800224\n",
      "i: 2100 , cost :  0.452294 , training accuracy :  0.830527\n",
      "i: 2200 , cost :  0.655772 , training accuracy :  0.791246\n",
      "i: 2300 , cost :  0.486771 , training accuracy :  0.833894\n",
      "i: 2400 , cost :  0.431579 , training accuracy :  0.82716\n",
      "i: 2500 , cost :  0.746065 , training accuracy :  0.795735\n",
      "i: 2600 , cost :  0.427404 , training accuracy :  0.823793\n",
      "i: 2700 , cost :  1.22322 , training accuracy :  0.631874\n",
      "i: 2800 , cost :  0.429828 , training accuracy :  0.823793\n",
      "i: 2900 , cost :  1.57139 , training accuracy :  0.731762\n",
      "i: 3000 , cost :  0.426665 , training accuracy :  0.830527\n",
      "i: 3100 , cost :  1.10662 , training accuracy :  0.754209\n",
      "i: 3200 , cost :  0.45645 , training accuracy :  0.818182\n",
      "i: 3300 , cost :  1.89871 , training accuracy :  0.73064\n",
      "i: 3400 , cost :  0.97405 , training accuracy :  0.717172\n",
      "i: 3500 , cost :  0.416616 , training accuracy :  0.840629\n",
      "i: 3600 , cost :  0.830942 , training accuracy :  0.771044\n",
      "i: 3700 , cost :  0.482303 , training accuracy :  0.809203\n",
      "i: 3800 , cost :  0.403919 , training accuracy :  0.840629\n",
      "i: 3900 , cost :  1.12097 , training accuracy :  0.754209\n",
      "i: 4000 , cost :  0.408758 , training accuracy :  0.840629\n",
      "i: 4100 , cost :  0.419626 , training accuracy :  0.828283\n",
      "i: 4200 , cost :  1.77712 , training accuracy :  0.722783\n",
      "i: 4300 , cost :  0.411795 , training accuracy :  0.828283\n",
      "i: 4400 , cost :  1.19694 , training accuracy :  0.769921\n",
      "i: 4500 , cost :  0.473086 , training accuracy :  0.828283\n",
      "i: 4600 , cost :  0.397119 , training accuracy :  0.833894\n",
      "i: 4700 , cost :  0.537264 , training accuracy :  0.800224\n",
      "i: 4800 , cost :  0.435103 , training accuracy :  0.82716\n",
      "i: 4900 , cost :  0.390713 , training accuracy :  0.840629\n",
      "i: 5000 , cost :  0.402184 , training accuracy :  0.83165\n",
      "i: 5100 , cost :  1.0012 , training accuracy :  0.748597\n",
      "i: 5200 , cost :  0.451152 , training accuracy :  0.835017\n",
      "i: 5300 , cost :  0.407552 , training accuracy :  0.840629\n",
      "i: 5400 , cost :  1.0947 , training accuracy :  0.760943\n",
      "i: 5500 , cost :  0.482244 , training accuracy :  0.820426\n",
      "i: 5600 , cost :  0.391402 , training accuracy :  0.838384\n",
      "i: 5700 , cost :  0.418042 , training accuracy :  0.824916\n",
      "i: 5800 , cost :  1.11292 , training accuracy :  0.781145\n",
      "i: 5900 , cost :  0.395062 , training accuracy :  0.840629\n",
      "i: 6000 , cost :  1.96702 , training accuracy :  0.705948\n",
      "i: 6100 , cost :  0.415898 , training accuracy :  0.833894\n",
      "i: 6200 , cost :  0.382968 , training accuracy :  0.836139\n",
      "i: 6300 , cost :  3.0736 , training accuracy :  0.690236\n",
      "i: 6400 , cost :  0.444653 , training accuracy :  0.841751\n",
      "i: 6500 , cost :  0.424371 , training accuracy :  0.840629\n",
      "i: 6600 , cost :  0.483198 , training accuracy :  0.79349\n",
      "i: 6700 , cost :  0.484488 , training accuracy :  0.837261\n",
      "i: 6800 , cost :  0.386505 , training accuracy :  0.840629\n",
      "i: 6900 , cost :  1.05206 , training accuracy :  0.648709\n",
      "i: 7000 , cost :  0.489188 , training accuracy :  0.783389\n",
      "i: 7100 , cost :  0.914609 , training accuracy :  0.720539\n",
      "i: 7200 , cost :  0.389765 , training accuracy :  0.840629\n",
      "i: 7300 , cost :  0.390858 , training accuracy :  0.843996\n",
      "i: 7400 , cost :  0.428044 , training accuracy :  0.839506\n",
      "i: 7500 , cost :  0.475934 , training accuracy :  0.79798\n",
      "i: 7600 , cost :  0.635291 , training accuracy :  0.774411\n",
      "i: 7700 , cost :  1.09657 , training accuracy :  0.674523\n",
      "i: 7800 , cost :  0.452609 , training accuracy :  0.833894\n",
      "i: 7900 , cost :  0.469886 , training accuracy :  0.818182\n",
      "i: 8000 , cost :  0.423059 , training accuracy :  0.842873\n",
      "i: 8100 , cost :  0.377651 , training accuracy :  0.838384\n",
      "i: 8200 , cost :  0.475176 , training accuracy :  0.820426\n",
      "i: 8300 , cost :  0.387974 , training accuracy :  0.833894\n",
      "i: 8400 , cost :  0.405245 , training accuracy :  0.828283\n",
      "i: 8500 , cost :  0.747714 , training accuracy :  0.74523\n",
      "i: 8600 , cost :  0.415252 , training accuracy :  0.847363\n",
      "i: 8700 , cost :  0.366514 , training accuracy :  0.840629\n",
      "i: 8800 , cost :  0.366527 , training accuracy :  0.847363\n",
      "i: 8900 , cost :  0.45736 , training accuracy :  0.84624\n",
      "i: 9000 , cost :  0.435507 , training accuracy :  0.842873\n",
      "i: 9100 , cost :  0.39942 , training accuracy :  0.841751\n",
      "i: 9200 , cost :  0.390812 , training accuracy :  0.847363\n",
      "i: 9300 , cost :  0.357699 , training accuracy :  0.852974\n",
      "i: 9400 , cost :  0.431391 , training accuracy :  0.817059\n",
      "i: 9500 , cost :  0.390228 , training accuracy :  0.838384\n",
      "i: 9600 , cost :  0.420733 , training accuracy :  0.836139\n",
      "i: 9700 , cost :  0.87207 , training accuracy :  0.790123\n",
      "i: 9800 , cost :  0.398054 , training accuracy :  0.841751\n",
      "i: 9900 , cost :  0.3548 , training accuracy :  0.857464\n",
      "Final training accuracy :  0.843996\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 10000 ][hidden: 15 ][file: bhavul_tr_acc_0.84_prediction.csv ] ACCURACY :  0.843996\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  332.269 , training accuracy :  0.40404\n",
      "i: 100 , cost :  4.46068 , training accuracy :  0.702581\n",
      "i: 200 , cost :  3.16177 , training accuracy :  0.753086\n",
      "i: 300 , cost :  6.61038 , training accuracy :  0.686869\n",
      "i: 400 , cost :  1.24912 , training accuracy :  0.814815\n",
      "i: 500 , cost :  1.45912 , training accuracy :  0.802469\n",
      "i: 600 , cost :  2.42547 , training accuracy :  0.777778\n",
      "i: 700 , cost :  1.01774 , training accuracy :  0.832772\n",
      "i: 800 , cost :  2.58368 , training accuracy :  0.680135\n",
      "i: 900 , cost :  6.17962 , training accuracy :  0.527497\n",
      "i: 1000 , cost :  1.26979 , training accuracy :  0.822671\n",
      "i: 1100 , cost :  1.53277 , training accuracy :  0.823793\n",
      "i: 1200 , cost :  1.22218 , training accuracy :  0.748597\n",
      "i: 1300 , cost :  1.58962 , training accuracy :  0.803591\n",
      "i: 1400 , cost :  0.759486 , training accuracy :  0.849607\n",
      "i: 1500 , cost :  0.786238 , training accuracy :  0.821549\n",
      "i: 1600 , cost :  5.04166 , training accuracy :  0.710438\n",
      "i: 1700 , cost :  0.747752 , training accuracy :  0.85073\n",
      "i: 1800 , cost :  3.83724 , training accuracy :  0.551066\n",
      "i: 1900 , cost :  0.740019 , training accuracy :  0.852974\n",
      "i: 2000 , cost :  8.91396 , training accuracy :  0.748597\n",
      "i: 2100 , cost :  0.947784 , training accuracy :  0.861953\n",
      "i: 2200 , cost :  0.804679 , training accuracy :  0.808081\n",
      "i: 2300 , cost :  2.17653 , training accuracy :  0.805836\n",
      "i: 2400 , cost :  0.677541 , training accuracy :  0.857464\n",
      "i: 2500 , cost :  3.84625 , training accuracy :  0.762065\n",
      "i: 2600 , cost :  2.28608 , training accuracy :  0.773288\n",
      "i: 2700 , cost :  0.720007 , training accuracy :  0.864198\n",
      "i: 2800 , cost :  2.60231 , training accuracy :  0.799102\n",
      "i: 2900 , cost :  2.23812 , training accuracy :  0.786756\n",
      "i: 3000 , cost :  3.2455 , training accuracy :  0.792368\n",
      "i: 3100 , cost :  1.41904 , training accuracy :  0.819304\n",
      "i: 3200 , cost :  0.919844 , training accuracy :  0.766554\n",
      "i: 3300 , cost :  0.90751 , training accuracy :  0.861953\n",
      "i: 3400 , cost :  1.25977 , training accuracy :  0.765432\n",
      "i: 3500 , cost :  6.91404 , training accuracy :  0.447811\n",
      "i: 3600 , cost :  1.57938 , training accuracy :  0.795735\n",
      "i: 3700 , cost :  0.699583 , training accuracy :  0.877666\n",
      "i: 3800 , cost :  1.89874 , training accuracy :  0.791246\n",
      "i: 3900 , cost :  0.922989 , training accuracy :  0.821549\n",
      "i: 4000 , cost :  6.61366 , training accuracy :  0.686869\n",
      "i: 4100 , cost :  1.28276 , training accuracy :  0.772166\n",
      "i: 4200 , cost :  5.57512 , training accuracy :  0.446689\n",
      "i: 4300 , cost :  0.666942 , training accuracy :  0.85073\n",
      "i: 4400 , cost :  1.7578 , training accuracy :  0.786756\n",
      "i: 4500 , cost :  0.749086 , training accuracy :  0.875421\n",
      "i: 4600 , cost :  0.997347 , training accuracy :  0.823793\n",
      "i: 4700 , cost :  4.83352 , training accuracy :  0.478114\n",
      "i: 4800 , cost :  0.45596 , training accuracy :  0.876543\n",
      "i: 4900 , cost :  1.25465 , training accuracy :  0.775533\n",
      "i: 5000 , cost :  0.518355 , training accuracy :  0.861953\n",
      "i: 5100 , cost :  1.82592 , training accuracy :  0.809203\n",
      "i: 5200 , cost :  0.490305 , training accuracy :  0.877666\n",
      "i: 5300 , cost :  1.39403 , training accuracy :  0.744108\n",
      "i: 5400 , cost :  0.625376 , training accuracy :  0.852974\n",
      "i: 5500 , cost :  4.27182 , training accuracy :  0.548822\n",
      "i: 5600 , cost :  0.64578 , training accuracy :  0.799102\n",
      "i: 5700 , cost :  0.744271 , training accuracy :  0.823793\n",
      "i: 5800 , cost :  0.615026 , training accuracy :  0.863075\n",
      "i: 5900 , cost :  0.593988 , training accuracy :  0.873176\n",
      "i: 6000 , cost :  0.401878 , training accuracy :  0.873176\n",
      "i: 6100 , cost :  1.10084 , training accuracy :  0.754209\n",
      "i: 6200 , cost :  0.670828 , training accuracy :  0.85073\n",
      "i: 6300 , cost :  1.10247 , training accuracy :  0.769921\n",
      "i: 6400 , cost :  0.633251 , training accuracy :  0.872054\n",
      "i: 6500 , cost :  3.37462 , training accuracy :  0.748597\n",
      "i: 6600 , cost :  0.518288 , training accuracy :  0.86532\n",
      "i: 6700 , cost :  0.810402 , training accuracy :  0.849607\n",
      "i: 6800 , cost :  0.513025 , training accuracy :  0.874299\n",
      "i: 6900 , cost :  2.06805 , training accuracy :  0.802469\n",
      "i: 7000 , cost :  2.79136 , training accuracy :  0.794613\n",
      "i: 7100 , cost :  1.2066 , training accuracy :  0.804714\n",
      "i: 7200 , cost :  0.41068 , training accuracy :  0.873176\n",
      "i: 7300 , cost :  1.62634 , training accuracy :  0.815937\n",
      "i: 7400 , cost :  0.377877 , training accuracy :  0.874299\n",
      "i: 7500 , cost :  7.2719 , training accuracy :  0.707071\n",
      "i: 7600 , cost :  0.526644 , training accuracy :  0.877666\n",
      "i: 7700 , cost :  0.340074 , training accuracy :  0.883277\n",
      "i: 7800 , cost :  1.71395 , training accuracy :  0.727273\n",
      "i: 7900 , cost :  0.345171 , training accuracy :  0.877666\n",
      "i: 8000 , cost :  0.823329 , training accuracy :  0.852974\n",
      "i: 8100 , cost :  0.347131 , training accuracy :  0.878788\n",
      "i: 8200 , cost :  0.616036 , training accuracy :  0.868687\n",
      "i: 8300 , cost :  0.39996 , training accuracy :  0.874299\n",
      "i: 8400 , cost :  0.723265 , training accuracy :  0.855219\n",
      "i: 8500 , cost :  0.585429 , training accuracy :  0.86532\n",
      "i: 8600 , cost :  0.331099 , training accuracy :  0.87991\n",
      "i: 8700 , cost :  0.515366 , training accuracy :  0.861953\n",
      "i: 8800 , cost :  1.47363 , training accuracy :  0.707071\n",
      "i: 8900 , cost :  0.493889 , training accuracy :  0.872054\n",
      "i: 9000 , cost :  1.0114 , training accuracy :  0.603816\n",
      "i: 9100 , cost :  0.427037 , training accuracy :  0.883277\n",
      "i: 9200 , cost :  2.69304 , training accuracy :  0.769921\n",
      "i: 9300 , cost :  0.639075 , training accuracy :  0.858586\n",
      "i: 9400 , cost :  0.317841 , training accuracy :  0.8844\n",
      "i: 9500 , cost :  2.26134 , training accuracy :  0.579125\n",
      "i: 9600 , cost :  0.370585 , training accuracy :  0.887767\n",
      "i: 9700 , cost :  1.18728 , training accuracy :  0.836139\n",
      "i: 9800 , cost :  2.61152 , training accuracy :  0.806958\n",
      "i: 9900 , cost :  0.402663 , training accuracy :  0.881033\n",
      "Final training accuracy :  0.87991\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 10000 ][hidden: 50 ][file: bhavul_tr_acc_0.88_prediction.csv ] ACCURACY :  0.87991\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1287.82 , training accuracy :  0.618406\n",
      "i: 100 , cost :  19.5555 , training accuracy :  0.703704\n",
      "i: 200 , cost :  5.83998 , training accuracy :  0.7789\n",
      "i: 300 , cost :  7.54655 , training accuracy :  0.686869\n",
      "i: 400 , cost :  5.45409 , training accuracy :  0.786756\n",
      "i: 500 , cost :  8.19852 , training accuracy :  0.739618\n",
      "i: 600 , cost :  4.42999 , training accuracy :  0.81257\n",
      "i: 700 , cost :  6.48495 , training accuracy :  0.773288\n",
      "i: 800 , cost :  5.38511 , training accuracy :  0.750842\n",
      "i: 900 , cost :  2.61718 , training accuracy :  0.843996\n",
      "i: 1000 , cost :  2.6263 , training accuracy :  0.828283\n",
      "i: 1100 , cost :  4.86534 , training accuracy :  0.781145\n",
      "i: 1200 , cost :  6.13309 , training accuracy :  0.7789\n",
      "i: 1300 , cost :  4.02569 , training accuracy :  0.792368\n",
      "i: 1400 , cost :  25.7058 , training accuracy :  0.40853\n",
      "i: 1500 , cost :  2.5397 , training accuracy :  0.848485\n",
      "i: 1600 , cost :  18.361 , training accuracy :  0.50954\n",
      "i: 1700 , cost :  2.10717 , training accuracy :  0.843996\n",
      "i: 1800 , cost :  8.59851 , training accuracy :  0.765432\n",
      "i: 1900 , cost :  2.89938 , training accuracy :  0.815937\n",
      "i: 2000 , cost :  4.0444 , training accuracy :  0.836139\n",
      "i: 2100 , cost :  10.9538 , training accuracy :  0.723906\n",
      "i: 2200 , cost :  2.90835 , training accuracy :  0.847363\n",
      "i: 2300 , cost :  10.3061 , training accuracy :  0.750842\n",
      "i: 2400 , cost :  2.5579 , training accuracy :  0.859708\n",
      "i: 2500 , cost :  17.9068 , training accuracy :  0.45679\n",
      "i: 2600 , cost :  2.3686 , training accuracy :  0.856341\n",
      "i: 2700 , cost :  8.34735 , training accuracy :  0.772166\n",
      "i: 2800 , cost :  5.35193 , training accuracy :  0.765432\n",
      "i: 2900 , cost :  4.12498 , training accuracy :  0.815937\n",
      "i: 3000 , cost :  5.4307 , training accuracy :  0.803591\n",
      "i: 3100 , cost :  1.99261 , training accuracy :  0.863075\n",
      "i: 3200 , cost :  10.2061 , training accuracy :  0.784512\n",
      "i: 3300 , cost :  2.30007 , training accuracy :  0.784512\n",
      "i: 3400 , cost :  6.69661 , training accuracy :  0.791246\n",
      "i: 3500 , cost :  1.92603 , training accuracy :  0.861953\n",
      "i: 3600 , cost :  8.77617 , training accuracy :  0.566779\n",
      "i: 3700 , cost :  4.08579 , training accuracy :  0.806958\n",
      "i: 3800 , cost :  1.54781 , training accuracy :  0.867565\n",
      "i: 3900 , cost :  3.95635 , training accuracy :  0.803591\n",
      "i: 4000 , cost :  5.13419 , training accuracy :  0.750842\n",
      "i: 4100 , cost :  2.1593 , training accuracy :  0.828283\n",
      "i: 4200 , cost :  2.85973 , training accuracy :  0.82716\n",
      "i: 4300 , cost :  7.43985 , training accuracy :  0.7789\n",
      "i: 4400 , cost :  4.46937 , training accuracy :  0.822671\n",
      "i: 4500 , cost :  22.3282 , training accuracy :  0.668911\n",
      "i: 4600 , cost :  13.4394 , training accuracy :  0.71156\n",
      "i: 4700 , cost :  1.79915 , training accuracy :  0.826038\n",
      "i: 4800 , cost :  5.09954 , training accuracy :  0.746352\n",
      "i: 4900 , cost :  1.50851 , training accuracy :  0.875421\n",
      "i: 5000 , cost :  4.46496 , training accuracy :  0.683502\n",
      "i: 5100 , cost :  1.82218 , training accuracy :  0.873176\n",
      "i: 5200 , cost :  2.89756 , training accuracy :  0.804714\n",
      "i: 5300 , cost :  2.8929 , training accuracy :  0.852974\n",
      "i: 5400 , cost :  28.1568 , training accuracy :  0.421998\n",
      "i: 5500 , cost :  1.85541 , training accuracy :  0.878788\n",
      "i: 5600 , cost :  35.3814 , training accuracy :  0.42312\n",
      "i: 5700 , cost :  2.09537 , training accuracy :  0.873176\n",
      "i: 5800 , cost :  3.06094 , training accuracy :  0.765432\n",
      "i: 5900 , cost :  4.93134 , training accuracy :  0.799102\n",
      "i: 6000 , cost :  3.99939 , training accuracy :  0.75982\n",
      "i: 6100 , cost :  10.1236 , training accuracy :  0.800224\n",
      "i: 6200 , cost :  5.29764 , training accuracy :  0.83165\n",
      "i: 6300 , cost :  1.85169 , training accuracy :  0.87991\n",
      "i: 6400 , cost :  15.0768 , training accuracy :  0.52413\n",
      "i: 6500 , cost :  1.2861 , training accuracy :  0.870932\n",
      "i: 6600 , cost :  12.9656 , training accuracy :  0.56229\n",
      "i: 6700 , cost :  1.69148 , training accuracy :  0.875421\n",
      "i: 6800 , cost :  2.87074 , training accuracy :  0.773288\n",
      "i: 6900 , cost :  1.0928 , training accuracy :  0.883277\n",
      "i: 7000 , cost :  1.77099 , training accuracy :  0.876543\n",
      "i: 7100 , cost :  5.73449 , training accuracy :  0.821549\n",
      "i: 7200 , cost :  9.90479 , training accuracy :  0.501683\n",
      "i: 7300 , cost :  1.853 , training accuracy :  0.878788\n",
      "i: 7400 , cost :  5.23153 , training accuracy :  0.803591\n",
      "i: 7500 , cost :  1.56535 , training accuracy :  0.869809\n",
      "i: 7600 , cost :  13.5706 , training accuracy :  0.485971\n",
      "i: 7700 , cost :  3.53354 , training accuracy :  0.854097\n",
      "i: 7800 , cost :  3.4749 , training accuracy :  0.837261\n",
      "i: 7900 , cost :  1.68326 , training accuracy :  0.876543\n",
      "i: 8000 , cost :  3.14289 , training accuracy :  0.836139\n",
      "i: 8100 , cost :  7.56174 , training accuracy :  0.792368\n",
      "i: 8200 , cost :  24.1113 , training accuracy :  0.462402\n",
      "i: 8300 , cost :  8.08415 , training accuracy :  0.794613\n",
      "i: 8400 , cost :  1.10775 , training accuracy :  0.886644\n",
      "i: 8500 , cost :  3.17228 , training accuracy :  0.847363\n",
      "i: 8600 , cost :  1.32492 , training accuracy :  0.891134\n",
      "i: 8700 , cost :  19.8966 , training accuracy :  0.719416\n",
      "i: 8800 , cost :  2.01749 , training accuracy :  0.877666\n",
      "i: 8900 , cost :  1.72745 , training accuracy :  0.841751\n",
      "i: 9000 , cost :  3.60294 , training accuracy :  0.79349\n",
      "i: 9100 , cost :  5.5204 , training accuracy :  0.803591\n",
      "i: 9200 , cost :  1.38066 , training accuracy :  0.888889\n",
      "i: 9300 , cost :  5.29312 , training accuracy :  0.694725\n",
      "i: 9400 , cost :  3.15647 , training accuracy :  0.792368\n",
      "i: 9500 , cost :  1.33897 , training accuracy :  0.863075\n",
      "i: 9600 , cost :  18.1874 , training accuracy :  0.460157\n",
      "i: 9700 , cost :  3.71913 , training accuracy :  0.757576\n",
      "i: 9800 , cost :  1.27357 , training accuracy :  0.881033\n",
      "i: 9900 , cost :  1.39215 , training accuracy :  0.886644\n",
      "Final training accuracy :  0.872054\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 10000 ][hidden: 100 ][file: bhavul_tr_acc_0.87_prediction.csv ] ACCURACY :  0.872054\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  20.9846 , training accuracy :  0.425365\n",
      "i: 100 , cost :  0.65774 , training accuracy :  0.635241\n",
      "i: 200 , cost :  0.656505 , training accuracy :  0.635241\n",
      "i: 300 , cost :  0.655156 , training accuracy :  0.635241\n",
      "i: 400 , cost :  0.654664 , training accuracy :  0.635241\n",
      "i: 500 , cost :  0.654588 , training accuracy :  0.635241\n",
      "i: 600 , cost :  0.654551 , training accuracy :  0.635241\n",
      "i: 700 , cost :  0.654518 , training accuracy :  0.635241\n",
      "i: 800 , cost :  0.654482 , training accuracy :  0.635241\n",
      "i: 900 , cost :  0.654449 , training accuracy :  0.635241\n",
      "i: 1000 , cost :  0.65442 , training accuracy :  0.635241\n",
      "i: 1100 , cost :  0.654139 , training accuracy :  0.635241\n",
      "i: 1200 , cost :  0.654047 , training accuracy :  0.635241\n",
      "i: 1300 , cost :  0.654004 , training accuracy :  0.635241\n",
      "i: 1400 , cost :  0.653962 , training accuracy :  0.635241\n",
      "i: 1500 , cost :  0.65392 , training accuracy :  0.635241\n",
      "i: 1600 , cost :  0.653875 , training accuracy :  0.635241\n",
      "i: 1700 , cost :  0.653837 , training accuracy :  0.635241\n",
      "i: 1800 , cost :  0.653808 , training accuracy :  0.635241\n",
      "i: 1900 , cost :  0.653775 , training accuracy :  0.635241\n",
      "i: 2000 , cost :  0.653751 , training accuracy :  0.635241\n",
      "i: 2100 , cost :  0.653728 , training accuracy :  0.635241\n",
      "i: 2200 , cost :  0.653705 , training accuracy :  0.635241\n",
      "i: 2300 , cost :  0.653681 , training accuracy :  0.635241\n",
      "i: 2400 , cost :  0.653657 , training accuracy :  0.636364\n",
      "i: 2500 , cost :  0.65363 , training accuracy :  0.636364\n",
      "i: 2600 , cost :  0.653604 , training accuracy :  0.636364\n",
      "i: 2700 , cost :  0.653576 , training accuracy :  0.636364\n",
      "i: 2800 , cost :  0.653547 , training accuracy :  0.636364\n",
      "i: 2900 , cost :  0.653516 , training accuracy :  0.636364\n",
      "i: 3000 , cost :  0.653186 , training accuracy :  0.635241\n",
      "i: 3100 , cost :  0.653132 , training accuracy :  0.635241\n",
      "i: 3200 , cost :  0.65309 , training accuracy :  0.636364\n",
      "i: 3300 , cost :  0.653046 , training accuracy :  0.636364\n",
      "i: 3400 , cost :  0.653 , training accuracy :  0.636364\n",
      "i: 3500 , cost :  0.65295 , training accuracy :  0.636364\n",
      "i: 3600 , cost :  0.652901 , training accuracy :  0.636364\n",
      "i: 3700 , cost :  0.652844 , training accuracy :  0.636364\n",
      "i: 3800 , cost :  0.652787 , training accuracy :  0.636364\n",
      "i: 3900 , cost :  0.652722 , training accuracy :  0.636364\n",
      "i: 4000 , cost :  0.652652 , training accuracy :  0.636364\n",
      "i: 4100 , cost :  0.652573 , training accuracy :  0.636364\n",
      "i: 4200 , cost :  0.65249 , training accuracy :  0.636364\n",
      "i: 4300 , cost :  0.6524 , training accuracy :  0.636364\n",
      "i: 4400 , cost :  0.652303 , training accuracy :  0.636364\n",
      "i: 4500 , cost :  0.652199 , training accuracy :  0.637486\n",
      "i: 4600 , cost :  0.652084 , training accuracy :  0.637486\n",
      "i: 4700 , cost :  0.65196 , training accuracy :  0.637486\n",
      "i: 4800 , cost :  0.65182 , training accuracy :  0.637486\n",
      "i: 4900 , cost :  0.65166 , training accuracy :  0.637486\n",
      "i: 5000 , cost :  0.651471 , training accuracy :  0.637486\n",
      "i: 5100 , cost :  0.651244 , training accuracy :  0.637486\n",
      "i: 5200 , cost :  0.650971 , training accuracy :  0.637486\n",
      "i: 5300 , cost :  0.650653 , training accuracy :  0.637486\n",
      "i: 5400 , cost :  0.650195 , training accuracy :  0.639731\n",
      "i: 5500 , cost :  0.649533 , training accuracy :  0.639731\n",
      "i: 5600 , cost :  0.648549 , training accuracy :  0.640853\n",
      "i: 5700 , cost :  0.644705 , training accuracy :  0.643098\n",
      "i: 5800 , cost :  0.481363 , training accuracy :  0.803591\n",
      "i: 5900 , cost :  0.464705 , training accuracy :  0.809203\n",
      "i: 6000 , cost :  0.478564 , training accuracy :  0.800224\n",
      "i: 6100 , cost :  0.454394 , training accuracy :  0.81257\n",
      "i: 6200 , cost :  0.453268 , training accuracy :  0.81257\n",
      "i: 6300 , cost :  0.453026 , training accuracy :  0.811448\n",
      "i: 6400 , cost :  0.45403 , training accuracy :  0.815937\n",
      "i: 6500 , cost :  0.456507 , training accuracy :  0.81257\n",
      "i: 6600 , cost :  0.452087 , training accuracy :  0.813693\n",
      "i: 6700 , cost :  0.452048 , training accuracy :  0.815937\n",
      "i: 6800 , cost :  0.45152 , training accuracy :  0.81257\n",
      "i: 6900 , cost :  0.451487 , training accuracy :  0.813693\n",
      "i: 7000 , cost :  0.451223 , training accuracy :  0.81257\n",
      "i: 7100 , cost :  0.451093 , training accuracy :  0.813693\n",
      "i: 7200 , cost :  0.451361 , training accuracy :  0.810326\n",
      "i: 7300 , cost :  0.451086 , training accuracy :  0.815937\n",
      "i: 7400 , cost :  0.451845 , training accuracy :  0.81257\n",
      "i: 7500 , cost :  0.451251 , training accuracy :  0.81257\n",
      "i: 7600 , cost :  0.45852 , training accuracy :  0.808081\n",
      "i: 7700 , cost :  0.455756 , training accuracy :  0.813693\n",
      "i: 7800 , cost :  0.450999 , training accuracy :  0.815937\n",
      "i: 7900 , cost :  0.451831 , training accuracy :  0.815937\n",
      "i: 8000 , cost :  0.451138 , training accuracy :  0.81257\n",
      "i: 8100 , cost :  0.450839 , training accuracy :  0.815937\n",
      "i: 8200 , cost :  0.450622 , training accuracy :  0.813693\n",
      "i: 8300 , cost :  0.451276 , training accuracy :  0.813693\n",
      "i: 8400 , cost :  0.450568 , training accuracy :  0.813693\n",
      "i: 8500 , cost :  0.451572 , training accuracy :  0.81257\n",
      "i: 8600 , cost :  0.451785 , training accuracy :  0.817059\n",
      "i: 8700 , cost :  0.451062 , training accuracy :  0.818182\n",
      "i: 8800 , cost :  0.451379 , training accuracy :  0.815937\n",
      "i: 8900 , cost :  0.457578 , training accuracy :  0.810326\n",
      "i: 9000 , cost :  0.450864 , training accuracy :  0.813693\n",
      "i: 9100 , cost :  0.451156 , training accuracy :  0.813693\n",
      "i: 9200 , cost :  0.451824 , training accuracy :  0.817059\n",
      "i: 9300 , cost :  0.451208 , training accuracy :  0.813693\n",
      "i: 9400 , cost :  0.451562 , training accuracy :  0.819304\n",
      "i: 9500 , cost :  0.450928 , training accuracy :  0.818182\n",
      "i: 9600 , cost :  0.454806 , training accuracy :  0.81257\n",
      "i: 9700 , cost :  0.450533 , training accuracy :  0.813693\n",
      "i: 9800 , cost :  0.450619 , training accuracy :  0.817059\n",
      "i: 9900 , cost :  0.450641 , training accuracy :  0.813693\n",
      "i: 10000 , cost :  0.452955 , training accuracy :  0.813693\n",
      "i: 10100 , cost :  0.450524 , training accuracy :  0.813693\n",
      "i: 10200 , cost :  0.452666 , training accuracy :  0.81257\n",
      "i: 10300 , cost :  0.451365 , training accuracy :  0.813693\n",
      "i: 10400 , cost :  0.451093 , training accuracy :  0.813693\n",
      "i: 10500 , cost :  0.451696 , training accuracy :  0.81257\n",
      "i: 10600 , cost :  0.451176 , training accuracy :  0.815937\n",
      "i: 10700 , cost :  0.451217 , training accuracy :  0.815937\n",
      "i: 10800 , cost :  0.453993 , training accuracy :  0.811448\n",
      "i: 10900 , cost :  0.451922 , training accuracy :  0.81257\n",
      "i: 11000 , cost :  0.45139 , training accuracy :  0.813693\n",
      "i: 11100 , cost :  0.451473 , training accuracy :  0.81257\n",
      "i: 11200 , cost :  0.450918 , training accuracy :  0.81257\n",
      "i: 11300 , cost :  0.453493 , training accuracy :  0.814815\n",
      "i: 11400 , cost :  0.45194 , training accuracy :  0.817059\n",
      "i: 11500 , cost :  0.450796 , training accuracy :  0.817059\n",
      "i: 11600 , cost :  0.450588 , training accuracy :  0.818182\n",
      "i: 11700 , cost :  0.450769 , training accuracy :  0.815937\n",
      "i: 11800 , cost :  0.451706 , training accuracy :  0.817059\n",
      "i: 11900 , cost :  0.452623 , training accuracy :  0.81257\n",
      "i: 12000 , cost :  0.450593 , training accuracy :  0.813693\n",
      "i: 12100 , cost :  0.450549 , training accuracy :  0.815937\n",
      "i: 12200 , cost :  0.451135 , training accuracy :  0.815937\n",
      "i: 12300 , cost :  0.451159 , training accuracy :  0.815937\n",
      "i: 12400 , cost :  0.451169 , training accuracy :  0.815937\n",
      "i: 12500 , cost :  0.451174 , training accuracy :  0.815937\n",
      "i: 12600 , cost :  0.451175 , training accuracy :  0.815937\n",
      "i: 12700 , cost :  0.451174 , training accuracy :  0.815937\n",
      "i: 12800 , cost :  0.451173 , training accuracy :  0.815937\n",
      "i: 12900 , cost :  0.45117 , training accuracy :  0.815937\n",
      "i: 13000 , cost :  0.451166 , training accuracy :  0.815937\n",
      "i: 13100 , cost :  0.451163 , training accuracy :  0.815937\n",
      "i: 13200 , cost :  0.45116 , training accuracy :  0.815937\n",
      "i: 13300 , cost :  0.451155 , training accuracy :  0.815937\n",
      "i: 13400 , cost :  0.451151 , training accuracy :  0.815937\n",
      "i: 13500 , cost :  0.451148 , training accuracy :  0.815937\n",
      "i: 13600 , cost :  0.451145 , training accuracy :  0.815937\n",
      "i: 13700 , cost :  0.451141 , training accuracy :  0.815937\n",
      "i: 13800 , cost :  0.451137 , training accuracy :  0.815937\n",
      "i: 13900 , cost :  0.451134 , training accuracy :  0.815937\n",
      "i: 14000 , cost :  0.451129 , training accuracy :  0.815937\n",
      "i: 14100 , cost :  0.451125 , training accuracy :  0.815937\n",
      "i: 14200 , cost :  0.451122 , training accuracy :  0.815937\n",
      "i: 14300 , cost :  0.451119 , training accuracy :  0.815937\n",
      "i: 14400 , cost :  0.451115 , training accuracy :  0.815937\n",
      "i: 14500 , cost :  0.451112 , training accuracy :  0.815937\n",
      "i: 14600 , cost :  0.451107 , training accuracy :  0.815937\n",
      "i: 14700 , cost :  0.451103 , training accuracy :  0.815937\n",
      "i: 14800 , cost :  0.4511 , training accuracy :  0.815937\n",
      "i: 14900 , cost :  0.451098 , training accuracy :  0.815937\n",
      "i: 15000 , cost :  0.451094 , training accuracy :  0.815937\n",
      "i: 15100 , cost :  0.451091 , training accuracy :  0.815937\n",
      "i: 15200 , cost :  0.451086 , training accuracy :  0.815937\n",
      "i: 15300 , cost :  0.451083 , training accuracy :  0.815937\n",
      "i: 15400 , cost :  0.451079 , training accuracy :  0.815937\n",
      "i: 15500 , cost :  0.451077 , training accuracy :  0.815937\n",
      "i: 15600 , cost :  0.451073 , training accuracy :  0.815937\n",
      "i: 15700 , cost :  0.451071 , training accuracy :  0.815937\n",
      "i: 15800 , cost :  0.451066 , training accuracy :  0.815937\n",
      "i: 15900 , cost :  0.451063 , training accuracy :  0.815937\n",
      "i: 16000 , cost :  0.45106 , training accuracy :  0.817059\n",
      "i: 16100 , cost :  0.451057 , training accuracy :  0.817059\n",
      "i: 16200 , cost :  0.451054 , training accuracy :  0.817059\n",
      "i: 16300 , cost :  0.451051 , training accuracy :  0.817059\n",
      "i: 16400 , cost :  0.451048 , training accuracy :  0.817059\n",
      "i: 16500 , cost :  0.451044 , training accuracy :  0.817059\n",
      "i: 16600 , cost :  0.451041 , training accuracy :  0.817059\n",
      "i: 16700 , cost :  0.451038 , training accuracy :  0.817059\n",
      "i: 16800 , cost :  0.451036 , training accuracy :  0.817059\n",
      "i: 16900 , cost :  0.451033 , training accuracy :  0.817059\n",
      "i: 17000 , cost :  0.45103 , training accuracy :  0.817059\n",
      "i: 17100 , cost :  0.451026 , training accuracy :  0.817059\n",
      "i: 17200 , cost :  0.451023 , training accuracy :  0.817059\n",
      "i: 17300 , cost :  0.45102 , training accuracy :  0.817059\n",
      "i: 17400 , cost :  0.451018 , training accuracy :  0.817059\n",
      "i: 17500 , cost :  0.451015 , training accuracy :  0.817059\n",
      "i: 17600 , cost :  0.451013 , training accuracy :  0.817059\n",
      "i: 17700 , cost :  0.45101 , training accuracy :  0.817059\n",
      "i: 17800 , cost :  0.451007 , training accuracy :  0.817059\n",
      "i: 17900 , cost :  0.451004 , training accuracy :  0.817059\n",
      "i: 18000 , cost :  0.451 , training accuracy :  0.817059\n",
      "i: 18100 , cost :  0.450998 , training accuracy :  0.817059\n",
      "i: 18200 , cost :  0.450996 , training accuracy :  0.817059\n",
      "i: 18300 , cost :  0.450994 , training accuracy :  0.817059\n",
      "i: 18400 , cost :  0.450991 , training accuracy :  0.817059\n",
      "i: 18500 , cost :  0.450988 , training accuracy :  0.817059\n",
      "i: 18600 , cost :  0.450985 , training accuracy :  0.817059\n",
      "i: 18700 , cost :  0.450982 , training accuracy :  0.817059\n",
      "i: 18800 , cost :  0.45098 , training accuracy :  0.817059\n",
      "i: 18900 , cost :  0.450977 , training accuracy :  0.817059\n",
      "i: 19000 , cost :  0.450975 , training accuracy :  0.818182\n",
      "i: 19100 , cost :  0.450973 , training accuracy :  0.818182\n",
      "i: 19200 , cost :  0.45097 , training accuracy :  0.818182\n",
      "i: 19300 , cost :  0.450968 , training accuracy :  0.818182\n",
      "i: 19400 , cost :  0.451513 , training accuracy :  0.817059\n",
      "i: 19500 , cost :  0.451223 , training accuracy :  0.813693\n",
      "i: 19600 , cost :  0.451031 , training accuracy :  0.81257\n",
      "i: 19700 , cost :  0.450942 , training accuracy :  0.819304\n",
      "i: 19800 , cost :  0.452193 , training accuracy :  0.81257\n",
      "i: 19900 , cost :  0.450462 , training accuracy :  0.813693\n",
      "i: 20000 , cost :  0.451195 , training accuracy :  0.813693\n",
      "i: 20100 , cost :  0.450815 , training accuracy :  0.818182\n",
      "i: 20200 , cost :  0.450809 , training accuracy :  0.818182\n",
      "i: 20300 , cost :  0.450673 , training accuracy :  0.814815\n",
      "i: 20400 , cost :  0.450603 , training accuracy :  0.815937\n",
      "i: 20500 , cost :  0.451396 , training accuracy :  0.813693\n",
      "i: 20600 , cost :  0.452827 , training accuracy :  0.81257\n",
      "i: 20700 , cost :  0.450853 , training accuracy :  0.818182\n",
      "i: 20800 , cost :  0.450488 , training accuracy :  0.814815\n",
      "i: 20900 , cost :  0.450499 , training accuracy :  0.814815\n",
      "i: 21000 , cost :  0.450607 , training accuracy :  0.815937\n",
      "i: 21100 , cost :  0.450881 , training accuracy :  0.819304\n",
      "i: 21200 , cost :  0.451735 , training accuracy :  0.81257\n",
      "i: 21300 , cost :  0.450566 , training accuracy :  0.818182\n",
      "i: 21400 , cost :  0.451516 , training accuracy :  0.818182\n",
      "i: 21500 , cost :  0.450455 , training accuracy :  0.813693\n",
      "i: 21600 , cost :  0.450465 , training accuracy :  0.813693\n",
      "i: 21700 , cost :  0.450455 , training accuracy :  0.813693\n",
      "i: 21800 , cost :  0.451005 , training accuracy :  0.81257\n",
      "i: 21900 , cost :  0.4505 , training accuracy :  0.814815\n",
      "i: 22000 , cost :  0.450586 , training accuracy :  0.815937\n",
      "i: 22100 , cost :  0.450497 , training accuracy :  0.814815\n",
      "i: 22200 , cost :  0.451754 , training accuracy :  0.811448\n",
      "i: 22300 , cost :  0.453096 , training accuracy :  0.81257\n",
      "i: 22400 , cost :  0.451803 , training accuracy :  0.81257\n",
      "i: 22500 , cost :  0.450851 , training accuracy :  0.818182\n",
      "i: 22600 , cost :  0.451273 , training accuracy :  0.813693\n",
      "i: 22700 , cost :  0.450555 , training accuracy :  0.813693\n",
      "i: 22800 , cost :  0.451971 , training accuracy :  0.81257\n",
      "i: 22900 , cost :  0.451619 , training accuracy :  0.81257\n",
      "i: 23000 , cost :  0.450469 , training accuracy :  0.813693\n",
      "i: 23100 , cost :  0.450844 , training accuracy :  0.819304\n",
      "i: 23200 , cost :  0.451297 , training accuracy :  0.815937\n",
      "i: 23300 , cost :  0.451147 , training accuracy :  0.813693\n",
      "i: 23400 , cost :  0.451333 , training accuracy :  0.813693\n",
      "i: 23500 , cost :  0.450607 , training accuracy :  0.813693\n",
      "i: 23600 , cost :  0.450557 , training accuracy :  0.817059\n",
      "i: 23700 , cost :  0.450451 , training accuracy :  0.813693\n",
      "i: 23800 , cost :  0.450466 , training accuracy :  0.813693\n",
      "i: 23900 , cost :  0.450466 , training accuracy :  0.813693\n",
      "i: 24000 , cost :  0.450453 , training accuracy :  0.813693\n",
      "i: 24100 , cost :  0.451852 , training accuracy :  0.81257\n",
      "i: 24200 , cost :  0.453158 , training accuracy :  0.815937\n",
      "i: 24300 , cost :  0.451435 , training accuracy :  0.813693\n",
      "i: 24400 , cost :  0.450478 , training accuracy :  0.814815\n",
      "i: 24500 , cost :  0.450881 , training accuracy :  0.81257\n",
      "i: 24600 , cost :  0.452435 , training accuracy :  0.81257\n",
      "i: 24700 , cost :  0.45133 , training accuracy :  0.813693\n",
      "i: 24800 , cost :  0.450735 , training accuracy :  0.817059\n",
      "i: 24900 , cost :  0.450706 , training accuracy :  0.815937\n",
      "i: 25000 , cost :  0.450512 , training accuracy :  0.814815\n",
      "i: 25100 , cost :  0.45114 , training accuracy :  0.814815\n",
      "i: 25200 , cost :  0.450736 , training accuracy :  0.813693\n",
      "i: 25300 , cost :  0.451114 , training accuracy :  0.815937\n",
      "i: 25400 , cost :  0.450732 , training accuracy :  0.815937\n",
      "i: 25500 , cost :  0.450925 , training accuracy :  0.813693\n",
      "i: 25600 , cost :  0.452021 , training accuracy :  0.81257\n",
      "i: 25700 , cost :  0.451009 , training accuracy :  0.819304\n",
      "i: 25800 , cost :  0.451515 , training accuracy :  0.813693\n",
      "i: 25900 , cost :  0.450954 , training accuracy :  0.813693\n",
      "i: 26000 , cost :  0.450947 , training accuracy :  0.813693\n",
      "i: 26100 , cost :  0.450941 , training accuracy :  0.813693\n",
      "i: 26200 , cost :  0.450936 , training accuracy :  0.813693\n",
      "i: 26300 , cost :  0.450935 , training accuracy :  0.813693\n",
      "i: 26400 , cost :  0.450933 , training accuracy :  0.813693\n",
      "i: 26500 , cost :  0.450932 , training accuracy :  0.813693\n",
      "i: 26600 , cost :  0.45093 , training accuracy :  0.813693\n",
      "i: 26700 , cost :  0.450929 , training accuracy :  0.813693\n",
      "i: 26800 , cost :  0.450928 , training accuracy :  0.813693\n",
      "i: 26900 , cost :  0.450926 , training accuracy :  0.813693\n",
      "i: 27000 , cost :  0.450925 , training accuracy :  0.813693\n",
      "i: 27100 , cost :  0.450925 , training accuracy :  0.813693\n",
      "i: 27200 , cost :  0.450924 , training accuracy :  0.813693\n",
      "i: 27300 , cost :  0.450922 , training accuracy :  0.813693\n",
      "i: 27400 , cost :  0.450921 , training accuracy :  0.813693\n",
      "i: 27500 , cost :  0.45092 , training accuracy :  0.813693\n",
      "i: 27600 , cost :  0.450919 , training accuracy :  0.813693\n",
      "i: 27700 , cost :  0.450917 , training accuracy :  0.813693\n",
      "i: 27800 , cost :  0.450916 , training accuracy :  0.813693\n",
      "i: 27900 , cost :  0.450915 , training accuracy :  0.813693\n",
      "i: 28000 , cost :  0.450913 , training accuracy :  0.813693\n",
      "i: 28100 , cost :  0.450912 , training accuracy :  0.813693\n",
      "i: 28200 , cost :  0.450911 , training accuracy :  0.813693\n",
      "i: 28300 , cost :  0.45091 , training accuracy :  0.813693\n",
      "i: 28400 , cost :  0.450909 , training accuracy :  0.813693\n",
      "i: 28500 , cost :  0.450906 , training accuracy :  0.813693\n",
      "i: 28600 , cost :  0.450905 , training accuracy :  0.813693\n",
      "i: 28700 , cost :  0.450904 , training accuracy :  0.813693\n",
      "i: 28800 , cost :  0.450902 , training accuracy :  0.813693\n",
      "i: 28900 , cost :  0.450902 , training accuracy :  0.813693\n",
      "i: 29000 , cost :  0.450901 , training accuracy :  0.813693\n",
      "i: 29100 , cost :  0.450899 , training accuracy :  0.813693\n",
      "i: 29200 , cost :  0.450898 , training accuracy :  0.813693\n",
      "i: 29300 , cost :  0.450896 , training accuracy :  0.813693\n",
      "i: 29400 , cost :  0.450895 , training accuracy :  0.813693\n",
      "i: 29500 , cost :  0.450894 , training accuracy :  0.813693\n",
      "i: 29600 , cost :  0.450893 , training accuracy :  0.813693\n",
      "i: 29700 , cost :  0.450892 , training accuracy :  0.813693\n",
      "i: 29800 , cost :  0.450891 , training accuracy :  0.813693\n",
      "i: 29900 , cost :  0.45089 , training accuracy :  0.813693\n",
      "Final training accuracy :  0.813693\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 30000 ][hidden: 3 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.813693\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  388.143 , training accuracy :  0.383838\n",
      "i: 100 , cost :  4.25642 , training accuracy :  0.662177\n",
      "i: 200 , cost :  0.516116 , training accuracy :  0.796857\n",
      "i: 300 , cost :  0.473718 , training accuracy :  0.809203\n",
      "i: 400 , cost :  0.473469 , training accuracy :  0.800224\n",
      "i: 500 , cost :  0.455138 , training accuracy :  0.81257\n",
      "i: 600 , cost :  0.504131 , training accuracy :  0.782267\n",
      "i: 700 , cost :  0.448181 , training accuracy :  0.809203\n",
      "i: 800 , cost :  0.592496 , training accuracy :  0.728395\n",
      "i: 900 , cost :  0.446386 , training accuracy :  0.808081\n",
      "i: 1000 , cost :  0.48899 , training accuracy :  0.792368\n",
      "i: 1100 , cost :  0.441793 , training accuracy :  0.81257\n",
      "i: 1200 , cost :  0.4431 , training accuracy :  0.808081\n",
      "i: 1300 , cost :  0.43601 , training accuracy :  0.819304\n",
      "i: 1400 , cost :  0.46134 , training accuracy :  0.813693\n",
      "i: 1500 , cost :  0.460052 , training accuracy :  0.813693\n",
      "i: 1600 , cost :  0.458044 , training accuracy :  0.817059\n",
      "i: 1700 , cost :  0.456049 , training accuracy :  0.819304\n",
      "i: 1800 , cost :  0.45514 , training accuracy :  0.822671\n",
      "i: 1900 , cost :  0.455022 , training accuracy :  0.821549\n",
      "i: 2000 , cost :  0.458501 , training accuracy :  0.817059\n",
      "i: 2100 , cost :  0.452557 , training accuracy :  0.824916\n",
      "i: 2200 , cost :  0.452567 , training accuracy :  0.826038\n",
      "i: 2300 , cost :  0.448189 , training accuracy :  0.818182\n",
      "i: 2400 , cost :  0.455831 , training accuracy :  0.815937\n",
      "i: 2500 , cost :  0.779054 , training accuracy :  0.716049\n",
      "i: 2600 , cost :  0.425311 , training accuracy :  0.82716\n",
      "i: 2700 , cost :  0.422792 , training accuracy :  0.819304\n",
      "i: 2800 , cost :  0.421715 , training accuracy :  0.819304\n",
      "i: 2900 , cost :  0.420794 , training accuracy :  0.821549\n",
      "i: 3000 , cost :  0.437161 , training accuracy :  0.792368\n",
      "i: 3100 , cost :  0.447469 , training accuracy :  0.795735\n",
      "i: 3200 , cost :  0.483115 , training accuracy :  0.79798\n",
      "i: 3300 , cost :  0.418476 , training accuracy :  0.823793\n",
      "i: 3400 , cost :  0.417292 , training accuracy :  0.822671\n",
      "i: 3500 , cost :  0.416492 , training accuracy :  0.820426\n",
      "i: 3600 , cost :  0.415729 , training accuracy :  0.822671\n",
      "i: 3700 , cost :  0.430022 , training accuracy :  0.795735\n",
      "i: 3800 , cost :  0.465986 , training accuracy :  0.801347\n",
      "i: 3900 , cost :  0.436036 , training accuracy :  0.820426\n",
      "i: 4000 , cost :  0.417783 , training accuracy :  0.814815\n",
      "i: 4100 , cost :  0.411214 , training accuracy :  0.815937\n",
      "i: 4200 , cost :  0.463872 , training accuracy :  0.785634\n",
      "i: 4300 , cost :  0.411933 , training accuracy :  0.81257\n",
      "i: 4400 , cost :  0.40927 , training accuracy :  0.817059\n",
      "i: 4500 , cost :  0.408469 , training accuracy :  0.817059\n",
      "i: 4600 , cost :  0.407966 , training accuracy :  0.815937\n",
      "i: 4700 , cost :  0.407625 , training accuracy :  0.815937\n",
      "i: 4800 , cost :  0.421068 , training accuracy :  0.823793\n",
      "i: 4900 , cost :  0.416239 , training accuracy :  0.811448\n",
      "i: 5000 , cost :  0.424235 , training accuracy :  0.826038\n",
      "i: 5100 , cost :  0.43323 , training accuracy :  0.822671\n",
      "i: 5200 , cost :  0.420716 , training accuracy :  0.82716\n",
      "i: 5300 , cost :  0.408659 , training accuracy :  0.83165\n",
      "i: 5400 , cost :  0.420095 , training accuracy :  0.808081\n",
      "i: 5500 , cost :  0.417218 , training accuracy :  0.811448\n",
      "i: 5600 , cost :  0.436679 , training accuracy :  0.820426\n",
      "i: 5700 , cost :  0.433297 , training accuracy :  0.801347\n",
      "i: 5800 , cost :  0.406469 , training accuracy :  0.815937\n",
      "i: 5900 , cost :  0.406421 , training accuracy :  0.82716\n",
      "i: 6000 , cost :  0.409183 , training accuracy :  0.811448\n",
      "i: 6100 , cost :  0.434698 , training accuracy :  0.821549\n",
      "i: 6200 , cost :  0.433617 , training accuracy :  0.800224\n",
      "i: 6300 , cost :  0.4347 , training accuracy :  0.819304\n",
      "i: 6400 , cost :  0.415986 , training accuracy :  0.809203\n",
      "i: 6500 , cost :  0.413569 , training accuracy :  0.830527\n",
      "i: 6600 , cost :  0.40647 , training accuracy :  0.810326\n",
      "i: 6700 , cost :  0.406754 , training accuracy :  0.82716\n",
      "i: 6800 , cost :  0.405856 , training accuracy :  0.821549\n",
      "i: 6900 , cost :  0.405976 , training accuracy :  0.822671\n",
      "i: 7000 , cost :  0.417827 , training accuracy :  0.826038\n",
      "i: 7100 , cost :  0.429237 , training accuracy :  0.801347\n",
      "i: 7200 , cost :  0.423311 , training accuracy :  0.824916\n",
      "i: 7300 , cost :  0.406285 , training accuracy :  0.814815\n",
      "i: 7400 , cost :  0.426392 , training accuracy :  0.800224\n",
      "i: 7500 , cost :  0.414681 , training accuracy :  0.824916\n",
      "i: 7600 , cost :  0.427922 , training accuracy :  0.799102\n",
      "i: 7700 , cost :  0.428741 , training accuracy :  0.826038\n",
      "i: 7800 , cost :  0.419523 , training accuracy :  0.804714\n",
      "i: 7900 , cost :  0.411349 , training accuracy :  0.830527\n",
      "i: 8000 , cost :  0.40542 , training accuracy :  0.820426\n",
      "i: 8100 , cost :  0.429118 , training accuracy :  0.800224\n",
      "i: 8200 , cost :  0.414981 , training accuracy :  0.809203\n",
      "i: 8300 , cost :  0.405896 , training accuracy :  0.830527\n",
      "i: 8400 , cost :  0.433767 , training accuracy :  0.819304\n",
      "i: 8500 , cost :  0.421719 , training accuracy :  0.79798\n",
      "i: 8600 , cost :  0.405321 , training accuracy :  0.821549\n",
      "i: 8700 , cost :  0.413296 , training accuracy :  0.830527\n",
      "i: 8800 , cost :  0.406409 , training accuracy :  0.817059\n",
      "i: 8900 , cost :  0.406484 , training accuracy :  0.82716\n",
      "i: 9000 , cost :  0.429805 , training accuracy :  0.802469\n",
      "i: 9100 , cost :  0.414719 , training accuracy :  0.809203\n",
      "i: 9200 , cost :  0.425712 , training accuracy :  0.828283\n",
      "i: 9300 , cost :  0.40616 , training accuracy :  0.817059\n",
      "i: 9400 , cost :  0.404999 , training accuracy :  0.826038\n",
      "i: 9500 , cost :  0.416078 , training accuracy :  0.824916\n",
      "i: 9600 , cost :  0.404728 , training accuracy :  0.823793\n",
      "i: 9700 , cost :  0.404646 , training accuracy :  0.824916\n",
      "i: 9800 , cost :  0.415953 , training accuracy :  0.809203\n",
      "i: 9900 , cost :  0.430185 , training accuracy :  0.802469\n",
      "i: 10000 , cost :  0.420371 , training accuracy :  0.801347\n",
      "i: 10100 , cost :  0.416177 , training accuracy :  0.808081\n",
      "i: 10200 , cost :  0.414778 , training accuracy :  0.805836\n",
      "i: 10300 , cost :  0.408835 , training accuracy :  0.809203\n",
      "i: 10400 , cost :  0.408939 , training accuracy :  0.830527\n",
      "i: 10500 , cost :  0.405953 , training accuracy :  0.814815\n",
      "i: 10600 , cost :  0.406985 , training accuracy :  0.81257\n",
      "i: 10700 , cost :  0.412795 , training accuracy :  0.806958\n",
      "i: 10800 , cost :  0.410789 , training accuracy :  0.826038\n",
      "i: 10900 , cost :  0.41374 , training accuracy :  0.826038\n",
      "i: 11000 , cost :  0.402807 , training accuracy :  0.829405\n",
      "i: 11100 , cost :  0.404137 , training accuracy :  0.814815\n",
      "i: 11200 , cost :  0.411165 , training accuracy :  0.808081\n",
      "i: 11300 , cost :  0.410424 , training accuracy :  0.823793\n",
      "i: 11400 , cost :  0.420794 , training accuracy :  0.79798\n",
      "i: 11500 , cost :  0.404645 , training accuracy :  0.811448\n",
      "i: 11600 , cost :  0.403096 , training accuracy :  0.813693\n",
      "i: 11700 , cost :  0.409688 , training accuracy :  0.808081\n",
      "i: 11800 , cost :  0.400274 , training accuracy :  0.823793\n",
      "i: 11900 , cost :  0.415561 , training accuracy :  0.823793\n",
      "i: 12000 , cost :  0.405265 , training accuracy :  0.81257\n",
      "i: 12100 , cost :  0.400116 , training accuracy :  0.81257\n",
      "i: 12200 , cost :  0.410348 , training accuracy :  0.808081\n",
      "i: 12300 , cost :  0.403053 , training accuracy :  0.829405\n",
      "i: 12400 , cost :  0.406523 , training accuracy :  0.811448\n",
      "i: 12500 , cost :  0.402986 , training accuracy :  0.820426\n",
      "i: 12600 , cost :  0.415031 , training accuracy :  0.824916\n",
      "i: 12700 , cost :  0.404828 , training accuracy :  0.832772\n",
      "i: 12800 , cost :  0.399463 , training accuracy :  0.829405\n",
      "i: 12900 , cost :  0.403388 , training accuracy :  0.828283\n",
      "i: 13000 , cost :  0.401552 , training accuracy :  0.81257\n",
      "i: 13100 , cost :  0.39965 , training accuracy :  0.830527\n",
      "i: 13200 , cost :  0.398976 , training accuracy :  0.824916\n",
      "i: 13300 , cost :  0.400962 , training accuracy :  0.814815\n",
      "i: 13400 , cost :  0.400175 , training accuracy :  0.829405\n",
      "i: 13500 , cost :  0.405246 , training accuracy :  0.81257\n",
      "i: 13600 , cost :  0.408953 , training accuracy :  0.824916\n",
      "i: 13700 , cost :  0.408011 , training accuracy :  0.819304\n",
      "i: 13800 , cost :  0.399174 , training accuracy :  0.829405\n",
      "i: 13900 , cost :  0.400857 , training accuracy :  0.83165\n",
      "i: 14000 , cost :  0.404409 , training accuracy :  0.830527\n",
      "i: 14100 , cost :  0.402567 , training accuracy :  0.832772\n",
      "i: 14200 , cost :  0.399544 , training accuracy :  0.829405\n",
      "i: 14300 , cost :  0.406119 , training accuracy :  0.809203\n",
      "i: 14400 , cost :  0.408448 , training accuracy :  0.82716\n",
      "i: 14500 , cost :  0.406044 , training accuracy :  0.82716\n",
      "i: 14600 , cost :  0.412418 , training accuracy :  0.803591\n",
      "i: 14700 , cost :  0.40903 , training accuracy :  0.806958\n",
      "i: 14800 , cost :  0.403814 , training accuracy :  0.83165\n",
      "i: 14900 , cost :  0.402149 , training accuracy :  0.835017\n",
      "i: 15000 , cost :  0.400997 , training accuracy :  0.830527\n",
      "i: 15100 , cost :  0.398647 , training accuracy :  0.828283\n",
      "i: 15200 , cost :  0.407098 , training accuracy :  0.808081\n",
      "i: 15300 , cost :  0.402287 , training accuracy :  0.814815\n",
      "i: 15400 , cost :  0.407874 , training accuracy :  0.820426\n",
      "i: 15500 , cost :  0.398325 , training accuracy :  0.823793\n",
      "i: 15600 , cost :  0.403857 , training accuracy :  0.81257\n",
      "i: 15700 , cost :  0.403689 , training accuracy :  0.81257\n",
      "i: 15800 , cost :  0.403151 , training accuracy :  0.81257\n",
      "i: 15900 , cost :  0.409375 , training accuracy :  0.802469\n",
      "i: 16000 , cost :  0.406412 , training accuracy :  0.824916\n",
      "i: 16100 , cost :  0.401115 , training accuracy :  0.811448\n",
      "i: 16200 , cost :  0.399965 , training accuracy :  0.829405\n",
      "i: 16300 , cost :  0.399144 , training accuracy :  0.829405\n",
      "i: 16400 , cost :  0.399237 , training accuracy :  0.817059\n",
      "i: 16500 , cost :  0.398138 , training accuracy :  0.826038\n",
      "i: 16600 , cost :  0.415648 , training accuracy :  0.801347\n",
      "i: 16700 , cost :  0.401158 , training accuracy :  0.811448\n",
      "i: 16800 , cost :  0.4004 , training accuracy :  0.830527\n",
      "i: 16900 , cost :  0.39805 , training accuracy :  0.820426\n",
      "i: 17000 , cost :  0.399454 , training accuracy :  0.829405\n",
      "i: 17100 , cost :  0.400523 , training accuracy :  0.828283\n",
      "i: 17200 , cost :  0.40723 , training accuracy :  0.806958\n",
      "i: 17300 , cost :  0.398508 , training accuracy :  0.829405\n",
      "i: 17400 , cost :  0.40856 , training accuracy :  0.805836\n",
      "i: 17500 , cost :  0.407455 , training accuracy :  0.81257\n",
      "i: 17600 , cost :  0.406008 , training accuracy :  0.813693\n",
      "i: 17700 , cost :  0.41344 , training accuracy :  0.82716\n",
      "i: 17800 , cost :  0.416456 , training accuracy :  0.801347\n",
      "i: 17900 , cost :  0.406127 , training accuracy :  0.823793\n",
      "i: 18000 , cost :  0.405764 , training accuracy :  0.820426\n",
      "i: 18100 , cost :  0.419409 , training accuracy :  0.79798\n",
      "i: 18200 , cost :  0.406504 , training accuracy :  0.823793\n",
      "i: 18300 , cost :  0.421732 , training accuracy :  0.820426\n",
      "i: 18400 , cost :  0.407846 , training accuracy :  0.823793\n",
      "i: 18500 , cost :  0.415317 , training accuracy :  0.802469\n",
      "i: 18600 , cost :  0.405352 , training accuracy :  0.814815\n",
      "i: 18700 , cost :  0.409975 , training accuracy :  0.803591\n",
      "i: 18800 , cost :  0.40527 , training accuracy :  0.818182\n",
      "i: 18900 , cost :  0.408525 , training accuracy :  0.806958\n",
      "i: 19000 , cost :  0.413348 , training accuracy :  0.829405\n",
      "i: 19100 , cost :  0.412746 , training accuracy :  0.802469\n",
      "i: 19200 , cost :  0.410529 , training accuracy :  0.829405\n",
      "i: 19300 , cost :  0.416488 , training accuracy :  0.799102\n",
      "i: 19400 , cost :  0.414477 , training accuracy :  0.824916\n",
      "i: 19500 , cost :  0.420116 , training accuracy :  0.818182\n",
      "i: 19600 , cost :  0.408637 , training accuracy :  0.806958\n",
      "i: 19700 , cost :  0.40529 , training accuracy :  0.822671\n",
      "i: 19800 , cost :  0.418918 , training accuracy :  0.794613\n",
      "i: 19900 , cost :  0.420508 , training accuracy :  0.817059\n",
      "i: 20000 , cost :  0.414521 , training accuracy :  0.803591\n",
      "i: 20100 , cost :  0.414414 , training accuracy :  0.826038\n",
      "i: 20200 , cost :  0.407221 , training accuracy :  0.824916\n",
      "i: 20300 , cost :  0.412963 , training accuracy :  0.800224\n",
      "i: 20400 , cost :  0.405053 , training accuracy :  0.822671\n",
      "i: 20500 , cost :  0.405462 , training accuracy :  0.815937\n",
      "i: 20600 , cost :  0.40933 , training accuracy :  0.805836\n",
      "i: 20700 , cost :  0.413602 , training accuracy :  0.830527\n",
      "i: 20800 , cost :  0.407466 , training accuracy :  0.81257\n",
      "i: 20900 , cost :  0.40666 , training accuracy :  0.81257\n",
      "i: 21000 , cost :  0.414164 , training accuracy :  0.826038\n",
      "i: 21100 , cost :  0.413213 , training accuracy :  0.801347\n",
      "i: 21200 , cost :  0.415774 , training accuracy :  0.822671\n",
      "i: 21300 , cost :  0.4108 , training accuracy :  0.802469\n",
      "i: 21400 , cost :  0.408808 , training accuracy :  0.82716\n",
      "i: 21500 , cost :  0.405896 , training accuracy :  0.811448\n",
      "i: 21600 , cost :  0.420478 , training accuracy :  0.799102\n",
      "i: 21700 , cost :  0.412573 , training accuracy :  0.801347\n",
      "i: 21800 , cost :  0.404566 , training accuracy :  0.819304\n",
      "i: 21900 , cost :  0.404827 , training accuracy :  0.822671\n",
      "i: 22000 , cost :  0.409153 , training accuracy :  0.804714\n",
      "i: 22100 , cost :  0.4065 , training accuracy :  0.824916\n",
      "i: 22200 , cost :  0.425255 , training accuracy :  0.796857\n",
      "i: 22300 , cost :  0.41034 , training accuracy :  0.804714\n",
      "i: 22400 , cost :  0.419159 , training accuracy :  0.818182\n",
      "i: 22500 , cost :  0.404787 , training accuracy :  0.814815\n",
      "i: 22600 , cost :  0.412055 , training accuracy :  0.803591\n",
      "i: 22700 , cost :  0.413049 , training accuracy :  0.830527\n",
      "i: 22800 , cost :  0.411869 , training accuracy :  0.802469\n",
      "i: 22900 , cost :  0.419346 , training accuracy :  0.799102\n",
      "i: 23000 , cost :  0.404466 , training accuracy :  0.819304\n",
      "i: 23100 , cost :  0.418484 , training accuracy :  0.799102\n",
      "i: 23200 , cost :  0.411808 , training accuracy :  0.82716\n",
      "i: 23300 , cost :  0.404303 , training accuracy :  0.817059\n",
      "i: 23400 , cost :  0.419132 , training accuracy :  0.818182\n",
      "i: 23500 , cost :  0.416081 , training accuracy :  0.794613\n",
      "i: 23600 , cost :  0.408252 , training accuracy :  0.826038\n",
      "i: 23700 , cost :  0.406441 , training accuracy :  0.823793\n",
      "i: 23800 , cost :  0.418511 , training accuracy :  0.819304\n",
      "i: 23900 , cost :  0.406543 , training accuracy :  0.823793\n",
      "i: 24000 , cost :  0.411406 , training accuracy :  0.803591\n",
      "i: 24100 , cost :  0.413805 , training accuracy :  0.82716\n",
      "i: 24200 , cost :  0.419611 , training accuracy :  0.801347\n",
      "i: 24300 , cost :  0.419141 , training accuracy :  0.819304\n",
      "i: 24400 , cost :  0.404864 , training accuracy :  0.826038\n",
      "i: 24500 , cost :  0.410381 , training accuracy :  0.826038\n",
      "i: 24600 , cost :  0.404684 , training accuracy :  0.813693\n",
      "i: 24700 , cost :  0.432664 , training accuracy :  0.79349\n",
      "i: 24800 , cost :  0.404136 , training accuracy :  0.817059\n",
      "i: 24900 , cost :  0.405495 , training accuracy :  0.823793\n",
      "i: 25000 , cost :  0.404241 , training accuracy :  0.817059\n",
      "i: 25100 , cost :  0.420777 , training accuracy :  0.818182\n",
      "i: 25200 , cost :  0.404438 , training accuracy :  0.817059\n",
      "i: 25300 , cost :  0.410297 , training accuracy :  0.803591\n",
      "i: 25400 , cost :  0.416599 , training accuracy :  0.820426\n",
      "i: 25500 , cost :  0.407611 , training accuracy :  0.808081\n",
      "i: 25600 , cost :  0.407202 , training accuracy :  0.824916\n",
      "i: 25700 , cost :  0.404658 , training accuracy :  0.815937\n",
      "i: 25800 , cost :  0.412077 , training accuracy :  0.829405\n",
      "i: 25900 , cost :  0.40519 , training accuracy :  0.81257\n",
      "i: 26000 , cost :  0.411616 , training accuracy :  0.803591\n",
      "i: 26100 , cost :  0.404473 , training accuracy :  0.818182\n",
      "i: 26200 , cost :  0.404031 , training accuracy :  0.817059\n",
      "i: 26300 , cost :  0.4176 , training accuracy :  0.795735\n",
      "i: 26400 , cost :  0.403952 , training accuracy :  0.817059\n",
      "i: 26500 , cost :  0.404047 , training accuracy :  0.819304\n",
      "i: 26600 , cost :  0.40867 , training accuracy :  0.824916\n",
      "i: 26700 , cost :  0.411028 , training accuracy :  0.804714\n",
      "i: 26800 , cost :  0.403823 , training accuracy :  0.818182\n",
      "i: 26900 , cost :  0.40755 , training accuracy :  0.824916\n",
      "i: 27000 , cost :  0.413625 , training accuracy :  0.808081\n",
      "i: 27100 , cost :  0.413222 , training accuracy :  0.81257\n",
      "i: 27200 , cost :  0.414615 , training accuracy :  0.817059\n",
      "i: 27300 , cost :  0.413406 , training accuracy :  0.808081\n",
      "i: 27400 , cost :  0.437265 , training accuracy :  0.804714\n",
      "i: 27500 , cost :  0.41323 , training accuracy :  0.817059\n",
      "i: 27600 , cost :  0.412438 , training accuracy :  0.813693\n",
      "i: 27700 , cost :  0.413025 , training accuracy :  0.817059\n",
      "i: 27800 , cost :  0.413745 , training accuracy :  0.818182\n",
      "i: 27900 , cost :  0.435988 , training accuracy :  0.79798\n",
      "i: 28000 , cost :  0.412127 , training accuracy :  0.813693\n",
      "i: 28100 , cost :  0.432977 , training accuracy :  0.810326\n",
      "i: 28200 , cost :  0.417362 , training accuracy :  0.821549\n",
      "i: 28300 , cost :  0.420101 , training accuracy :  0.796857\n",
      "i: 28400 , cost :  0.412625 , training accuracy :  0.810326\n",
      "i: 28500 , cost :  0.411873 , training accuracy :  0.813693\n",
      "i: 28600 , cost :  0.417017 , training accuracy :  0.802469\n",
      "i: 28700 , cost :  0.421568 , training accuracy :  0.815937\n",
      "i: 28800 , cost :  0.413854 , training accuracy :  0.819304\n",
      "i: 28900 , cost :  0.411645 , training accuracy :  0.81257\n",
      "i: 29000 , cost :  0.412009 , training accuracy :  0.811448\n",
      "i: 29100 , cost :  0.426278 , training accuracy :  0.810326\n",
      "i: 29200 , cost :  0.416601 , training accuracy :  0.822671\n",
      "i: 29300 , cost :  0.418855 , training accuracy :  0.826038\n",
      "i: 29400 , cost :  0.417964 , training accuracy :  0.824916\n",
      "i: 29500 , cost :  0.415215 , training accuracy :  0.806958\n",
      "i: 29600 , cost :  0.411632 , training accuracy :  0.813693\n",
      "i: 29700 , cost :  0.412235 , training accuracy :  0.809203\n",
      "i: 29800 , cost :  0.419187 , training accuracy :  0.824916\n",
      "i: 29900 , cost :  0.41135 , training accuracy :  0.813693\n",
      "Final training accuracy :  0.794613\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 30000 ][hidden: 10 ][file: bhavul_tr_acc_0.79_prediction.csv ] ACCURACY :  0.794613\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  247.314 , training accuracy :  0.415264\n",
      "i: 100 , cost :  2.66806 , training accuracy :  0.704826\n",
      "i: 200 , cost :  1.42729 , training accuracy :  0.73064\n",
      "i: 300 , cost :  0.870054 , training accuracy :  0.775533\n",
      "i: 400 , cost :  2.9764 , training accuracy :  0.699214\n",
      "i: 500 , cost :  1.87469 , training accuracy :  0.741863\n",
      "i: 600 , cost :  0.607475 , training accuracy :  0.808081\n",
      "i: 700 , cost :  2.3047 , training accuracy :  0.667789\n",
      "i: 800 , cost :  0.617098 , training accuracy :  0.813693\n",
      "i: 900 , cost :  1.49445 , training accuracy :  0.619529\n",
      "i: 1000 , cost :  0.702186 , training accuracy :  0.808081\n",
      "i: 1100 , cost :  1.64207 , training accuracy :  0.561167\n",
      "i: 1200 , cost :  2.41188 , training accuracy :  0.674523\n",
      "i: 1300 , cost :  0.911386 , training accuracy :  0.728395\n",
      "i: 1400 , cost :  0.609939 , training accuracy :  0.795735\n",
      "i: 1500 , cost :  0.63387 , training accuracy :  0.810326\n",
      "i: 1600 , cost :  2.57609 , training accuracy :  0.406285\n",
      "i: 1700 , cost :  1.04508 , training accuracy :  0.771044\n",
      "i: 1800 , cost :  0.943231 , training accuracy :  0.79349\n",
      "i: 1900 , cost :  1.11424 , training accuracy :  0.772166\n",
      "i: 2000 , cost :  0.90481 , training accuracy :  0.79798\n",
      "i: 2100 , cost :  0.916729 , training accuracy :  0.794613\n",
      "i: 2200 , cost :  0.896221 , training accuracy :  0.794613\n",
      "i: 2300 , cost :  0.882024 , training accuracy :  0.79798\n",
      "i: 2400 , cost :  0.869325 , training accuracy :  0.800224\n",
      "i: 2500 , cost :  0.858834 , training accuracy :  0.79798\n",
      "i: 2600 , cost :  0.846787 , training accuracy :  0.800224\n",
      "i: 2700 , cost :  0.843338 , training accuracy :  0.799102\n",
      "i: 2800 , cost :  0.812636 , training accuracy :  0.802469\n",
      "i: 2900 , cost :  0.968152 , training accuracy :  0.782267\n",
      "i: 3000 , cost :  0.573079 , training accuracy :  0.809203\n",
      "i: 3100 , cost :  0.465624 , training accuracy :  0.819304\n",
      "i: 3200 , cost :  0.5595 , training accuracy :  0.801347\n",
      "i: 3300 , cost :  0.456955 , training accuracy :  0.806958\n",
      "i: 3400 , cost :  0.547782 , training accuracy :  0.7789\n",
      "i: 3500 , cost :  0.524016 , training accuracy :  0.7789\n",
      "i: 3600 , cost :  1.97296 , training accuracy :  0.514029\n",
      "i: 3700 , cost :  2.07738 , training accuracy :  0.52413\n",
      "i: 3800 , cost :  1.49856 , training accuracy :  0.608305\n",
      "i: 3900 , cost :  1.49798 , training accuracy :  0.607183\n",
      "i: 4000 , cost :  1.44109 , training accuracy :  0.615039\n",
      "i: 4100 , cost :  1.35919 , training accuracy :  0.634119\n",
      "i: 4200 , cost :  1.4354 , training accuracy :  0.611672\n",
      "i: 4300 , cost :  0.499615 , training accuracy :  0.821549\n",
      "i: 4400 , cost :  0.450539 , training accuracy :  0.821549\n",
      "i: 4500 , cost :  0.481659 , training accuracy :  0.810326\n",
      "i: 4600 , cost :  1.05955 , training accuracy :  0.686869\n",
      "i: 4700 , cost :  0.443852 , training accuracy :  0.821549\n",
      "i: 4800 , cost :  0.914145 , training accuracy :  0.76431\n",
      "i: 4900 , cost :  0.475216 , training accuracy :  0.817059\n",
      "i: 5000 , cost :  0.470984 , training accuracy :  0.801347\n",
      "i: 5100 , cost :  0.688066 , training accuracy :  0.747475\n",
      "i: 5200 , cost :  0.434064 , training accuracy :  0.826038\n",
      "i: 5300 , cost :  2.58366 , training accuracy :  0.416386\n",
      "i: 5400 , cost :  0.44097 , training accuracy :  0.822671\n",
      "i: 5500 , cost :  0.686012 , training accuracy :  0.769921\n",
      "i: 5600 , cost :  0.437641 , training accuracy :  0.822671\n",
      "i: 5700 , cost :  1.5169 , training accuracy :  0.593715\n",
      "i: 5800 , cost :  1.62383 , training accuracy :  0.57688\n",
      "i: 5900 , cost :  1.33713 , training accuracy :  0.62514\n",
      "i: 6000 , cost :  1.27035 , training accuracy :  0.636364\n",
      "i: 6100 , cost :  1.22895 , training accuracy :  0.637486\n",
      "i: 6200 , cost :  1.17643 , training accuracy :  0.645342\n",
      "i: 6300 , cost :  1.10985 , training accuracy :  0.654321\n",
      "i: 6400 , cost :  0.986884 , training accuracy :  0.684624\n",
      "i: 6500 , cost :  0.609407 , training accuracy :  0.781145\n",
      "i: 6600 , cost :  0.444412 , training accuracy :  0.818182\n",
      "i: 6700 , cost :  0.429908 , training accuracy :  0.821549\n",
      "i: 6800 , cost :  0.434075 , training accuracy :  0.811448\n",
      "i: 6900 , cost :  1.01245 , training accuracy :  0.622896\n",
      "i: 7000 , cost :  0.759825 , training accuracy :  0.791246\n",
      "i: 7100 , cost :  0.813445 , training accuracy :  0.783389\n",
      "i: 7200 , cost :  0.695085 , training accuracy :  0.794613\n",
      "i: 7300 , cost :  0.801343 , training accuracy :  0.777778\n",
      "i: 7400 , cost :  0.671135 , training accuracy :  0.79798\n",
      "i: 7500 , cost :  0.622656 , training accuracy :  0.803591\n",
      "i: 7600 , cost :  0.684926 , training accuracy :  0.792368\n",
      "i: 7700 , cost :  0.6149 , training accuracy :  0.802469\n",
      "i: 7800 , cost :  0.642225 , training accuracy :  0.796857\n",
      "i: 7900 , cost :  0.638501 , training accuracy :  0.796857\n",
      "i: 8000 , cost :  0.485642 , training accuracy :  0.822671\n",
      "i: 8100 , cost :  0.429918 , training accuracy :  0.823793\n",
      "i: 8200 , cost :  0.421319 , training accuracy :  0.820426\n",
      "i: 8300 , cost :  0.416837 , training accuracy :  0.819304\n",
      "i: 8400 , cost :  0.47376 , training accuracy :  0.802469\n",
      "i: 8500 , cost :  0.481892 , training accuracy :  0.782267\n",
      "i: 8600 , cost :  0.462613 , training accuracy :  0.813693\n",
      "i: 8700 , cost :  0.426762 , training accuracy :  0.821549\n",
      "i: 8800 , cost :  0.423795 , training accuracy :  0.808081\n",
      "i: 8900 , cost :  0.451923 , training accuracy :  0.794613\n",
      "i: 9000 , cost :  0.47425 , training accuracy :  0.782267\n",
      "i: 9100 , cost :  0.487408 , training accuracy :  0.775533\n",
      "i: 9200 , cost :  0.489085 , training accuracy :  0.776655\n",
      "i: 9300 , cost :  0.493064 , training accuracy :  0.780022\n",
      "i: 9400 , cost :  0.496261 , training accuracy :  0.7789\n",
      "i: 9500 , cost :  0.489385 , training accuracy :  0.782267\n",
      "i: 9600 , cost :  0.467271 , training accuracy :  0.790123\n",
      "i: 9700 , cost :  0.453887 , training accuracy :  0.79798\n",
      "i: 9800 , cost :  0.454114 , training accuracy :  0.79798\n",
      "i: 9900 , cost :  0.456402 , training accuracy :  0.794613\n",
      "i: 10000 , cost :  0.462308 , training accuracy :  0.794613\n",
      "i: 10100 , cost :  0.486114 , training accuracy :  0.786756\n",
      "i: 10200 , cost :  0.536571 , training accuracy :  0.75982\n",
      "i: 10300 , cost :  0.52869 , training accuracy :  0.76431\n",
      "i: 10400 , cost :  0.435222 , training accuracy :  0.817059\n",
      "i: 10500 , cost :  0.566197 , training accuracy :  0.781145\n",
      "i: 10600 , cost :  0.535339 , training accuracy :  0.758698\n",
      "i: 10700 , cost :  0.626531 , training accuracy :  0.76431\n",
      "i: 10800 , cost :  0.505612 , training accuracy :  0.783389\n",
      "i: 10900 , cost :  0.475693 , training accuracy :  0.806958\n",
      "i: 11000 , cost :  0.426074 , training accuracy :  0.814815\n",
      "i: 11100 , cost :  0.43715 , training accuracy :  0.81257\n",
      "i: 11200 , cost :  0.448935 , training accuracy :  0.819304\n",
      "i: 11300 , cost :  0.493142 , training accuracy :  0.780022\n",
      "i: 11400 , cost :  0.55135 , training accuracy :  0.785634\n",
      "i: 11500 , cost :  0.527739 , training accuracy :  0.758698\n",
      "i: 11600 , cost :  0.632531 , training accuracy :  0.758698\n",
      "i: 11700 , cost :  0.505558 , training accuracy :  0.777778\n",
      "i: 11800 , cost :  0.487009 , training accuracy :  0.801347\n",
      "i: 11900 , cost :  0.42003 , training accuracy :  0.817059\n",
      "i: 12000 , cost :  0.420895 , training accuracy :  0.811448\n",
      "i: 12100 , cost :  0.466645 , training accuracy :  0.809203\n",
      "i: 12200 , cost :  0.492529 , training accuracy :  0.782267\n",
      "i: 12300 , cost :  0.531732 , training accuracy :  0.792368\n",
      "i: 12400 , cost :  0.51367 , training accuracy :  0.766554\n",
      "i: 12500 , cost :  0.623064 , training accuracy :  0.763187\n",
      "i: 12600 , cost :  0.48196 , training accuracy :  0.783389\n",
      "i: 12700 , cost :  0.474257 , training accuracy :  0.802469\n",
      "i: 12800 , cost :  0.422314 , training accuracy :  0.814815\n",
      "i: 12900 , cost :  0.417733 , training accuracy :  0.819304\n",
      "i: 13000 , cost :  0.48797 , training accuracy :  0.801347\n",
      "i: 13100 , cost :  0.497726 , training accuracy :  0.781145\n",
      "i: 13200 , cost :  0.508915 , training accuracy :  0.796857\n",
      "i: 13300 , cost :  0.473174 , training accuracy :  0.808081\n",
      "i: 13400 , cost :  0.603874 , training accuracy :  0.789001\n",
      "i: 13500 , cost :  0.440264 , training accuracy :  0.800224\n",
      "i: 13600 , cost :  0.428201 , training accuracy :  0.813693\n",
      "i: 13700 , cost :  0.447869 , training accuracy :  0.795735\n",
      "i: 13800 , cost :  0.49973 , training accuracy :  0.766554\n",
      "i: 13900 , cost :  0.409973 , training accuracy :  0.814815\n",
      "i: 14000 , cost :  0.555975 , training accuracy :  0.782267\n",
      "i: 14100 , cost :  0.429982 , training accuracy :  0.810326\n",
      "i: 14200 , cost :  0.502119 , training accuracy :  0.765432\n",
      "i: 14300 , cost :  0.503486 , training accuracy :  0.76431\n",
      "i: 14400 , cost :  0.437148 , training accuracy :  0.802469\n",
      "i: 14500 , cost :  0.510761 , training accuracy :  0.800224\n",
      "i: 14600 , cost :  0.46365 , training accuracy :  0.810326\n",
      "i: 14700 , cost :  0.450191 , training accuracy :  0.794613\n",
      "i: 14800 , cost :  0.466056 , training accuracy :  0.79349\n",
      "i: 14900 , cost :  0.409263 , training accuracy :  0.818182\n",
      "i: 15000 , cost :  0.552405 , training accuracy :  0.775533\n",
      "i: 15100 , cost :  0.43121 , training accuracy :  0.814815\n",
      "i: 15200 , cost :  0.441122 , training accuracy :  0.804714\n",
      "i: 15300 , cost :  0.488067 , training accuracy :  0.783389\n",
      "i: 15400 , cost :  0.43828 , training accuracy :  0.803591\n",
      "i: 15500 , cost :  0.395998 , training accuracy :  0.828283\n",
      "i: 15600 , cost :  0.541168 , training accuracy :  0.792368\n",
      "i: 15700 , cost :  0.433795 , training accuracy :  0.806958\n",
      "i: 15800 , cost :  0.517483 , training accuracy :  0.800224\n",
      "i: 15900 , cost :  0.473545 , training accuracy :  0.792368\n",
      "i: 16000 , cost :  0.411576 , training accuracy :  0.811448\n",
      "i: 16100 , cost :  0.469857 , training accuracy :  0.809203\n",
      "i: 16200 , cost :  0.400989 , training accuracy :  0.821549\n",
      "i: 16300 , cost :  0.471283 , training accuracy :  0.79798\n",
      "i: 16400 , cost :  0.413299 , training accuracy :  0.823793\n",
      "i: 16500 , cost :  0.47996 , training accuracy :  0.806958\n",
      "i: 16600 , cost :  0.45555 , training accuracy :  0.804714\n",
      "i: 16700 , cost :  0.394772 , training accuracy :  0.826038\n",
      "i: 16800 , cost :  0.392618 , training accuracy :  0.826038\n",
      "i: 16900 , cost :  0.539925 , training accuracy :  0.79349\n",
      "i: 17000 , cost :  0.45032 , training accuracy :  0.811448\n",
      "i: 17100 , cost :  0.389855 , training accuracy :  0.821549\n",
      "i: 17200 , cost :  0.420142 , training accuracy :  0.811448\n",
      "i: 17300 , cost :  0.418467 , training accuracy :  0.814815\n",
      "i: 17400 , cost :  0.389446 , training accuracy :  0.832772\n",
      "i: 17500 , cost :  0.473069 , training accuracy :  0.790123\n",
      "i: 17600 , cost :  0.445538 , training accuracy :  0.809203\n",
      "i: 17700 , cost :  0.435159 , training accuracy :  0.806958\n",
      "i: 17800 , cost :  0.452305 , training accuracy :  0.806958\n",
      "i: 17900 , cost :  0.388071 , training accuracy :  0.830527\n",
      "i: 18000 , cost :  0.419338 , training accuracy :  0.813693\n",
      "i: 18100 , cost :  0.429344 , training accuracy :  0.821549\n",
      "i: 18200 , cost :  0.409318 , training accuracy :  0.815937\n",
      "i: 18300 , cost :  0.471916 , training accuracy :  0.803591\n",
      "i: 18400 , cost :  0.392661 , training accuracy :  0.821549\n",
      "i: 18500 , cost :  0.394679 , training accuracy :  0.829405\n",
      "i: 18600 , cost :  0.412631 , training accuracy :  0.832772\n",
      "i: 18700 , cost :  0.409076 , training accuracy :  0.828283\n",
      "i: 18800 , cost :  0.393069 , training accuracy :  0.830527\n",
      "i: 18900 , cost :  0.411077 , training accuracy :  0.83165\n",
      "i: 19000 , cost :  0.380246 , training accuracy :  0.837261\n",
      "i: 19100 , cost :  0.449059 , training accuracy :  0.810326\n",
      "i: 19200 , cost :  0.418874 , training accuracy :  0.821549\n",
      "i: 19300 , cost :  0.383513 , training accuracy :  0.833894\n",
      "i: 19400 , cost :  0.431595 , training accuracy :  0.811448\n",
      "i: 19500 , cost :  0.463191 , training accuracy :  0.811448\n",
      "i: 19600 , cost :  0.426735 , training accuracy :  0.815937\n",
      "i: 19700 , cost :  0.412914 , training accuracy :  0.826038\n",
      "i: 19800 , cost :  0.417316 , training accuracy :  0.835017\n",
      "i: 19900 , cost :  0.385252 , training accuracy :  0.835017\n",
      "i: 20000 , cost :  0.435684 , training accuracy :  0.81257\n",
      "i: 20100 , cost :  0.423077 , training accuracy :  0.818182\n",
      "i: 20200 , cost :  0.414554 , training accuracy :  0.833894\n",
      "i: 20300 , cost :  0.458474 , training accuracy :  0.81257\n",
      "i: 20400 , cost :  0.386598 , training accuracy :  0.833894\n",
      "i: 20500 , cost :  0.434219 , training accuracy :  0.814815\n",
      "i: 20600 , cost :  0.396118 , training accuracy :  0.814815\n",
      "i: 20700 , cost :  0.418827 , training accuracy :  0.824916\n",
      "i: 20800 , cost :  0.406151 , training accuracy :  0.817059\n",
      "i: 20900 , cost :  0.381371 , training accuracy :  0.835017\n",
      "i: 21000 , cost :  0.378515 , training accuracy :  0.835017\n",
      "i: 21100 , cost :  0.385349 , training accuracy :  0.826038\n",
      "i: 21200 , cost :  0.390569 , training accuracy :  0.836139\n",
      "i: 21300 , cost :  0.406547 , training accuracy :  0.832772\n",
      "i: 21400 , cost :  0.416603 , training accuracy :  0.817059\n",
      "i: 21500 , cost :  0.436893 , training accuracy :  0.811448\n",
      "i: 21600 , cost :  0.425095 , training accuracy :  0.818182\n",
      "i: 21700 , cost :  0.380043 , training accuracy :  0.836139\n",
      "i: 21800 , cost :  0.381009 , training accuracy :  0.840629\n",
      "i: 21900 , cost :  0.377419 , training accuracy :  0.83165\n",
      "i: 22000 , cost :  0.408231 , training accuracy :  0.828283\n",
      "i: 22100 , cost :  0.418688 , training accuracy :  0.818182\n",
      "i: 22200 , cost :  0.410215 , training accuracy :  0.830527\n",
      "i: 22300 , cost :  0.417113 , training accuracy :  0.814815\n",
      "i: 22400 , cost :  0.407311 , training accuracy :  0.835017\n",
      "i: 22500 , cost :  0.379514 , training accuracy :  0.823793\n",
      "i: 22600 , cost :  0.427488 , training accuracy :  0.817059\n",
      "i: 22700 , cost :  0.409839 , training accuracy :  0.830527\n",
      "i: 22800 , cost :  0.378112 , training accuracy :  0.82716\n",
      "i: 22900 , cost :  0.379849 , training accuracy :  0.836139\n",
      "i: 23000 , cost :  0.40624 , training accuracy :  0.817059\n",
      "i: 23100 , cost :  0.378856 , training accuracy :  0.837261\n",
      "i: 23200 , cost :  0.409548 , training accuracy :  0.830527\n",
      "i: 23300 , cost :  0.377007 , training accuracy :  0.826038\n",
      "i: 23400 , cost :  0.422465 , training accuracy :  0.815937\n",
      "i: 23500 , cost :  0.410669 , training accuracy :  0.830527\n",
      "i: 23600 , cost :  0.407487 , training accuracy :  0.814815\n",
      "i: 23700 , cost :  0.376305 , training accuracy :  0.835017\n",
      "i: 23800 , cost :  0.378336 , training accuracy :  0.839506\n",
      "i: 23900 , cost :  0.376073 , training accuracy :  0.833894\n",
      "i: 24000 , cost :  0.374661 , training accuracy :  0.830527\n",
      "i: 24100 , cost :  0.378993 , training accuracy :  0.841751\n",
      "i: 24200 , cost :  0.375441 , training accuracy :  0.82716\n",
      "i: 24300 , cost :  0.398263 , training accuracy :  0.821549\n",
      "i: 24400 , cost :  0.392542 , training accuracy :  0.835017\n",
      "i: 24500 , cost :  0.377945 , training accuracy :  0.839506\n",
      "i: 24600 , cost :  0.379759 , training accuracy :  0.835017\n",
      "i: 24700 , cost :  0.386958 , training accuracy :  0.822671\n",
      "i: 24800 , cost :  0.402925 , training accuracy :  0.833894\n",
      "i: 24900 , cost :  0.414176 , training accuracy :  0.817059\n",
      "i: 25000 , cost :  0.418068 , training accuracy :  0.823793\n",
      "i: 25100 , cost :  0.396698 , training accuracy :  0.822671\n",
      "i: 25200 , cost :  0.386788 , training accuracy :  0.822671\n",
      "i: 25300 , cost :  0.376691 , training accuracy :  0.828283\n",
      "i: 25400 , cost :  0.375445 , training accuracy :  0.829405\n",
      "i: 25500 , cost :  0.374107 , training accuracy :  0.835017\n",
      "i: 25600 , cost :  0.403329 , training accuracy :  0.818182\n",
      "i: 25700 , cost :  0.378107 , training accuracy :  0.836139\n",
      "i: 25800 , cost :  0.385699 , training accuracy :  0.823793\n",
      "i: 25900 , cost :  0.376501 , training accuracy :  0.828283\n",
      "i: 26000 , cost :  0.383144 , training accuracy :  0.82716\n",
      "i: 26100 , cost :  0.383125 , training accuracy :  0.839506\n",
      "i: 26200 , cost :  0.385067 , training accuracy :  0.824916\n",
      "i: 26300 , cost :  0.400376 , training accuracy :  0.832772\n",
      "i: 26400 , cost :  0.387567 , training accuracy :  0.824916\n",
      "i: 26500 , cost :  0.403249 , training accuracy :  0.822671\n",
      "i: 26600 , cost :  0.370732 , training accuracy :  0.832772\n",
      "i: 26700 , cost :  0.370848 , training accuracy :  0.829405\n",
      "i: 26800 , cost :  0.369774 , training accuracy :  0.830527\n",
      "i: 26900 , cost :  0.369704 , training accuracy :  0.830527\n",
      "i: 27000 , cost :  0.383157 , training accuracy :  0.833894\n",
      "i: 27100 , cost :  0.380058 , training accuracy :  0.826038\n",
      "i: 27200 , cost :  0.373953 , training accuracy :  0.828283\n",
      "i: 27300 , cost :  0.371126 , training accuracy :  0.832772\n",
      "i: 27400 , cost :  0.36907 , training accuracy :  0.83165\n",
      "i: 27500 , cost :  0.371297 , training accuracy :  0.829405\n",
      "i: 27600 , cost :  0.372726 , training accuracy :  0.836139\n",
      "i: 27700 , cost :  0.377047 , training accuracy :  0.82716\n",
      "i: 27800 , cost :  0.370452 , training accuracy :  0.832772\n",
      "i: 27900 , cost :  0.38893 , training accuracy :  0.823793\n",
      "i: 28000 , cost :  0.372281 , training accuracy :  0.832772\n",
      "i: 28100 , cost :  0.368974 , training accuracy :  0.832772\n",
      "i: 28200 , cost :  0.401687 , training accuracy :  0.822671\n",
      "i: 28300 , cost :  0.381628 , training accuracy :  0.839506\n",
      "i: 28400 , cost :  0.369735 , training accuracy :  0.832772\n",
      "i: 28500 , cost :  0.369357 , training accuracy :  0.83165\n",
      "i: 28600 , cost :  0.369649 , training accuracy :  0.832772\n",
      "i: 28700 , cost :  0.375081 , training accuracy :  0.838384\n",
      "i: 28800 , cost :  0.372949 , training accuracy :  0.836139\n",
      "i: 28900 , cost :  0.383634 , training accuracy :  0.828283\n",
      "i: 29000 , cost :  0.383172 , training accuracy :  0.836139\n",
      "i: 29100 , cost :  0.38113 , training accuracy :  0.837261\n",
      "i: 29200 , cost :  0.388003 , training accuracy :  0.824916\n",
      "i: 29300 , cost :  0.369435 , training accuracy :  0.832772\n",
      "i: 29400 , cost :  0.371373 , training accuracy :  0.830527\n",
      "i: 29500 , cost :  0.368772 , training accuracy :  0.832772\n",
      "i: 29600 , cost :  0.369924 , training accuracy :  0.83165\n",
      "i: 29700 , cost :  0.369968 , training accuracy :  0.832772\n",
      "i: 29800 , cost :  0.368284 , training accuracy :  0.83165\n",
      "i: 29900 , cost :  0.38287 , training accuracy :  0.836139\n",
      "Final training accuracy :  0.83165\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 30000 ][hidden: 15 ][file: bhavul_tr_acc_0.83_prediction.csv ] ACCURACY :  0.83165\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1197.03 , training accuracy :  0.383838\n",
      "i: 100 , cost :  5.39269 , training accuracy :  0.72615\n",
      "i: 200 , cost :  8.11724 , training accuracy :  0.713805\n",
      "i: 300 , cost :  6.25092 , training accuracy :  0.631874\n",
      "i: 400 , cost :  2.25483 , training accuracy :  0.725028\n",
      "i: 500 , cost :  1.694 , training accuracy :  0.794613\n",
      "i: 600 , cost :  1.88118 , training accuracy :  0.796857\n",
      "i: 700 , cost :  7.99087 , training accuracy :  0.680135\n",
      "i: 800 , cost :  4.42688 , training accuracy :  0.768799\n",
      "i: 900 , cost :  5.29176 , training accuracy :  0.760943\n",
      "i: 1000 , cost :  2.01245 , training accuracy :  0.802469\n",
      "i: 1100 , cost :  27.9475 , training accuracy :  0.704826\n",
      "i: 1200 , cost :  1.42753 , training accuracy :  0.813693\n",
      "i: 1300 , cost :  3.24367 , training accuracy :  0.755331\n",
      "i: 1400 , cost :  1.28955 , training accuracy :  0.823793\n",
      "i: 1500 , cost :  5.73485 , training accuracy :  0.632997\n",
      "i: 1600 , cost :  1.23967 , training accuracy :  0.830527\n",
      "i: 1700 , cost :  5.50828 , training accuracy :  0.567901\n",
      "i: 1800 , cost :  1.25236 , training accuracy :  0.830527\n",
      "i: 1900 , cost :  2.82134 , training accuracy :  0.723906\n",
      "i: 2000 , cost :  2.07505 , training accuracy :  0.818182\n",
      "i: 2100 , cost :  6.17896 , training accuracy :  0.758698\n",
      "i: 2200 , cost :  1.88294 , training accuracy :  0.796857\n",
      "i: 2300 , cost :  2.10064 , training accuracy :  0.802469\n",
      "i: 2400 , cost :  4.35889 , training accuracy :  0.768799\n",
      "i: 2500 , cost :  1.59875 , training accuracy :  0.814815\n",
      "i: 2600 , cost :  10.0131 , training accuracy :  0.578002\n",
      "i: 2700 , cost :  1.20854 , training accuracy :  0.838384\n",
      "i: 2800 , cost :  3.50513 , training accuracy :  0.71156\n",
      "i: 2900 , cost :  7.9884 , training accuracy :  0.744108\n",
      "i: 3000 , cost :  1.14382 , training accuracy :  0.835017\n",
      "i: 3100 , cost :  1.52291 , training accuracy :  0.820426\n",
      "i: 3200 , cost :  1.38022 , training accuracy :  0.836139\n",
      "i: 3300 , cost :  4.74897 , training accuracy :  0.735129\n",
      "i: 3400 , cost :  1.19904 , training accuracy :  0.832772\n",
      "i: 3500 , cost :  8.07013 , training accuracy :  0.712682\n",
      "i: 3600 , cost :  1.01501 , training accuracy :  0.845118\n",
      "i: 3700 , cost :  2.38873 , training accuracy :  0.795735\n",
      "i: 3800 , cost :  7.85643 , training accuracy :  0.735129\n",
      "i: 3900 , cost :  1.0365 , training accuracy :  0.828283\n",
      "i: 4000 , cost :  0.995272 , training accuracy :  0.845118\n",
      "i: 4100 , cost :  2.2179 , training accuracy :  0.804714\n",
      "i: 4200 , cost :  0.991062 , training accuracy :  0.847363\n",
      "i: 4300 , cost :  3.60289 , training accuracy :  0.784512\n",
      "i: 4400 , cost :  0.951016 , training accuracy :  0.841751\n",
      "i: 4500 , cost :  1.51469 , training accuracy :  0.836139\n",
      "i: 4600 , cost :  3.09059 , training accuracy :  0.674523\n",
      "i: 4700 , cost :  1.66952 , training accuracy :  0.794613\n",
      "i: 4800 , cost :  1.49023 , training accuracy :  0.840629\n",
      "i: 4900 , cost :  8.85857 , training accuracy :  0.729517\n",
      "i: 5000 , cost :  1.88495 , training accuracy :  0.813693\n",
      "i: 5100 , cost :  21.2373 , training accuracy :  0.393939\n",
      "i: 5200 , cost :  1.67985 , training accuracy :  0.803591\n",
      "i: 5300 , cost :  8.04308 , training accuracy :  0.689113\n",
      "i: 5400 , cost :  2.49047 , training accuracy :  0.795735\n",
      "i: 5500 , cost :  0.823924 , training accuracy :  0.856341\n",
      "i: 5600 , cost :  1.55371 , training accuracy :  0.822671\n",
      "i: 5700 , cost :  0.96128 , training accuracy :  0.856341\n",
      "i: 5800 , cost :  3.03822 , training accuracy :  0.75982\n",
      "i: 5900 , cost :  1.08121 , training accuracy :  0.851852\n",
      "i: 6000 , cost :  9.98644 , training accuracy :  0.652076\n",
      "i: 6100 , cost :  0.979843 , training accuracy :  0.845118\n",
      "i: 6200 , cost :  3.04292 , training accuracy :  0.700337\n",
      "i: 6300 , cost :  1.50225 , training accuracy :  0.829405\n",
      "i: 6400 , cost :  0.79722 , training accuracy :  0.845118\n",
      "i: 6500 , cost :  0.896093 , training accuracy :  0.849607\n",
      "i: 6600 , cost :  4.19416 , training accuracy :  0.580247\n",
      "i: 6700 , cost :  12.1566 , training accuracy :  0.676768\n",
      "i: 6800 , cost :  3.75349 , training accuracy :  0.698092\n",
      "i: 6900 , cost :  0.888588 , training accuracy :  0.866442\n",
      "i: 7000 , cost :  3.68342 , training accuracy :  0.789001\n",
      "i: 7100 , cost :  0.92904 , training accuracy :  0.861953\n",
      "i: 7200 , cost :  6.84153 , training accuracy :  0.710438\n",
      "i: 7300 , cost :  0.936393 , training accuracy :  0.857464\n",
      "i: 7400 , cost :  7.42514 , training accuracy :  0.729517\n",
      "i: 7500 , cost :  1.02961 , training accuracy :  0.85073\n",
      "i: 7600 , cost :  9.44278 , training accuracy :  0.419753\n",
      "i: 7700 , cost :  4.81296 , training accuracy :  0.728395\n",
      "i: 7800 , cost :  0.922983 , training accuracy :  0.863075\n",
      "i: 7900 , cost :  0.829062 , training accuracy :  0.84624\n",
      "i: 8000 , cost :  3.22107 , training accuracy :  0.729517\n",
      "i: 8100 , cost :  0.710533 , training accuracy :  0.863075\n",
      "i: 8200 , cost :  3.90712 , training accuracy :  0.530864\n",
      "i: 8300 , cost :  1.29859 , training accuracy :  0.740741\n",
      "i: 8400 , cost :  2.87366 , training accuracy :  0.787879\n",
      "i: 8500 , cost :  0.797762 , training accuracy :  0.864198\n",
      "i: 8600 , cost :  1.50832 , training accuracy :  0.821549\n",
      "i: 8700 , cost :  1.06312 , training accuracy :  0.856341\n",
      "i: 8800 , cost :  3.11349 , training accuracy :  0.75982\n",
      "i: 8900 , cost :  0.950852 , training accuracy :  0.859708\n",
      "i: 9000 , cost :  5.99793 , training accuracy :  0.755331\n",
      "i: 9100 , cost :  0.912048 , training accuracy :  0.857464\n",
      "i: 9200 , cost :  2.5278 , training accuracy :  0.801347\n",
      "i: 9300 , cost :  1.16371 , training accuracy :  0.839506\n",
      "i: 9400 , cost :  0.673021 , training accuracy :  0.856341\n",
      "i: 9500 , cost :  0.976356 , training accuracy :  0.843996\n",
      "i: 9600 , cost :  5.68395 , training accuracy :  0.72615\n",
      "i: 9700 , cost :  0.969318 , training accuracy :  0.852974\n",
      "i: 9800 , cost :  3.3707 , training accuracy :  0.808081\n",
      "i: 9900 , cost :  0.659496 , training accuracy :  0.861953\n",
      "i: 10000 , cost :  5.3072 , training accuracy :  0.744108\n",
      "i: 10100 , cost :  2.64304 , training accuracy :  0.791246\n",
      "i: 10200 , cost :  1.10702 , training accuracy :  0.842873\n",
      "i: 10300 , cost :  2.82029 , training accuracy :  0.624018\n",
      "i: 10400 , cost :  0.724906 , training accuracy :  0.864198\n",
      "i: 10500 , cost :  1.16791 , training accuracy :  0.845118\n",
      "i: 10600 , cost :  0.660712 , training accuracy :  0.86532\n",
      "i: 10700 , cost :  1.69122 , training accuracy :  0.828283\n",
      "i: 10800 , cost :  0.70844 , training accuracy :  0.860831\n",
      "i: 10900 , cost :  1.27091 , training accuracy :  0.82716\n",
      "i: 11000 , cost :  1.49129 , training accuracy :  0.789001\n",
      "i: 11100 , cost :  0.605207 , training accuracy :  0.860831\n",
      "i: 11200 , cost :  5.77883 , training accuracy :  0.491582\n",
      "i: 11300 , cost :  0.651524 , training accuracy :  0.866442\n",
      "i: 11400 , cost :  2.05922 , training accuracy :  0.810326\n",
      "i: 11500 , cost :  0.540362 , training accuracy :  0.863075\n",
      "i: 11600 , cost :  2.79271 , training accuracy :  0.723906\n",
      "i: 11700 , cost :  0.617932 , training accuracy :  0.861953\n",
      "i: 11800 , cost :  4.52689 , training accuracy :  0.556678\n",
      "i: 11900 , cost :  0.768142 , training accuracy :  0.861953\n",
      "i: 12000 , cost :  8.41586 , training accuracy :  0.689113\n",
      "i: 12100 , cost :  1.08813 , training accuracy :  0.840629\n",
      "i: 12200 , cost :  0.545836 , training accuracy :  0.864198\n",
      "i: 12300 , cost :  0.839342 , training accuracy :  0.859708\n",
      "i: 12400 , cost :  1.31912 , training accuracy :  0.826038\n",
      "i: 12500 , cost :  0.55374 , training accuracy :  0.867565\n",
      "i: 12600 , cost :  0.768412 , training accuracy :  0.859708\n",
      "i: 12700 , cost :  1.99748 , training accuracy :  0.814815\n",
      "i: 12800 , cost :  2.86006 , training accuracy :  0.765432\n",
      "i: 12900 , cost :  0.500939 , training accuracy :  0.870932\n",
      "i: 13000 , cost :  1.63023 , training accuracy :  0.819304\n",
      "i: 13100 , cost :  0.532845 , training accuracy :  0.866442\n",
      "i: 13200 , cost :  1.42558 , training accuracy :  0.775533\n",
      "i: 13300 , cost :  1.74989 , training accuracy :  0.811448\n",
      "i: 13400 , cost :  0.529261 , training accuracy :  0.869809\n",
      "i: 13500 , cost :  5.67889 , training accuracy :  0.520763\n",
      "i: 13600 , cost :  0.630058 , training accuracy :  0.864198\n",
      "i: 13700 , cost :  1.95153 , training accuracy :  0.81257\n",
      "i: 13800 , cost :  0.940293 , training accuracy :  0.843996\n",
      "i: 13900 , cost :  0.821279 , training accuracy :  0.854097\n",
      "i: 14000 , cost :  1.04334 , training accuracy :  0.833894\n",
      "i: 14100 , cost :  1.29039 , training accuracy :  0.823793\n",
      "i: 14200 , cost :  0.539014 , training accuracy :  0.874299\n",
      "i: 14300 , cost :  4.71972 , training accuracy :  0.777778\n",
      "i: 14400 , cost :  1.62848 , training accuracy :  0.750842\n",
      "i: 14500 , cost :  0.501998 , training accuracy :  0.878788\n",
      "i: 14600 , cost :  2.93563 , training accuracy :  0.753086\n",
      "i: 14700 , cost :  1.95682 , training accuracy :  0.671156\n",
      "i: 14800 , cost :  1.11152 , training accuracy :  0.852974\n",
      "i: 14900 , cost :  0.620018 , training accuracy :  0.875421\n",
      "i: 15000 , cost :  0.463005 , training accuracy :  0.868687\n",
      "i: 15100 , cost :  1.211 , training accuracy :  0.783389\n",
      "i: 15200 , cost :  0.540547 , training accuracy :  0.874299\n",
      "i: 15300 , cost :  1.87771 , training accuracy :  0.731762\n",
      "i: 15400 , cost :  4.45974 , training accuracy :  0.792368\n",
      "i: 15500 , cost :  0.575226 , training accuracy :  0.869809\n",
      "i: 15600 , cost :  13.5265 , training accuracy :  0.421998\n",
      "i: 15700 , cost :  0.640803 , training accuracy :  0.876543\n",
      "i: 15800 , cost :  0.602182 , training accuracy :  0.866442\n",
      "i: 15900 , cost :  0.531909 , training accuracy :  0.84624\n",
      "i: 16000 , cost :  0.636887 , training accuracy :  0.874299\n",
      "i: 16100 , cost :  1.20605 , training accuracy :  0.849607\n",
      "i: 16200 , cost :  0.496861 , training accuracy :  0.877666\n",
      "i: 16300 , cost :  1.58216 , training accuracy :  0.83165\n",
      "i: 16400 , cost :  1.62843 , training accuracy :  0.826038\n",
      "i: 16500 , cost :  0.57454 , training accuracy :  0.87991\n",
      "i: 16600 , cost :  0.658181 , training accuracy :  0.81257\n",
      "i: 16700 , cost :  1.48172 , training accuracy :  0.823793\n",
      "i: 16800 , cost :  0.443308 , training accuracy :  0.885522\n",
      "i: 16900 , cost :  0.80218 , training accuracy :  0.849607\n",
      "i: 17000 , cost :  6.31872 , training accuracy :  0.750842\n",
      "i: 17100 , cost :  2.01126 , training accuracy :  0.824916\n",
      "i: 17200 , cost :  2.53594 , training accuracy :  0.586981\n",
      "i: 17300 , cost :  1.82316 , training accuracy :  0.826038\n",
      "i: 17400 , cost :  0.492212 , training accuracy :  0.8844\n",
      "i: 17500 , cost :  3.88054 , training accuracy :  0.763187\n",
      "i: 17600 , cost :  6.85769 , training accuracy :  0.721661\n",
      "i: 17700 , cost :  1.13488 , training accuracy :  0.849607\n",
      "i: 17800 , cost :  0.677585 , training accuracy :  0.881033\n",
      "i: 17900 , cost :  0.464258 , training accuracy :  0.887767\n",
      "i: 18000 , cost :  1.14342 , training accuracy :  0.837261\n",
      "i: 18100 , cost :  0.397945 , training accuracy :  0.894501\n",
      "i: 18200 , cost :  0.988897 , training accuracy :  0.843996\n",
      "i: 18300 , cost :  1.91123 , training accuracy :  0.83165\n",
      "i: 18400 , cost :  0.846037 , training accuracy :  0.821549\n",
      "i: 18500 , cost :  0.423011 , training accuracy :  0.894501\n",
      "i: 18600 , cost :  2.27325 , training accuracy :  0.803591\n",
      "i: 18700 , cost :  1.81002 , training accuracy :  0.842873\n",
      "i: 18800 , cost :  0.995975 , training accuracy :  0.845118\n",
      "i: 18900 , cost :  0.494117 , training accuracy :  0.894501\n",
      "i: 19000 , cost :  0.385726 , training accuracy :  0.893378\n",
      "i: 19100 , cost :  0.526604 , training accuracy :  0.885522\n",
      "i: 19200 , cost :  1.18722 , training accuracy :  0.822671\n",
      "i: 19300 , cost :  0.901468 , training accuracy :  0.864198\n",
      "i: 19400 , cost :  0.444772 , training accuracy :  0.890011\n",
      "i: 19500 , cost :  0.685209 , training accuracy :  0.762065\n",
      "i: 19600 , cost :  1.50024 , training accuracy :  0.833894\n",
      "i: 19700 , cost :  0.446802 , training accuracy :  0.891134\n",
      "i: 19800 , cost :  0.353607 , training accuracy :  0.897868\n",
      "i: 19900 , cost :  1.68523 , training accuracy :  0.840629\n",
      "i: 20000 , cost :  0.823017 , training accuracy :  0.875421\n",
      "i: 20100 , cost :  0.391633 , training accuracy :  0.893378\n",
      "i: 20200 , cost :  1.72395 , training accuracy :  0.785634\n",
      "i: 20300 , cost :  2.34795 , training accuracy :  0.82716\n",
      "i: 20400 , cost :  0.691108 , training accuracy :  0.876543\n",
      "i: 20500 , cost :  0.443618 , training accuracy :  0.891134\n",
      "i: 20600 , cost :  0.3438 , training accuracy :  0.893378\n",
      "i: 20700 , cost :  1.68174 , training accuracy :  0.817059\n",
      "i: 20800 , cost :  8.36945 , training accuracy :  0.450056\n",
      "i: 20900 , cost :  0.502609 , training accuracy :  0.877666\n",
      "i: 21000 , cost :  0.746177 , training accuracy :  0.869809\n",
      "i: 21100 , cost :  0.378161 , training accuracy :  0.890011\n",
      "i: 21200 , cost :  6.47633 , training accuracy :  0.459035\n",
      "i: 21300 , cost :  0.796444 , training accuracy :  0.85073\n",
      "i: 21400 , cost :  0.589674 , training accuracy :  0.861953\n",
      "i: 21500 , cost :  0.967234 , training accuracy :  0.856341\n",
      "i: 21600 , cost :  0.456407 , training accuracy :  0.893378\n",
      "i: 21700 , cost :  0.345197 , training accuracy :  0.895623\n",
      "i: 21800 , cost :  3.55601 , training accuracy :  0.75982\n",
      "i: 21900 , cost :  1.74509 , training accuracy :  0.833894\n",
      "i: 22000 , cost :  1.66681 , training accuracy :  0.837261\n",
      "i: 22100 , cost :  0.71044 , training accuracy :  0.869809\n",
      "i: 22200 , cost :  0.336662 , training accuracy :  0.896745\n",
      "i: 22300 , cost :  5.01124 , training accuracy :  0.746352\n",
      "i: 22400 , cost :  1.33788 , training accuracy :  0.822671\n",
      "i: 22500 , cost :  1.5538 , training accuracy :  0.832772\n",
      "i: 22600 , cost :  0.47489 , training accuracy :  0.882155\n",
      "i: 22700 , cost :  4.48535 , training accuracy :  0.767677\n",
      "i: 22800 , cost :  0.750761 , training accuracy :  0.870932\n",
      "i: 22900 , cost :  0.333013 , training accuracy :  0.902357\n",
      "i: 23000 , cost :  0.93374 , training accuracy :  0.842873\n",
      "i: 23100 , cost :  0.965872 , training accuracy :  0.864198\n",
      "i: 23200 , cost :  4.50141 , training accuracy :  0.763187\n",
      "i: 23300 , cost :  1.14558 , training accuracy :  0.855219\n",
      "i: 23400 , cost :  1.0449 , training accuracy :  0.735129\n",
      "i: 23500 , cost :  0.334887 , training accuracy :  0.893378\n",
      "i: 23600 , cost :  2.52929 , training accuracy :  0.792368\n",
      "i: 23700 , cost :  0.806358 , training accuracy :  0.86532\n",
      "i: 23800 , cost :  0.569841 , training accuracy :  0.841751\n",
      "i: 23900 , cost :  1.07882 , training accuracy :  0.873176\n",
      "i: 24000 , cost :  0.468215 , training accuracy :  0.883277\n",
      "i: 24100 , cost :  0.33893 , training accuracy :  0.900112\n",
      "i: 24200 , cost :  1.44783 , training accuracy :  0.822671\n",
      "i: 24300 , cost :  6.14079 , training accuracy :  0.464646\n",
      "i: 24400 , cost :  1.34606 , training accuracy :  0.842873\n",
      "i: 24500 , cost :  2.95513 , training accuracy :  0.804714\n",
      "i: 24600 , cost :  0.801472 , training accuracy :  0.877666\n",
      "i: 24700 , cost :  4.94135 , training accuracy :  0.744108\n",
      "i: 24800 , cost :  1.09091 , training accuracy :  0.868687\n",
      "i: 24900 , cost :  1.03965 , training accuracy :  0.841751\n",
      "i: 25000 , cost :  0.361444 , training accuracy :  0.895623\n",
      "i: 25100 , cost :  0.580733 , training accuracy :  0.883277\n",
      "i: 25200 , cost :  0.687832 , training accuracy :  0.866442\n",
      "i: 25300 , cost :  0.384744 , training accuracy :  0.894501\n",
      "i: 25400 , cost :  0.306758 , training accuracy :  0.897868\n",
      "i: 25500 , cost :  0.506977 , training accuracy :  0.8844\n",
      "i: 25600 , cost :  1.23858 , training accuracy :  0.810326\n",
      "i: 25700 , cost :  1.22385 , training accuracy :  0.820426\n",
      "i: 25800 , cost :  0.750381 , training accuracy :  0.837261\n",
      "i: 25900 , cost :  0.558085 , training accuracy :  0.855219\n",
      "i: 26000 , cost :  1.53232 , training accuracy :  0.748597\n",
      "i: 26100 , cost :  0.37531 , training accuracy :  0.897868\n",
      "i: 26200 , cost :  0.51814 , training accuracy :  0.883277\n",
      "i: 26300 , cost :  0.305695 , training accuracy :  0.897868\n",
      "i: 26400 , cost :  0.398198 , training accuracy :  0.8844\n",
      "i: 26500 , cost :  1.805 , training accuracy :  0.79798\n",
      "i: 26600 , cost :  0.983243 , training accuracy :  0.858586\n",
      "i: 26700 , cost :  0.77182 , training accuracy :  0.867565\n",
      "i: 26800 , cost :  0.364837 , training accuracy :  0.902357\n",
      "i: 26900 , cost :  0.289499 , training accuracy :  0.900112\n",
      "i: 27000 , cost :  2.01521 , training accuracy :  0.691358\n",
      "i: 27100 , cost :  1.17022 , training accuracy :  0.856341\n",
      "i: 27200 , cost :  0.745992 , training accuracy :  0.856341\n",
      "i: 27300 , cost :  0.489101 , training accuracy :  0.893378\n",
      "i: 27400 , cost :  3.14612 , training accuracy :  0.7789\n",
      "i: 27500 , cost :  0.343616 , training accuracy :  0.89899\n",
      "i: 27600 , cost :  0.285301 , training accuracy :  0.901235\n",
      "i: 27700 , cost :  0.829868 , training accuracy :  0.866442\n",
      "i: 27800 , cost :  2.54642 , training accuracy :  0.799102\n",
      "i: 27900 , cost :  1.34018 , training accuracy :  0.845118\n",
      "i: 28000 , cost :  1.07745 , training accuracy :  0.799102\n",
      "i: 28100 , cost :  0.284387 , training accuracy :  0.903479\n",
      "i: 28200 , cost :  0.772762 , training accuracy :  0.832772\n",
      "i: 28300 , cost :  2.2927 , training accuracy :  0.81257\n",
      "i: 28400 , cost :  0.341189 , training accuracy :  0.900112\n",
      "i: 28500 , cost :  0.276447 , training accuracy :  0.904602\n",
      "i: 28600 , cost :  0.565444 , training accuracy :  0.895623\n",
      "i: 28700 , cost :  3.7555 , training accuracy :  0.794613\n",
      "i: 28800 , cost :  0.782128 , training accuracy :  0.882155\n",
      "i: 28900 , cost :  2.52873 , training accuracy :  0.822671\n",
      "i: 29000 , cost :  0.419919 , training accuracy :  0.864198\n",
      "i: 29100 , cost :  0.631177 , training accuracy :  0.866442\n",
      "i: 29200 , cost :  0.301987 , training accuracy :  0.906846\n",
      "i: 29300 , cost :  1.0455 , training accuracy :  0.841751\n",
      "i: 29400 , cost :  0.785394 , training accuracy :  0.851852\n",
      "i: 29500 , cost :  0.428311 , training accuracy :  0.89899\n",
      "i: 29600 , cost :  0.281882 , training accuracy :  0.902357\n",
      "i: 29700 , cost :  0.378847 , training accuracy :  0.886644\n",
      "i: 29800 , cost :  0.774779 , training accuracy :  0.878788\n",
      "i: 29900 , cost :  0.460624 , training accuracy :  0.900112\n",
      "Final training accuracy :  0.87991\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 30000 ][hidden: 50 ][file: bhavul_tr_acc_0.88_prediction.csv ] ACCURACY :  0.87991\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1877.22 , training accuracy :  0.379349\n",
      "i: 100 , cost :  10.4078 , training accuracy :  0.544332\n",
      "i: 200 , cost :  3.23837 , training accuracy :  0.805836\n",
      "i: 300 , cost :  20.8323 , training accuracy :  0.714927\n",
      "i: 400 , cost :  4.76448 , training accuracy :  0.700337\n",
      "i: 500 , cost :  2.58802 , training accuracy :  0.83165\n",
      "i: 600 , cost :  5.02724 , training accuracy :  0.790123\n",
      "i: 700 , cost :  6.45148 , training accuracy :  0.76431\n",
      "i: 800 , cost :  2.08934 , training accuracy :  0.833894\n",
      "i: 900 , cost :  5.61409 , training accuracy :  0.79798\n",
      "i: 1000 , cost :  2.01985 , training accuracy :  0.824916\n",
      "i: 1100 , cost :  8.29683 , training accuracy :  0.703704\n",
      "i: 1200 , cost :  1.75422 , training accuracy :  0.855219\n",
      "i: 1300 , cost :  2.87174 , training accuracy :  0.837261\n",
      "i: 1400 , cost :  8.32099 , training accuracy :  0.718294\n",
      "i: 1500 , cost :  2.1868 , training accuracy :  0.854097\n",
      "i: 1600 , cost :  15.9011 , training accuracy :  0.71156\n",
      "i: 1700 , cost :  1.7728 , training accuracy :  0.863075\n",
      "i: 1800 , cost :  8.25725 , training accuracy :  0.794613\n",
      "i: 1900 , cost :  1.6299 , training accuracy :  0.863075\n",
      "i: 2000 , cost :  6.34423 , training accuracy :  0.698092\n",
      "i: 2100 , cost :  45.3929 , training accuracy :  0.398429\n",
      "i: 2200 , cost :  4.69587 , training accuracy :  0.725028\n",
      "i: 2300 , cost :  1.56801 , training accuracy :  0.864198\n",
      "i: 2400 , cost :  11.1683 , training accuracy :  0.760943\n",
      "i: 2500 , cost :  1.81934 , training accuracy :  0.861953\n",
      "i: 2600 , cost :  9.96816 , training accuracy :  0.731762\n",
      "i: 2700 , cost :  2.03464 , training accuracy :  0.866442\n",
      "i: 2800 , cost :  12.2388 , training accuracy :  0.734007\n",
      "i: 2900 , cost :  2.11019 , training accuracy :  0.857464\n",
      "i: 3000 , cost :  8.68953 , training accuracy :  0.771044\n",
      "i: 3100 , cost :  1.58305 , training accuracy :  0.836139\n",
      "i: 3200 , cost :  1.88522 , training accuracy :  0.835017\n",
      "i: 3300 , cost :  1.82313 , training accuracy :  0.864198\n",
      "i: 3400 , cost :  2.87255 , training accuracy :  0.810326\n",
      "i: 3500 , cost :  3.95539 , training accuracy :  0.815937\n",
      "i: 3600 , cost :  13.7093 , training accuracy :  0.709315\n",
      "i: 3700 , cost :  2.49369 , training accuracy :  0.823793\n",
      "i: 3800 , cost :  12.9498 , training accuracy :  0.432099\n",
      "i: 3900 , cost :  1.93042 , training accuracy :  0.856341\n",
      "i: 4000 , cost :  1.95345 , training accuracy :  0.85073\n",
      "i: 4100 , cost :  4.08719 , training accuracy :  0.803591\n",
      "i: 4200 , cost :  12.3863 , training accuracy :  0.721661\n",
      "i: 4300 , cost :  2.07944 , training accuracy :  0.86532\n",
      "i: 4400 , cost :  5.25527 , training accuracy :  0.768799\n",
      "i: 4500 , cost :  6.00617 , training accuracy :  0.689113\n",
      "i: 4600 , cost :  1.58391 , training accuracy :  0.854097\n",
      "i: 4700 , cost :  9.08253 , training accuracy :  0.756453\n",
      "i: 4800 , cost :  1.27914 , training accuracy :  0.869809\n",
      "i: 4900 , cost :  15.4578 , training accuracy :  0.454545\n",
      "i: 5000 , cost :  3.01435 , training accuracy :  0.806958\n",
      "i: 5100 , cost :  1.30145 , training accuracy :  0.855219\n",
      "i: 5200 , cost :  6.56611 , training accuracy :  0.766554\n",
      "i: 5300 , cost :  3.95296 , training accuracy :  0.795735\n",
      "i: 5400 , cost :  1.86425 , training accuracy :  0.86532\n",
      "i: 5500 , cost :  11.9276 , training accuracy :  0.748597\n",
      "i: 5600 , cost :  4.63258 , training accuracy :  0.805836\n",
      "i: 5700 , cost :  1.39102 , training accuracy :  0.863075\n",
      "i: 5800 , cost :  6.46129 , training accuracy :  0.802469\n",
      "i: 5900 , cost :  2.51718 , training accuracy :  0.817059\n",
      "i: 6000 , cost :  6.19645 , training accuracy :  0.783389\n",
      "i: 6100 , cost :  17.8662 , training accuracy :  0.438833\n",
      "i: 6200 , cost :  3.30544 , training accuracy :  0.787879\n",
      "i: 6300 , cost :  7.70639 , training accuracy :  0.786756\n",
      "i: 6400 , cost :  1.44023 , training accuracy :  0.860831\n",
      "i: 6500 , cost :  10.0089 , training accuracy :  0.749719\n",
      "i: 6600 , cost :  1.61723 , training accuracy :  0.856341\n",
      "i: 6700 , cost :  3.1711 , training accuracy :  0.808081\n",
      "i: 6800 , cost :  8.46865 , training accuracy :  0.768799\n",
      "i: 6900 , cost :  19.001 , training accuracy :  0.72615\n",
      "i: 7000 , cost :  5.25027 , training accuracy :  0.81257\n",
      "i: 7100 , cost :  2.65247 , training accuracy :  0.811448\n",
      "i: 7200 , cost :  1.18866 , training accuracy :  0.86532\n",
      "i: 7300 , cost :  13.744 , training accuracy :  0.466891\n",
      "i: 7400 , cost :  0.965829 , training accuracy :  0.877666\n",
      "i: 7500 , cost :  1.28634 , training accuracy :  0.857464\n",
      "i: 7600 , cost :  1.08028 , training accuracy :  0.859708\n",
      "i: 7700 , cost :  6.293 , training accuracy :  0.802469\n",
      "i: 7800 , cost :  1.39556 , training accuracy :  0.86532\n",
      "i: 7900 , cost :  3.54699 , training accuracy :  0.794613\n",
      "i: 8000 , cost :  1.3008 , training accuracy :  0.860831\n",
      "i: 8100 , cost :  2.28176 , training accuracy :  0.826038\n",
      "i: 8200 , cost :  7.58816 , training accuracy :  0.572391\n",
      "i: 8300 , cost :  1.13325 , training accuracy :  0.870932\n",
      "i: 8400 , cost :  1.96521 , training accuracy :  0.835017\n",
      "i: 8500 , cost :  2.36354 , training accuracy :  0.84624\n",
      "i: 8600 , cost :  5.44168 , training accuracy :  0.783389\n",
      "i: 8700 , cost :  1.6718 , training accuracy :  0.863075\n",
      "i: 8800 , cost :  13.6179 , training accuracy :  0.747475\n",
      "i: 8900 , cost :  1.14228 , training accuracy :  0.875421\n",
      "i: 9000 , cost :  6.37577 , training accuracy :  0.789001\n",
      "i: 9100 , cost :  1.584 , training accuracy :  0.864198\n",
      "i: 9200 , cost :  1.37569 , training accuracy :  0.830527\n",
      "i: 9300 , cost :  0.968241 , training accuracy :  0.878788\n",
      "i: 9400 , cost :  1.6321 , training accuracy :  0.842873\n",
      "i: 9500 , cost :  5.4455 , training accuracy :  0.808081\n",
      "i: 9600 , cost :  1.09325 , training accuracy :  0.876543\n",
      "i: 9700 , cost :  12.7666 , training accuracy :  0.426487\n",
      "i: 9800 , cost :  4.79549 , training accuracy :  0.702581\n",
      "i: 9900 , cost :  0.80409 , training accuracy :  0.881033\n",
      "i: 10000 , cost :  2.71563 , training accuracy :  0.841751\n",
      "i: 10100 , cost :  10.1808 , training accuracy :  0.765432\n",
      "i: 10200 , cost :  1.28799 , training accuracy :  0.868687\n",
      "i: 10300 , cost :  0.971099 , training accuracy :  0.886644\n",
      "i: 10400 , cost :  2.33382 , training accuracy :  0.847363\n",
      "i: 10500 , cost :  2.08322 , training accuracy :  0.823793\n",
      "i: 10600 , cost :  0.790013 , training accuracy :  0.875421\n",
      "i: 10700 , cost :  8.31929 , training accuracy :  0.76431\n",
      "i: 10800 , cost :  5.48767 , training accuracy :  0.76431\n",
      "i: 10900 , cost :  8.6413 , training accuracy :  0.749719\n",
      "i: 11000 , cost :  2.12455 , training accuracy :  0.803591\n",
      "i: 11100 , cost :  6.62496 , training accuracy :  0.668911\n",
      "i: 11200 , cost :  5.272 , training accuracy :  0.815937\n",
      "i: 11300 , cost :  0.725014 , training accuracy :  0.892256\n",
      "i: 11400 , cost :  0.791654 , training accuracy :  0.8844\n",
      "i: 11500 , cost :  1.00193 , training accuracy :  0.87991\n",
      "i: 11600 , cost :  1.66316 , training accuracy :  0.847363\n",
      "i: 11700 , cost :  0.864044 , training accuracy :  0.885522\n",
      "i: 11800 , cost :  1.10189 , training accuracy :  0.867565\n",
      "i: 11900 , cost :  0.859589 , training accuracy :  0.887767\n",
      "i: 12000 , cost :  1.98026 , training accuracy :  0.845118\n",
      "i: 12100 , cost :  1.12519 , training accuracy :  0.885522\n",
      "i: 12200 , cost :  5.21496 , training accuracy :  0.723906\n",
      "i: 12300 , cost :  0.68369 , training accuracy :  0.882155\n",
      "i: 12400 , cost :  1.43311 , training accuracy :  0.876543\n",
      "i: 12500 , cost :  0.987474 , training accuracy :  0.866442\n",
      "i: 12600 , cost :  3.87819 , training accuracy :  0.828283\n",
      "i: 12700 , cost :  0.714457 , training accuracy :  0.887767\n",
      "i: 12800 , cost :  2.97089 , training accuracy :  0.829405\n",
      "i: 12900 , cost :  1.23793 , training accuracy :  0.877666\n",
      "i: 13000 , cost :  9.3943 , training accuracy :  0.755331\n",
      "i: 13100 , cost :  4.4933 , training accuracy :  0.829405\n",
      "i: 13200 , cost :  0.770176 , training accuracy :  0.882155\n",
      "i: 13300 , cost :  3.29543 , training accuracy :  0.757576\n",
      "i: 13400 , cost :  6.99909 , training accuracy :  0.561167\n",
      "i: 13500 , cost :  1.29072 , training accuracy :  0.873176\n",
      "i: 13600 , cost :  0.979796 , training accuracy :  0.890011\n",
      "i: 13700 , cost :  0.883771 , training accuracy :  0.890011\n",
      "i: 13800 , cost :  6.1858 , training accuracy :  0.792368\n",
      "i: 13900 , cost :  0.717758 , training accuracy :  0.887767\n",
      "i: 14000 , cost :  2.20999 , training accuracy :  0.845118\n",
      "i: 14100 , cost :  2.98971 , training accuracy :  0.781145\n",
      "i: 14200 , cost :  1.56362 , training accuracy :  0.868687\n",
      "i: 14300 , cost :  0.717077 , training accuracy :  0.885522\n",
      "i: 14400 , cost :  2.9503 , training accuracy :  0.820426\n",
      "i: 14500 , cost :  1.52966 , training accuracy :  0.891134\n",
      "i: 14600 , cost :  9.47906 , training accuracy :  0.762065\n",
      "i: 14700 , cost :  8.28988 , training accuracy :  0.511784\n",
      "i: 14800 , cost :  13.4978 , training accuracy :  0.444444\n",
      "i: 14900 , cost :  0.737438 , training accuracy :  0.888889\n",
      "i: 15000 , cost :  4.89083 , training accuracy :  0.680135\n",
      "i: 15100 , cost :  5.72237 , training accuracy :  0.819304\n",
      "i: 15200 , cost :  1.12924 , training accuracy :  0.877666\n",
      "i: 15300 , cost :  5.10032 , training accuracy :  0.821549\n",
      "i: 15400 , cost :  3.07649 , training accuracy :  0.769921\n",
      "i: 15500 , cost :  9.60169 , training accuracy :  0.506173\n",
      "i: 15600 , cost :  1.04027 , training accuracy :  0.870932\n",
      "i: 15700 , cost :  0.935893 , training accuracy :  0.891134\n",
      "i: 15800 , cost :  0.569572 , training accuracy :  0.893378\n",
      "i: 15900 , cost :  2.91152 , training accuracy :  0.833894\n",
      "i: 16000 , cost :  2.486 , training accuracy :  0.842873\n",
      "i: 16100 , cost :  0.951439 , training accuracy :  0.888889\n",
      "i: 16200 , cost :  4.58971 , training accuracy :  0.804714\n",
      "i: 16300 , cost :  0.640218 , training accuracy :  0.904602\n",
      "i: 16400 , cost :  6.40465 , training accuracy :  0.794613\n",
      "i: 16500 , cost :  1.17667 , training accuracy :  0.893378\n",
      "i: 16600 , cost :  1.03471 , training accuracy :  0.864198\n",
      "i: 16700 , cost :  9.00776 , training accuracy :  0.769921\n",
      "i: 16800 , cost :  1.99579 , training accuracy :  0.86532\n",
      "i: 16900 , cost :  0.771324 , training accuracy :  0.906846\n",
      "i: 17000 , cost :  0.515865 , training accuracy :  0.903479\n",
      "i: 17100 , cost :  2.90473 , training accuracy :  0.838384\n",
      "i: 17200 , cost :  0.984319 , training accuracy :  0.860831\n",
      "i: 17300 , cost :  5.35801 , training accuracy :  0.588103\n",
      "i: 17400 , cost :  2.08482 , training accuracy :  0.787879\n",
      "i: 17500 , cost :  5.05718 , training accuracy :  0.808081\n",
      "i: 17600 , cost :  0.55434 , training accuracy :  0.902357\n",
      "i: 17700 , cost :  7.12768 , training accuracy :  0.809203\n",
      "i: 17800 , cost :  9.70031 , training accuracy :  0.76431\n",
      "i: 17900 , cost :  0.70297 , training accuracy :  0.882155\n",
      "i: 18000 , cost :  0.587373 , training accuracy :  0.903479\n",
      "i: 18100 , cost :  2.59381 , training accuracy :  0.854097\n",
      "i: 18200 , cost :  2.49811 , training accuracy :  0.854097\n",
      "i: 18300 , cost :  1.96717 , training accuracy :  0.863075\n",
      "i: 18400 , cost :  14.6071 , training accuracy :  0.465769\n",
      "i: 18500 , cost :  2.96263 , training accuracy :  0.648709\n",
      "i: 18600 , cost :  0.913655 , training accuracy :  0.891134\n",
      "i: 18700 , cost :  3.0064 , training accuracy :  0.826038\n",
      "i: 18800 , cost :  0.582724 , training accuracy :  0.896745\n",
      "i: 18900 , cost :  4.97508 , training accuracy :  0.527497\n",
      "i: 19000 , cost :  0.451716 , training accuracy :  0.904602\n",
      "i: 19100 , cost :  4.49139 , training accuracy :  0.826038\n",
      "i: 19200 , cost :  1.55627 , training accuracy :  0.869809\n",
      "i: 19300 , cost :  0.671478 , training accuracy :  0.900112\n",
      "i: 19400 , cost :  1.86812 , training accuracy :  0.857464\n",
      "i: 19500 , cost :  1.62521 , training accuracy :  0.877666\n",
      "i: 19600 , cost :  0.428243 , training accuracy :  0.905724\n",
      "i: 19700 , cost :  1.05799 , training accuracy :  0.856341\n",
      "i: 19800 , cost :  0.886892 , training accuracy :  0.854097\n",
      "i: 19900 , cost :  1.10561 , training accuracy :  0.892256\n",
      "i: 20000 , cost :  11.8563 , training accuracy :  0.455668\n",
      "i: 20100 , cost :  0.403849 , training accuracy :  0.904602\n",
      "i: 20200 , cost :  1.88668 , training accuracy :  0.847363\n",
      "i: 20300 , cost :  0.701907 , training accuracy :  0.902357\n",
      "i: 20400 , cost :  0.8075 , training accuracy :  0.893378\n",
      "i: 20500 , cost :  0.474992 , training accuracy :  0.89899\n",
      "i: 20600 , cost :  0.568533 , training accuracy :  0.89899\n",
      "i: 20700 , cost :  8.18416 , training accuracy :  0.507295\n",
      "i: 20800 , cost :  5.04125 , training accuracy :  0.822671\n",
      "i: 20900 , cost :  1.121 , training accuracy :  0.863075\n",
      "i: 21000 , cost :  0.477001 , training accuracy :  0.901235\n",
      "i: 21100 , cost :  0.620977 , training accuracy :  0.907969\n",
      "i: 21200 , cost :  0.34133 , training accuracy :  0.903479\n",
      "i: 21300 , cost :  0.538321 , training accuracy :  0.895623\n",
      "i: 21400 , cost :  0.388969 , training accuracy :  0.906846\n",
      "i: 21500 , cost :  1.1897 , training accuracy :  0.845118\n",
      "i: 21600 , cost :  0.657698 , training accuracy :  0.903479\n",
      "i: 21700 , cost :  2.08086 , training accuracy :  0.656566\n",
      "i: 21800 , cost :  2.46434 , training accuracy :  0.845118\n",
      "i: 21900 , cost :  0.355147 , training accuracy :  0.904602\n",
      "i: 22000 , cost :  4.98787 , training accuracy :  0.81257\n",
      "i: 22100 , cost :  0.442505 , training accuracy :  0.904602\n",
      "i: 22200 , cost :  0.439818 , training accuracy :  0.900112\n",
      "i: 22300 , cost :  0.426786 , training accuracy :  0.903479\n",
      "i: 22400 , cost :  0.440241 , training accuracy :  0.906846\n",
      "i: 22500 , cost :  1.81009 , training accuracy :  0.855219\n",
      "i: 22600 , cost :  0.71823 , training accuracy :  0.895623\n",
      "i: 22700 , cost :  0.342143 , training accuracy :  0.903479\n",
      "i: 22800 , cost :  0.715855 , training accuracy :  0.907969\n",
      "i: 22900 , cost :  0.719141 , training accuracy :  0.854097\n",
      "i: 23000 , cost :  0.383735 , training accuracy :  0.901235\n",
      "i: 23100 , cost :  0.384372 , training accuracy :  0.900112\n",
      "i: 23200 , cost :  8.5127 , training accuracy :  0.508417\n",
      "i: 23300 , cost :  0.395146 , training accuracy :  0.902357\n",
      "i: 23400 , cost :  1.26023 , training accuracy :  0.867565\n",
      "i: 23500 , cost :  0.53626 , training accuracy :  0.905724\n",
      "i: 23600 , cost :  0.623376 , training accuracy :  0.907969\n",
      "i: 23700 , cost :  0.3985 , training accuracy :  0.906846\n",
      "i: 23800 , cost :  0.686293 , training accuracy :  0.8844\n",
      "i: 23900 , cost :  0.290244 , training accuracy :  0.911336\n",
      "i: 24000 , cost :  0.900397 , training accuracy :  0.905724\n",
      "i: 24100 , cost :  1.16523 , training accuracy :  0.849607\n",
      "i: 24200 , cost :  3.55206 , training accuracy :  0.836139\n",
      "i: 24300 , cost :  4.2407 , training accuracy :  0.561167\n",
      "i: 24400 , cost :  0.971242 , training accuracy :  0.901235\n",
      "i: 24500 , cost :  2.26738 , training accuracy :  0.840629\n",
      "i: 24600 , cost :  0.325895 , training accuracy :  0.911336\n",
      "i: 24700 , cost :  0.895859 , training accuracy :  0.904602\n",
      "i: 24800 , cost :  0.927982 , training accuracy :  0.8844\n",
      "i: 24900 , cost :  0.318054 , training accuracy :  0.91358\n",
      "i: 25000 , cost :  0.857633 , training accuracy :  0.870932\n",
      "i: 25100 , cost :  0.490554 , training accuracy :  0.920314\n",
      "i: 25200 , cost :  1.08376 , training accuracy :  0.878788\n",
      "i: 25300 , cost :  0.509994 , training accuracy :  0.909091\n",
      "i: 25400 , cost :  0.301986 , training accuracy :  0.909091\n",
      "i: 25500 , cost :  2.02395 , training accuracy :  0.773288\n",
      "i: 25600 , cost :  2.9914 , training accuracy :  0.843996\n",
      "i: 25700 , cost :  0.289649 , training accuracy :  0.916947\n",
      "i: 25800 , cost :  1.20675 , training accuracy :  0.887767\n",
      "i: 25900 , cost :  0.268645 , training accuracy :  0.919192\n",
      "i: 26000 , cost :  1.30628 , training accuracy :  0.832772\n",
      "i: 26100 , cost :  0.895523 , training accuracy :  0.895623\n",
      "i: 26200 , cost :  0.457405 , training accuracy :  0.909091\n",
      "i: 26300 , cost :  0.626897 , training accuracy :  0.877666\n",
      "i: 26400 , cost :  0.234063 , training accuracy :  0.922559\n",
      "i: 26500 , cost :  0.864237 , training accuracy :  0.909091\n",
      "i: 26600 , cost :  2.9714 , training accuracy :  0.824916\n",
      "i: 26700 , cost :  0.679597 , training accuracy :  0.875421\n",
      "i: 26800 , cost :  0.23471 , training accuracy :  0.921437\n",
      "i: 26900 , cost :  0.238241 , training accuracy :  0.920314\n",
      "i: 27000 , cost :  0.530909 , training accuracy :  0.891134\n",
      "i: 27100 , cost :  2.10173 , training accuracy :  0.799102\n",
      "i: 27200 , cost :  0.614118 , training accuracy :  0.906846\n",
      "i: 27300 , cost :  0.713675 , training accuracy :  0.892256\n",
      "i: 27400 , cost :  0.862591 , training accuracy :  0.903479\n",
      "i: 27500 , cost :  0.455128 , training accuracy :  0.920314\n",
      "i: 27600 , cost :  0.223122 , training accuracy :  0.927048\n",
      "i: 27700 , cost :  1.61266 , training accuracy :  0.819304\n",
      "i: 27800 , cost :  0.624868 , training accuracy :  0.888889\n",
      "i: 27900 , cost :  0.230857 , training accuracy :  0.928171\n",
      "i: 28000 , cost :  5.685 , training accuracy :  0.794613\n",
      "i: 28100 , cost :  3.32513 , training accuracy :  0.575758\n",
      "i: 28200 , cost :  0.625963 , training accuracy :  0.906846\n",
      "i: 28300 , cost :  1.15325 , training accuracy :  0.874299\n",
      "i: 28400 , cost :  0.261352 , training accuracy :  0.922559\n",
      "i: 28500 , cost :  0.20856 , training accuracy :  0.930415\n",
      "i: 28600 , cost :  2.25197 , training accuracy :  0.858586\n",
      "i: 28700 , cost :  0.589335 , training accuracy :  0.888889\n",
      "i: 28800 , cost :  0.71915 , training accuracy :  0.888889\n",
      "i: 28900 , cost :  0.48879 , training accuracy :  0.890011\n",
      "i: 29000 , cost :  0.356779 , training accuracy :  0.907969\n",
      "i: 29100 , cost :  0.212344 , training accuracy :  0.927048\n",
      "i: 29200 , cost :  0.845227 , training accuracy :  0.885522\n",
      "i: 29300 , cost :  2.57886 , training accuracy :  0.841751\n",
      "i: 29400 , cost :  1.81628 , training accuracy :  0.848485\n",
      "i: 29500 , cost :  0.35445 , training accuracy :  0.910213\n",
      "i: 29600 , cost :  1.2622 , training accuracy :  0.868687\n",
      "i: 29700 , cost :  0.273286 , training accuracy :  0.924804\n",
      "i: 29800 , cost :  0.456615 , training accuracy :  0.91807\n",
      "i: 29900 , cost :  0.785582 , training accuracy :  0.786756\n",
      "Final training accuracy :  0.930415\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.02 ][epoch: 30000 ][hidden: 100 ][file: bhavul_tr_acc_0.93_prediction.csv ] ACCURACY :  0.930415\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  40.8782 , training accuracy :  0.368126\n",
      "i: 100 , cost :  0.655577 , training accuracy :  0.638608\n",
      "i: 200 , cost :  0.641243 , training accuracy :  0.662177\n",
      "i: 300 , cost :  0.663981 , training accuracy :  0.619529\n",
      "i: 400 , cost :  0.659583 , training accuracy :  0.631874\n",
      "i: 500 , cost :  0.636738 , training accuracy :  0.668911\n",
      "i: 600 , cost :  0.628452 , training accuracy :  0.670034\n",
      "i: 700 , cost :  0.665931 , training accuracy :  0.616162\n",
      "i: 800 , cost :  0.665911 , training accuracy :  0.616162\n",
      "i: 900 , cost :  0.665913 , training accuracy :  0.616162\n",
      "Final training accuracy :  0.616162\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 1000 ][hidden: 3 ][file: bhavul_tr_acc_0.62_prediction.csv ] ACCURACY :  0.616162\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  689.582 , training accuracy :  0.383838\n",
      "i: 100 , cost :  3.28623 , training accuracy :  0.695847\n",
      "i: 200 , cost :  2.67306 , training accuracy :  0.693603\n",
      "i: 300 , cost :  2.12089 , training accuracy :  0.707071\n",
      "i: 400 , cost :  1.68362 , training accuracy :  0.73064\n",
      "i: 500 , cost :  0.65784 , training accuracy :  0.783389\n",
      "i: 600 , cost :  3.19292 , training accuracy :  0.719416\n",
      "i: 700 , cost :  0.902842 , training accuracy :  0.776655\n",
      "i: 800 , cost :  0.537572 , training accuracy :  0.787879\n",
      "i: 900 , cost :  0.47577 , training accuracy :  0.791246\n",
      "Final training accuracy :  0.771044\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 1000 ][hidden: 10 ][file: bhavul_tr_acc_0.77_prediction.csv ] ACCURACY :  0.771044\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  637.372 , training accuracy :  0.383838\n",
      "i: 100 , cost :  2.68996 , training accuracy :  0.73064\n",
      "i: 200 , cost :  2.63748 , training accuracy :  0.748597\n",
      "i: 300 , cost :  1.98794 , training accuracy :  0.765432\n",
      "i: 400 , cost :  2.32297 , training accuracy :  0.742985\n",
      "i: 500 , cost :  1.14666 , training accuracy :  0.79349\n",
      "i: 600 , cost :  2.40945 , training accuracy :  0.628507\n",
      "i: 700 , cost :  0.831302 , training accuracy :  0.795735\n",
      "i: 800 , cost :  1.76533 , training accuracy :  0.561167\n",
      "i: 900 , cost :  1.13839 , training accuracy :  0.773288\n",
      "Final training accuracy :  0.787879\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 1000 ][hidden: 15 ][file: bhavul_tr_acc_0.79_prediction.csv ] ACCURACY :  0.787879\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  559.274 , training accuracy :  0.615039\n",
      "i: 100 , cost :  12.6373 , training accuracy :  0.518519\n",
      "i: 200 , cost :  4.74791 , training accuracy :  0.762065\n",
      "i: 300 , cost :  6.67751 , training accuracy :  0.634119\n",
      "i: 400 , cost :  12.1637 , training accuracy :  0.672278\n",
      "i: 500 , cost :  2.7723 , training accuracy :  0.801347\n",
      "i: 600 , cost :  5.426 , training accuracy :  0.753086\n",
      "i: 700 , cost :  6.52966 , training accuracy :  0.61055\n",
      "i: 800 , cost :  1.11027 , training accuracy :  0.838384\n",
      "i: 900 , cost :  2.01647 , training accuracy :  0.809203\n",
      "Final training accuracy :  0.774411\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 1000 ][hidden: 50 ][file: bhavul_tr_acc_0.77_prediction.csv ] ACCURACY :  0.774411\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  4444.63 , training accuracy :  0.383838\n",
      "i: 100 , cost :  18.0571 , training accuracy :  0.704826\n",
      "i: 200 , cost :  21.1151 , training accuracy :  0.433221\n",
      "i: 300 , cost :  14.7514 , training accuracy :  0.698092\n",
      "i: 400 , cost :  3.46593 , training accuracy :  0.821549\n",
      "i: 500 , cost :  12.1673 , training accuracy :  0.685746\n",
      "i: 600 , cost :  2.97915 , training accuracy :  0.829405\n",
      "i: 700 , cost :  8.31329 , training accuracy :  0.746352\n",
      "i: 800 , cost :  5.18444 , training accuracy :  0.784512\n",
      "i: 900 , cost :  3.08111 , training accuracy :  0.829405\n",
      "Final training accuracy :  0.76431\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 1000 ][hidden: 100 ][file: bhavul_tr_acc_0.76_prediction.csv ] ACCURACY :  0.76431\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  145.859 , training accuracy :  0.621773\n",
      "i: 100 , cost :  0.675747 , training accuracy :  0.616162\n",
      "i: 200 , cost :  0.667185 , training accuracy :  0.618406\n",
      "i: 300 , cost :  0.665617 , training accuracy :  0.618406\n",
      "i: 400 , cost :  0.664401 , training accuracy :  0.619529\n",
      "i: 500 , cost :  0.663757 , training accuracy :  0.619529\n",
      "i: 600 , cost :  0.663403 , training accuracy :  0.619529\n",
      "i: 700 , cost :  0.663052 , training accuracy :  0.619529\n",
      "i: 800 , cost :  0.662734 , training accuracy :  0.620651\n",
      "i: 900 , cost :  0.662412 , training accuracy :  0.620651\n",
      "i: 1000 , cost :  0.662095 , training accuracy :  0.620651\n",
      "i: 1100 , cost :  0.66179 , training accuracy :  0.620651\n",
      "i: 1200 , cost :  0.661495 , training accuracy :  0.619529\n",
      "i: 1300 , cost :  0.661211 , training accuracy :  0.619529\n",
      "i: 1400 , cost :  0.660935 , training accuracy :  0.619529\n",
      "i: 1500 , cost :  0.66066 , training accuracy :  0.620651\n",
      "i: 1600 , cost :  0.660384 , training accuracy :  0.620651\n",
      "i: 1700 , cost :  0.660114 , training accuracy :  0.620651\n",
      "i: 1800 , cost :  0.659837 , training accuracy :  0.620651\n",
      "i: 1900 , cost :  0.659554 , training accuracy :  0.620651\n",
      "i: 2000 , cost :  0.659266 , training accuracy :  0.620651\n",
      "i: 2100 , cost :  0.658979 , training accuracy :  0.621773\n",
      "i: 2200 , cost :  0.658686 , training accuracy :  0.621773\n",
      "i: 2300 , cost :  0.658412 , training accuracy :  0.622896\n",
      "i: 2400 , cost :  0.658148 , training accuracy :  0.622896\n",
      "i: 2500 , cost :  0.657885 , training accuracy :  0.622896\n",
      "i: 2600 , cost :  0.657642 , training accuracy :  0.622896\n",
      "i: 2700 , cost :  0.657415 , training accuracy :  0.622896\n",
      "i: 2800 , cost :  0.657202 , training accuracy :  0.622896\n",
      "i: 2900 , cost :  0.657 , training accuracy :  0.622896\n",
      "i: 3000 , cost :  0.656811 , training accuracy :  0.622896\n",
      "i: 3100 , cost :  0.656646 , training accuracy :  0.622896\n",
      "i: 3200 , cost :  0.656487 , training accuracy :  0.622896\n",
      "i: 3300 , cost :  0.656348 , training accuracy :  0.622896\n",
      "i: 3400 , cost :  0.656222 , training accuracy :  0.622896\n",
      "i: 3500 , cost :  0.656109 , training accuracy :  0.622896\n",
      "i: 3600 , cost :  0.656004 , training accuracy :  0.622896\n",
      "i: 3700 , cost :  0.655908 , training accuracy :  0.622896\n",
      "i: 3800 , cost :  0.655835 , training accuracy :  0.622896\n",
      "i: 3900 , cost :  0.655773 , training accuracy :  0.622896\n",
      "i: 4000 , cost :  0.655701 , training accuracy :  0.622896\n",
      "i: 4100 , cost :  0.655635 , training accuracy :  0.622896\n",
      "i: 4200 , cost :  0.655588 , training accuracy :  0.622896\n",
      "i: 4300 , cost :  0.655537 , training accuracy :  0.622896\n",
      "i: 4400 , cost :  0.655494 , training accuracy :  0.622896\n",
      "i: 4500 , cost :  0.655464 , training accuracy :  0.622896\n",
      "i: 4600 , cost :  0.65544 , training accuracy :  0.622896\n",
      "i: 4700 , cost :  0.655392 , training accuracy :  0.622896\n",
      "i: 4800 , cost :  0.655374 , training accuracy :  0.622896\n",
      "i: 4900 , cost :  0.655386 , training accuracy :  0.622896\n",
      "Final training accuracy :  0.622896\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 5000 ][hidden: 3 ][file: bhavul_tr_acc_0.62_prediction.csv ] ACCURACY :  0.622896\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  298.321 , training accuracy :  0.611672\n",
      "i: 100 , cost :  3.33775 , training accuracy :  0.681257\n",
      "i: 200 , cost :  1.27883 , training accuracy :  0.736251\n",
      "i: 300 , cost :  8.70435 , training accuracy :  0.387205\n",
      "i: 400 , cost :  0.787452 , training accuracy :  0.786756\n",
      "i: 500 , cost :  8.63338 , training accuracy :  0.400673\n",
      "i: 600 , cost :  0.669001 , training accuracy :  0.7789\n",
      "i: 700 , cost :  1.71978 , training accuracy :  0.753086\n",
      "i: 800 , cost :  0.673011 , training accuracy :  0.783389\n",
      "i: 900 , cost :  3.62667 , training accuracy :  0.662177\n",
      "i: 1000 , cost :  1.14504 , training accuracy :  0.734007\n",
      "i: 1100 , cost :  0.53348 , training accuracy :  0.777778\n",
      "i: 1200 , cost :  0.863475 , training accuracy :  0.750842\n",
      "i: 1300 , cost :  0.794486 , training accuracy :  0.760943\n",
      "i: 1400 , cost :  0.841347 , training accuracy :  0.748597\n",
      "i: 1500 , cost :  1.08879 , training accuracy :  0.656566\n",
      "i: 1600 , cost :  0.507295 , training accuracy :  0.776655\n",
      "i: 1700 , cost :  6.15216 , training accuracy :  0.662177\n",
      "i: 1800 , cost :  0.644394 , training accuracy :  0.768799\n",
      "i: 1900 , cost :  0.477297 , training accuracy :  0.7789\n",
      "i: 2000 , cost :  1.76089 , training accuracy :  0.569024\n",
      "i: 2100 , cost :  0.540808 , training accuracy :  0.783389\n",
      "i: 2200 , cost :  0.563006 , training accuracy :  0.785634\n",
      "i: 2300 , cost :  0.485273 , training accuracy :  0.789001\n",
      "i: 2400 , cost :  2.37832 , training accuracy :  0.673401\n",
      "i: 2500 , cost :  1.01337 , training accuracy :  0.657688\n",
      "i: 2600 , cost :  1.16729 , training accuracy :  0.704826\n",
      "i: 2700 , cost :  0.668218 , training accuracy :  0.720539\n",
      "i: 2800 , cost :  0.611227 , training accuracy :  0.7789\n",
      "i: 2900 , cost :  0.623126 , training accuracy :  0.754209\n",
      "i: 3000 , cost :  0.694437 , training accuracy :  0.716049\n",
      "i: 3100 , cost :  0.506635 , training accuracy :  0.787879\n",
      "i: 3200 , cost :  0.637621 , training accuracy :  0.756453\n",
      "i: 3300 , cost :  0.585101 , training accuracy :  0.771044\n",
      "i: 3400 , cost :  0.523422 , training accuracy :  0.782267\n",
      "i: 3500 , cost :  0.454726 , training accuracy :  0.794613\n",
      "i: 3600 , cost :  0.635684 , training accuracy :  0.775533\n",
      "i: 3700 , cost :  0.437873 , training accuracy :  0.813693\n",
      "i: 3800 , cost :  0.618497 , training accuracy :  0.758698\n",
      "i: 3900 , cost :  0.451459 , training accuracy :  0.810326\n",
      "i: 4000 , cost :  0.439452 , training accuracy :  0.808081\n",
      "i: 4100 , cost :  0.514746 , training accuracy :  0.789001\n",
      "i: 4200 , cost :  0.433048 , training accuracy :  0.815937\n",
      "i: 4300 , cost :  0.652875 , training accuracy :  0.740741\n",
      "i: 4400 , cost :  0.575904 , training accuracy :  0.755331\n",
      "i: 4500 , cost :  0.542261 , training accuracy :  0.746352\n",
      "i: 4600 , cost :  0.775552 , training accuracy :  0.763187\n",
      "i: 4700 , cost :  0.454627 , training accuracy :  0.804714\n",
      "i: 4800 , cost :  0.431646 , training accuracy :  0.820426\n",
      "i: 4900 , cost :  0.999238 , training accuracy :  0.57688\n",
      "Final training accuracy :  0.755331\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 5000 ][hidden: 10 ][file: bhavul_tr_acc_0.76_prediction.csv ] ACCURACY :  0.755331\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  368.505 , training accuracy :  0.384961\n",
      "i: 100 , cost :  0.889888 , training accuracy :  0.695847\n",
      "i: 200 , cost :  0.813451 , training accuracy :  0.588103\n",
      "i: 300 , cost :  0.501258 , training accuracy :  0.777778\n",
      "i: 400 , cost :  0.442632 , training accuracy :  0.801347\n",
      "i: 500 , cost :  0.479992 , training accuracy :  0.784512\n",
      "i: 600 , cost :  0.482285 , training accuracy :  0.782267\n",
      "i: 700 , cost :  0.479158 , training accuracy :  0.782267\n",
      "i: 800 , cost :  0.475333 , training accuracy :  0.784512\n",
      "i: 900 , cost :  0.496495 , training accuracy :  0.777778\n",
      "i: 1000 , cost :  0.430064 , training accuracy :  0.809203\n",
      "i: 1100 , cost :  0.43258 , training accuracy :  0.804714\n",
      "i: 1200 , cost :  0.440087 , training accuracy :  0.805836\n",
      "i: 1300 , cost :  0.420074 , training accuracy :  0.813693\n",
      "i: 1400 , cost :  0.418807 , training accuracy :  0.815937\n",
      "i: 1500 , cost :  0.447129 , training accuracy :  0.792368\n",
      "i: 1600 , cost :  0.456735 , training accuracy :  0.784512\n",
      "i: 1700 , cost :  0.442429 , training accuracy :  0.791246\n",
      "i: 1800 , cost :  0.463673 , training accuracy :  0.79349\n",
      "i: 1900 , cost :  0.421515 , training accuracy :  0.81257\n",
      "i: 2000 , cost :  0.449372 , training accuracy :  0.786756\n",
      "i: 2100 , cost :  0.478811 , training accuracy :  0.782267\n",
      "i: 2200 , cost :  0.428973 , training accuracy :  0.817059\n",
      "i: 2300 , cost :  0.43999 , training accuracy :  0.815937\n",
      "i: 2400 , cost :  0.420786 , training accuracy :  0.811448\n",
      "i: 2500 , cost :  0.435728 , training accuracy :  0.795735\n",
      "i: 2600 , cost :  0.417508 , training accuracy :  0.81257\n",
      "i: 2700 , cost :  0.413189 , training accuracy :  0.820426\n",
      "i: 2800 , cost :  0.412756 , training accuracy :  0.821549\n",
      "i: 2900 , cost :  0.412371 , training accuracy :  0.821549\n",
      "i: 3000 , cost :  0.414114 , training accuracy :  0.817059\n",
      "i: 3100 , cost :  0.418299 , training accuracy :  0.809203\n",
      "i: 3200 , cost :  0.45585 , training accuracy :  0.802469\n",
      "i: 3300 , cost :  0.428896 , training accuracy :  0.803591\n",
      "i: 3400 , cost :  0.413725 , training accuracy :  0.815937\n",
      "i: 3500 , cost :  0.630271 , training accuracy :  0.741863\n",
      "i: 3600 , cost :  0.413003 , training accuracy :  0.817059\n",
      "i: 3700 , cost :  0.410963 , training accuracy :  0.818182\n",
      "i: 3800 , cost :  0.410364 , training accuracy :  0.819304\n",
      "i: 3900 , cost :  0.409987 , training accuracy :  0.819304\n",
      "i: 4000 , cost :  0.409705 , training accuracy :  0.817059\n",
      "i: 4100 , cost :  0.414718 , training accuracy :  0.819304\n",
      "i: 4200 , cost :  0.410535 , training accuracy :  0.820426\n",
      "i: 4300 , cost :  0.409471 , training accuracy :  0.818182\n",
      "i: 4400 , cost :  0.417107 , training accuracy :  0.814815\n",
      "i: 4500 , cost :  0.409825 , training accuracy :  0.815937\n",
      "i: 4600 , cost :  0.409003 , training accuracy :  0.819304\n",
      "i: 4700 , cost :  0.41624 , training accuracy :  0.813693\n",
      "i: 4800 , cost :  0.413483 , training accuracy :  0.818182\n",
      "i: 4900 , cost :  0.407628 , training accuracy :  0.818182\n",
      "Final training accuracy :  0.814815\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 5000 ][hidden: 15 ][file: bhavul_tr_acc_0.81_prediction.csv ] ACCURACY :  0.814815\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  313.519 , training accuracy :  0.554433\n",
      "i: 100 , cost :  3.45824 , training accuracy :  0.780022\n",
      "i: 200 , cost :  7.64093 , training accuracy :  0.767677\n",
      "i: 300 , cost :  4.51623 , training accuracy :  0.682379\n",
      "i: 400 , cost :  3.15357 , training accuracy :  0.803591\n",
      "i: 500 , cost :  5.10596 , training accuracy :  0.748597\n",
      "i: 600 , cost :  3.16058 , training accuracy :  0.782267\n",
      "i: 700 , cost :  3.50556 , training accuracy :  0.777778\n",
      "i: 800 , cost :  9.04899 , training accuracy :  0.720539\n",
      "i: 900 , cost :  2.87287 , training accuracy :  0.810326\n",
      "i: 1000 , cost :  1.77219 , training accuracy :  0.83165\n",
      "i: 1100 , cost :  2.87476 , training accuracy :  0.787879\n",
      "i: 1200 , cost :  2.98574 , training accuracy :  0.776655\n",
      "i: 1300 , cost :  5.18208 , training accuracy :  0.763187\n",
      "i: 1400 , cost :  1.97652 , training accuracy :  0.826038\n",
      "i: 1500 , cost :  15.6951 , training accuracy :  0.406285\n",
      "i: 1600 , cost :  2.78778 , training accuracy :  0.794613\n",
      "i: 1700 , cost :  3.48216 , training accuracy :  0.738496\n",
      "i: 1800 , cost :  1.26066 , training accuracy :  0.832772\n",
      "i: 1900 , cost :  2.97846 , training accuracy :  0.709315\n",
      "i: 2000 , cost :  5.36648 , training accuracy :  0.746352\n",
      "i: 2100 , cost :  1.14712 , training accuracy :  0.83165\n",
      "i: 2200 , cost :  2.17485 , training accuracy :  0.765432\n",
      "i: 2300 , cost :  7.69327 , training accuracy :  0.690236\n",
      "i: 2400 , cost :  5.72497 , training accuracy :  0.744108\n",
      "i: 2500 , cost :  1.03143 , training accuracy :  0.859708\n",
      "i: 2600 , cost :  0.773143 , training accuracy :  0.849607\n",
      "i: 2700 , cost :  3.81088 , training accuracy :  0.766554\n",
      "i: 2800 , cost :  1.31714 , training accuracy :  0.849607\n",
      "i: 2900 , cost :  4.71579 , training accuracy :  0.72615\n",
      "i: 3000 , cost :  3.28471 , training accuracy :  0.742985\n",
      "i: 3100 , cost :  0.814902 , training accuracy :  0.864198\n",
      "i: 3200 , cost :  0.678769 , training accuracy :  0.822671\n",
      "i: 3300 , cost :  3.09809 , training accuracy :  0.76431\n",
      "i: 3400 , cost :  0.862231 , training accuracy :  0.832772\n",
      "i: 3500 , cost :  0.551763 , training accuracy :  0.859708\n",
      "i: 3600 , cost :  2.26842 , training accuracy :  0.784512\n",
      "i: 3700 , cost :  0.782712 , training accuracy :  0.837261\n",
      "i: 3800 , cost :  0.456748 , training accuracy :  0.86532\n",
      "i: 3900 , cost :  0.857628 , training accuracy :  0.832772\n",
      "i: 4000 , cost :  0.58263 , training accuracy :  0.858586\n",
      "i: 4100 , cost :  0.760124 , training accuracy :  0.845118\n",
      "i: 4200 , cost :  0.632747 , training accuracy :  0.858586\n",
      "i: 4300 , cost :  0.480661 , training accuracy :  0.868687\n",
      "i: 4400 , cost :  0.43259 , training accuracy :  0.852974\n",
      "i: 4500 , cost :  0.594467 , training accuracy :  0.859708\n",
      "i: 4600 , cost :  0.387014 , training accuracy :  0.868687\n",
      "i: 4700 , cost :  0.921026 , training accuracy :  0.782267\n",
      "i: 4800 , cost :  0.401555 , training accuracy :  0.869809\n",
      "i: 4900 , cost :  1.25246 , training accuracy :  0.757576\n",
      "Final training accuracy :  0.873176\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 5000 ][hidden: 50 ][file: bhavul_tr_acc_0.87_prediction.csv ] ACCURACY :  0.873176\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  691.545 , training accuracy :  0.387205\n",
      "i: 100 , cost :  15.6401 , training accuracy :  0.682379\n",
      "i: 200 , cost :  18.4664 , training accuracy :  0.488215\n",
      "i: 300 , cost :  29.4107 , training accuracy :  0.529742\n",
      "i: 400 , cost :  5.55243 , training accuracy :  0.777778\n",
      "i: 500 , cost :  11.3944 , training accuracy :  0.758698\n",
      "i: 600 , cost :  6.98809 , training accuracy :  0.787879\n",
      "i: 700 , cost :  27.9902 , training accuracy :  0.446689\n",
      "i: 800 , cost :  4.58334 , training accuracy :  0.830527\n",
      "i: 900 , cost :  17.792 , training accuracy :  0.72615\n",
      "i: 1000 , cost :  11.0061 , training accuracy :  0.721661\n",
      "i: 1100 , cost :  14.5744 , training accuracy :  0.774411\n",
      "i: 1200 , cost :  10.9587 , training accuracy :  0.76431\n",
      "i: 1300 , cost :  2.86611 , training accuracy :  0.837261\n",
      "i: 1400 , cost :  6.43288 , training accuracy :  0.784512\n",
      "i: 1500 , cost :  6.11831 , training accuracy :  0.804714\n",
      "i: 1600 , cost :  2.59759 , training accuracy :  0.841751\n",
      "i: 1700 , cost :  7.47643 , training accuracy :  0.774411\n",
      "i: 1800 , cost :  8.02136 , training accuracy :  0.800224\n",
      "i: 1900 , cost :  3.65222 , training accuracy :  0.839506\n",
      "i: 2000 , cost :  2.84259 , training accuracy :  0.840629\n",
      "i: 2100 , cost :  2.49781 , training accuracy :  0.826038\n",
      "i: 2200 , cost :  2.37688 , training accuracy :  0.841751\n",
      "i: 2300 , cost :  6.6111 , training accuracy :  0.796857\n",
      "i: 2400 , cost :  13.2903 , training accuracy :  0.737374\n",
      "i: 2500 , cost :  9.86173 , training accuracy :  0.720539\n",
      "i: 2600 , cost :  2.30561 , training accuracy :  0.84624\n",
      "i: 2700 , cost :  1.70164 , training accuracy :  0.826038\n",
      "i: 2800 , cost :  11.3929 , training accuracy :  0.748597\n",
      "i: 2900 , cost :  2.1058 , training accuracy :  0.811448\n",
      "i: 3000 , cost :  5.13936 , training accuracy :  0.691358\n",
      "i: 3100 , cost :  1.66829 , training accuracy :  0.849607\n",
      "i: 3200 , cost :  3.60287 , training accuracy :  0.82716\n",
      "i: 3300 , cost :  1.78872 , training accuracy :  0.85073\n",
      "i: 3400 , cost :  9.39017 , training accuracy :  0.780022\n",
      "i: 3500 , cost :  8.78116 , training accuracy :  0.619529\n",
      "i: 3600 , cost :  1.59179 , training accuracy :  0.859708\n",
      "i: 3700 , cost :  12.4432 , training accuracy :  0.739618\n",
      "i: 3800 , cost :  2.12435 , training accuracy :  0.845118\n",
      "i: 3900 , cost :  1.19985 , training accuracy :  0.857464\n",
      "i: 4000 , cost :  6.24006 , training accuracy :  0.796857\n",
      "i: 4100 , cost :  1.69529 , training accuracy :  0.847363\n",
      "i: 4200 , cost :  0.957246 , training accuracy :  0.847363\n",
      "i: 4300 , cost :  9.00334 , training accuracy :  0.563412\n",
      "i: 4400 , cost :  1.91743 , training accuracy :  0.849607\n",
      "i: 4500 , cost :  1.32421 , training accuracy :  0.801347\n",
      "i: 4600 , cost :  1.54979 , training accuracy :  0.841751\n",
      "i: 4700 , cost :  2.92935 , training accuracy :  0.803591\n",
      "i: 4800 , cost :  0.895801 , training accuracy :  0.849607\n",
      "i: 4900 , cost :  1.63957 , training accuracy :  0.832772\n",
      "Final training accuracy :  0.855219\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 5000 ][hidden: 100 ][file: bhavul_tr_acc_0.86_prediction.csv ] ACCURACY :  0.855219\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  140.5 , training accuracy :  0.649832\n",
      "i: 100 , cost :  6.72118 , training accuracy :  0.419753\n",
      "i: 200 , cost :  2.14545 , training accuracy :  0.689113\n",
      "i: 300 , cost :  1.30014 , training accuracy :  0.722783\n",
      "i: 400 , cost :  0.941029 , training accuracy :  0.754209\n",
      "i: 500 , cost :  2.13444 , training accuracy :  0.575758\n",
      "i: 600 , cost :  2.18754 , training accuracy :  0.670034\n",
      "i: 700 , cost :  1.45169 , training accuracy :  0.736251\n",
      "i: 800 , cost :  0.542338 , training accuracy :  0.7789\n",
      "i: 900 , cost :  1.81952 , training accuracy :  0.525253\n",
      "i: 1000 , cost :  0.650626 , training accuracy :  0.777778\n",
      "i: 1100 , cost :  0.709539 , training accuracy :  0.773288\n",
      "i: 1200 , cost :  1.92921 , training accuracy :  0.705948\n",
      "i: 1300 , cost :  0.571822 , training accuracy :  0.775533\n",
      "i: 1400 , cost :  1.9603 , training accuracy :  0.686869\n",
      "i: 1500 , cost :  1.59491 , training accuracy :  0.699214\n",
      "i: 1600 , cost :  0.523245 , training accuracy :  0.777778\n",
      "i: 1700 , cost :  6.52524 , training accuracy :  0.67789\n",
      "i: 1800 , cost :  0.717112 , training accuracy :  0.776655\n",
      "i: 1900 , cost :  0.523799 , training accuracy :  0.768799\n",
      "i: 2000 , cost :  3.12572 , training accuracy :  0.671156\n",
      "i: 2100 , cost :  0.79698 , training accuracy :  0.707071\n",
      "i: 2200 , cost :  0.661223 , training accuracy :  0.75982\n",
      "i: 2300 , cost :  0.516964 , training accuracy :  0.772166\n",
      "i: 2400 , cost :  1.18703 , training accuracy :  0.695847\n",
      "i: 2500 , cost :  0.481351 , training accuracy :  0.768799\n",
      "i: 2600 , cost :  1.18863 , training accuracy :  0.69248\n",
      "i: 2700 , cost :  0.812101 , training accuracy :  0.687991\n",
      "i: 2800 , cost :  0.547134 , training accuracy :  0.765432\n",
      "i: 2900 , cost :  0.842489 , training accuracy :  0.659933\n",
      "i: 3000 , cost :  0.871051 , training accuracy :  0.721661\n",
      "i: 3100 , cost :  0.768793 , training accuracy :  0.748597\n",
      "i: 3200 , cost :  0.532555 , training accuracy :  0.767677\n",
      "i: 3300 , cost :  0.800078 , training accuracy :  0.682379\n",
      "i: 3400 , cost :  0.812664 , training accuracy :  0.727273\n",
      "i: 3500 , cost :  0.69933 , training accuracy :  0.755331\n",
      "i: 3600 , cost :  0.525195 , training accuracy :  0.775533\n",
      "i: 3700 , cost :  0.713885 , training accuracy :  0.703704\n",
      "i: 3800 , cost :  0.747499 , training accuracy :  0.746352\n",
      "i: 3900 , cost :  0.63347 , training accuracy :  0.758698\n",
      "i: 4000 , cost :  0.488831 , training accuracy :  0.791246\n",
      "i: 4100 , cost :  0.689695 , training accuracy :  0.712682\n",
      "i: 4200 , cost :  0.701622 , training accuracy :  0.739618\n",
      "i: 4300 , cost :  0.649942 , training accuracy :  0.760943\n",
      "i: 4400 , cost :  0.659246 , training accuracy :  0.721661\n",
      "i: 4500 , cost :  0.469795 , training accuracy :  0.795735\n",
      "i: 4600 , cost :  0.456174 , training accuracy :  0.789001\n",
      "i: 4700 , cost :  0.47272 , training accuracy :  0.780022\n",
      "i: 4800 , cost :  0.519708 , training accuracy :  0.763187\n",
      "i: 4900 , cost :  0.489516 , training accuracy :  0.76431\n",
      "i: 5000 , cost :  0.478414 , training accuracy :  0.776655\n",
      "i: 5100 , cost :  0.715064 , training accuracy :  0.731762\n",
      "i: 5200 , cost :  0.526341 , training accuracy :  0.750842\n",
      "i: 5300 , cost :  0.469666 , training accuracy :  0.79349\n",
      "i: 5400 , cost :  0.454084 , training accuracy :  0.792368\n",
      "i: 5500 , cost :  0.45003 , training accuracy :  0.795735\n",
      "i: 5600 , cost :  0.475974 , training accuracy :  0.775533\n",
      "i: 5700 , cost :  0.482403 , training accuracy :  0.772166\n",
      "i: 5800 , cost :  0.456047 , training accuracy :  0.792368\n",
      "i: 5900 , cost :  0.454639 , training accuracy :  0.799102\n",
      "i: 6000 , cost :  0.483971 , training accuracy :  0.787879\n",
      "i: 6100 , cost :  0.878676 , training accuracy :  0.737374\n",
      "i: 6200 , cost :  0.542355 , training accuracy :  0.744108\n",
      "i: 6300 , cost :  0.517981 , training accuracy :  0.74523\n",
      "i: 6400 , cost :  0.496531 , training accuracy :  0.751964\n",
      "i: 6500 , cost :  0.493814 , training accuracy :  0.758698\n",
      "i: 6600 , cost :  0.493271 , training accuracy :  0.760943\n",
      "i: 6700 , cost :  0.493028 , training accuracy :  0.758698\n",
      "i: 6800 , cost :  0.492825 , training accuracy :  0.758698\n",
      "i: 6900 , cost :  0.49264 , training accuracy :  0.757576\n",
      "i: 7000 , cost :  0.503814 , training accuracy :  0.741863\n",
      "i: 7100 , cost :  0.518428 , training accuracy :  0.732884\n",
      "i: 7200 , cost :  0.49873 , training accuracy :  0.772166\n",
      "i: 7300 , cost :  0.564552 , training accuracy :  0.741863\n",
      "i: 7400 , cost :  0.533867 , training accuracy :  0.727273\n",
      "i: 7500 , cost :  0.496385 , training accuracy :  0.765432\n",
      "i: 7600 , cost :  0.53996 , training accuracy :  0.749719\n",
      "i: 7700 , cost :  0.525851 , training accuracy :  0.73064\n",
      "i: 7800 , cost :  0.499459 , training accuracy :  0.746352\n",
      "i: 7900 , cost :  0.556058 , training accuracy :  0.744108\n",
      "i: 8000 , cost :  0.498863 , training accuracy :  0.765432\n",
      "i: 8100 , cost :  0.518964 , training accuracy :  0.736251\n",
      "i: 8200 , cost :  0.527862 , training accuracy :  0.728395\n",
      "i: 8300 , cost :  0.494618 , training accuracy :  0.747475\n",
      "i: 8400 , cost :  0.502803 , training accuracy :  0.763187\n",
      "i: 8500 , cost :  0.546323 , training accuracy :  0.749719\n",
      "i: 8600 , cost :  0.555167 , training accuracy :  0.74523\n",
      "i: 8700 , cost :  0.520701 , training accuracy :  0.758698\n",
      "i: 8800 , cost :  0.505773 , training accuracy :  0.757576\n",
      "i: 8900 , cost :  0.499363 , training accuracy :  0.76431\n",
      "i: 9000 , cost :  0.497475 , training accuracy :  0.765432\n",
      "i: 9100 , cost :  0.49941 , training accuracy :  0.763187\n",
      "i: 9200 , cost :  0.50133 , training accuracy :  0.760943\n",
      "i: 9300 , cost :  0.506613 , training accuracy :  0.763187\n",
      "i: 9400 , cost :  0.520121 , training accuracy :  0.758698\n",
      "i: 9500 , cost :  0.532327 , training accuracy :  0.750842\n",
      "i: 9600 , cost :  0.543928 , training accuracy :  0.747475\n",
      "i: 9700 , cost :  0.537243 , training accuracy :  0.746352\n",
      "i: 9800 , cost :  0.523112 , training accuracy :  0.756453\n",
      "i: 9900 , cost :  0.511549 , training accuracy :  0.763187\n",
      "Final training accuracy :  0.769921\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 10000 ][hidden: 3 ][file: bhavul_tr_acc_0.77_prediction.csv ] ACCURACY :  0.769921\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  702.32 , training accuracy :  0.383838\n",
      "i: 100 , cost :  2.05332 , training accuracy :  0.671156\n",
      "i: 200 , cost :  0.630867 , training accuracy :  0.760943\n",
      "i: 300 , cost :  0.495002 , training accuracy :  0.787879\n",
      "i: 400 , cost :  0.577657 , training accuracy :  0.768799\n",
      "i: 500 , cost :  0.671174 , training accuracy :  0.739618\n",
      "i: 600 , cost :  0.461778 , training accuracy :  0.79349\n",
      "i: 700 , cost :  0.453504 , training accuracy :  0.796857\n",
      "i: 800 , cost :  0.805683 , training accuracy :  0.712682\n",
      "i: 900 , cost :  0.450552 , training accuracy :  0.796857\n",
      "i: 1000 , cost :  0.446483 , training accuracy :  0.795735\n",
      "i: 1100 , cost :  0.458494 , training accuracy :  0.802469\n",
      "i: 1200 , cost :  0.45226 , training accuracy :  0.800224\n",
      "i: 1300 , cost :  0.441788 , training accuracy :  0.800224\n",
      "i: 1400 , cost :  0.441887 , training accuracy :  0.804714\n",
      "i: 1500 , cost :  1.09949 , training accuracy :  0.638608\n",
      "i: 1600 , cost :  0.43892 , training accuracy :  0.808081\n",
      "i: 1700 , cost :  0.436459 , training accuracy :  0.810326\n",
      "i: 1800 , cost :  0.460323 , training accuracy :  0.801347\n",
      "i: 1900 , cost :  0.427684 , training accuracy :  0.806958\n",
      "i: 2000 , cost :  0.42502 , training accuracy :  0.811448\n",
      "i: 2100 , cost :  0.423208 , training accuracy :  0.815937\n",
      "i: 2200 , cost :  0.421607 , training accuracy :  0.815937\n",
      "i: 2300 , cost :  0.420665 , training accuracy :  0.813693\n",
      "i: 2400 , cost :  0.419879 , training accuracy :  0.81257\n",
      "i: 2500 , cost :  0.418951 , training accuracy :  0.811448\n",
      "i: 2600 , cost :  0.4426 , training accuracy :  0.803591\n",
      "i: 2700 , cost :  0.417421 , training accuracy :  0.820426\n",
      "i: 2800 , cost :  0.416898 , training accuracy :  0.811448\n",
      "i: 2900 , cost :  0.434433 , training accuracy :  0.803591\n",
      "i: 3000 , cost :  0.414401 , training accuracy :  0.818182\n",
      "i: 3100 , cost :  0.417016 , training accuracy :  0.820426\n",
      "i: 3200 , cost :  0.412545 , training accuracy :  0.817059\n",
      "i: 3300 , cost :  0.412149 , training accuracy :  0.818182\n",
      "i: 3400 , cost :  0.429505 , training accuracy :  0.806958\n",
      "i: 3500 , cost :  0.413593 , training accuracy :  0.821549\n",
      "i: 3600 , cost :  0.412351 , training accuracy :  0.819304\n",
      "i: 3700 , cost :  0.422719 , training accuracy :  0.815937\n",
      "i: 3800 , cost :  0.412454 , training accuracy :  0.813693\n",
      "i: 3900 , cost :  0.440809 , training accuracy :  0.808081\n",
      "i: 4000 , cost :  0.434795 , training accuracy :  0.803591\n",
      "i: 4100 , cost :  0.42292 , training accuracy :  0.815937\n",
      "i: 4200 , cost :  0.408413 , training accuracy :  0.81257\n",
      "i: 4300 , cost :  0.419421 , training accuracy :  0.818182\n",
      "i: 4400 , cost :  0.407391 , training accuracy :  0.813693\n",
      "i: 4500 , cost :  0.444805 , training accuracy :  0.803591\n",
      "i: 4600 , cost :  0.412264 , training accuracy :  0.822671\n",
      "i: 4700 , cost :  0.405411 , training accuracy :  0.823793\n",
      "i: 4800 , cost :  0.46598 , training accuracy :  0.794613\n",
      "i: 4900 , cost :  0.404376 , training accuracy :  0.818182\n",
      "i: 5000 , cost :  0.410351 , training accuracy :  0.811448\n",
      "i: 5100 , cost :  0.419437 , training accuracy :  0.809203\n",
      "i: 5200 , cost :  0.407254 , training accuracy :  0.823793\n",
      "i: 5300 , cost :  0.405083 , training accuracy :  0.817059\n",
      "i: 5400 , cost :  0.454794 , training accuracy :  0.790123\n",
      "i: 5500 , cost :  0.418335 , training accuracy :  0.808081\n",
      "i: 5600 , cost :  0.419866 , training accuracy :  0.804714\n",
      "i: 5700 , cost :  0.403222 , training accuracy :  0.822671\n",
      "i: 5800 , cost :  0.407796 , training accuracy :  0.817059\n",
      "i: 5900 , cost :  0.406891 , training accuracy :  0.818182\n",
      "i: 6000 , cost :  0.399539 , training accuracy :  0.822671\n",
      "i: 6100 , cost :  0.40779 , training accuracy :  0.813693\n",
      "i: 6200 , cost :  0.396367 , training accuracy :  0.829405\n",
      "i: 6300 , cost :  0.408821 , training accuracy :  0.814815\n",
      "i: 6400 , cost :  0.408547 , training accuracy :  0.815937\n",
      "i: 6500 , cost :  0.392671 , training accuracy :  0.832772\n",
      "i: 6600 , cost :  0.395272 , training accuracy :  0.823793\n",
      "i: 6700 , cost :  0.3945 , training accuracy :  0.824916\n",
      "i: 6800 , cost :  0.393181 , training accuracy :  0.828283\n",
      "i: 6900 , cost :  0.392626 , training accuracy :  0.828283\n",
      "i: 7000 , cost :  0.39217 , training accuracy :  0.828283\n",
      "i: 7100 , cost :  0.391758 , training accuracy :  0.824916\n",
      "i: 7200 , cost :  0.393504 , training accuracy :  0.826038\n",
      "i: 7300 , cost :  0.392391 , training accuracy :  0.821549\n",
      "i: 7400 , cost :  0.391496 , training accuracy :  0.826038\n",
      "i: 7500 , cost :  0.407661 , training accuracy :  0.817059\n",
      "i: 7600 , cost :  0.401992 , training accuracy :  0.818182\n",
      "i: 7700 , cost :  0.391421 , training accuracy :  0.822671\n",
      "i: 7800 , cost :  0.394594 , training accuracy :  0.830527\n",
      "i: 7900 , cost :  0.399475 , training accuracy :  0.82716\n",
      "i: 8000 , cost :  0.396019 , training accuracy :  0.824916\n",
      "i: 8100 , cost :  0.387656 , training accuracy :  0.82716\n",
      "i: 8200 , cost :  0.38948 , training accuracy :  0.822671\n",
      "i: 8300 , cost :  0.392436 , training accuracy :  0.832772\n",
      "i: 8400 , cost :  0.398144 , training accuracy :  0.823793\n",
      "i: 8500 , cost :  0.406674 , training accuracy :  0.820426\n",
      "i: 8600 , cost :  0.389538 , training accuracy :  0.832772\n",
      "i: 8700 , cost :  0.399971 , training accuracy :  0.823793\n",
      "i: 8800 , cost :  0.390898 , training accuracy :  0.823793\n",
      "i: 8900 , cost :  0.412417 , training accuracy :  0.820426\n",
      "i: 9000 , cost :  0.424283 , training accuracy :  0.823793\n",
      "i: 9100 , cost :  0.386156 , training accuracy :  0.842873\n",
      "i: 9200 , cost :  0.383246 , training accuracy :  0.832772\n",
      "i: 9300 , cost :  0.381077 , training accuracy :  0.835017\n",
      "i: 9400 , cost :  0.382111 , training accuracy :  0.835017\n",
      "i: 9500 , cost :  0.405022 , training accuracy :  0.821549\n",
      "i: 9600 , cost :  0.379913 , training accuracy :  0.837261\n",
      "i: 9700 , cost :  0.400868 , training accuracy :  0.823793\n",
      "i: 9800 , cost :  0.393622 , training accuracy :  0.830527\n",
      "i: 9900 , cost :  0.38838 , training accuracy :  0.826038\n",
      "Final training accuracy :  0.833894\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 10000 ][hidden: 10 ][file: bhavul_tr_acc_0.83_prediction.csv ] ACCURACY :  0.833894\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  62.5529 , training accuracy :  0.618406\n",
      "i: 100 , cost :  3.45781 , training accuracy :  0.683502\n",
      "i: 200 , cost :  0.805126 , training accuracy :  0.792368\n",
      "i: 300 , cost :  1.75879 , training accuracy :  0.742985\n",
      "i: 400 , cost :  0.676483 , training accuracy :  0.801347\n",
      "i: 500 , cost :  1.84922 , training accuracy :  0.641975\n",
      "i: 600 , cost :  0.837707 , training accuracy :  0.757576\n",
      "i: 700 , cost :  0.871046 , training accuracy :  0.758698\n",
      "i: 800 , cost :  2.01676 , training accuracy :  0.694725\n",
      "i: 900 , cost :  0.799498 , training accuracy :  0.800224\n",
      "i: 1000 , cost :  0.792295 , training accuracy :  0.773288\n",
      "i: 1100 , cost :  1.88638 , training accuracy :  0.598204\n",
      "i: 1200 , cost :  5.10463 , training accuracy :  0.388328\n",
      "i: 1300 , cost :  2.6807 , training accuracy :  0.476992\n",
      "i: 1400 , cost :  1.25285 , training accuracy :  0.754209\n",
      "i: 1500 , cost :  0.478144 , training accuracy :  0.809203\n",
      "i: 1600 , cost :  0.954892 , training accuracy :  0.768799\n",
      "i: 1700 , cost :  0.506161 , training accuracy :  0.809203\n",
      "i: 1800 , cost :  5.46417 , training accuracy :  0.392817\n",
      "i: 1900 , cost :  1.63047 , training accuracy :  0.634119\n",
      "i: 2000 , cost :  0.527064 , training accuracy :  0.815937\n",
      "i: 2100 , cost :  0.677234 , training accuracy :  0.738496\n",
      "i: 2200 , cost :  1.55317 , training accuracy :  0.742985\n",
      "i: 2300 , cost :  0.499392 , training accuracy :  0.820426\n",
      "i: 2400 , cost :  0.526687 , training accuracy :  0.795735\n",
      "i: 2500 , cost :  0.507394 , training accuracy :  0.824916\n",
      "i: 2600 , cost :  0.861283 , training accuracy :  0.690236\n",
      "i: 2700 , cost :  0.705175 , training accuracy :  0.803591\n",
      "i: 2800 , cost :  0.456728 , training accuracy :  0.818182\n",
      "i: 2900 , cost :  2.39612 , training accuracy :  0.710438\n",
      "i: 3000 , cost :  0.779011 , training accuracy :  0.784512\n",
      "i: 3100 , cost :  0.441675 , training accuracy :  0.82716\n",
      "i: 3200 , cost :  0.734758 , training accuracy :  0.777778\n",
      "i: 3300 , cost :  0.4648 , training accuracy :  0.83165\n",
      "i: 3400 , cost :  0.40464 , training accuracy :  0.830527\n",
      "i: 3500 , cost :  1.30392 , training accuracy :  0.756453\n",
      "i: 3600 , cost :  0.446476 , training accuracy :  0.835017\n",
      "i: 3700 , cost :  0.400744 , training accuracy :  0.837261\n",
      "i: 3800 , cost :  0.39941 , training accuracy :  0.836139\n",
      "i: 3900 , cost :  0.426365 , training accuracy :  0.828283\n",
      "i: 4000 , cost :  0.769722 , training accuracy :  0.755331\n",
      "i: 4100 , cost :  1.34436 , training accuracy :  0.52862\n",
      "i: 4200 , cost :  0.408066 , training accuracy :  0.835017\n",
      "i: 4300 , cost :  0.389918 , training accuracy :  0.836139\n",
      "i: 4400 , cost :  0.421186 , training accuracy :  0.818182\n",
      "i: 4500 , cost :  0.495728 , training accuracy :  0.809203\n",
      "i: 4600 , cost :  0.391077 , training accuracy :  0.836139\n",
      "i: 4700 , cost :  0.405605 , training accuracy :  0.83165\n",
      "i: 4800 , cost :  0.399344 , training accuracy :  0.83165\n",
      "i: 4900 , cost :  0.459358 , training accuracy :  0.79798\n",
      "i: 5000 , cost :  0.68443 , training accuracy :  0.755331\n",
      "i: 5100 , cost :  0.519981 , training accuracy :  0.796857\n",
      "i: 5200 , cost :  0.46616 , training accuracy :  0.795735\n",
      "i: 5300 , cost :  0.422692 , training accuracy :  0.818182\n",
      "i: 5400 , cost :  0.415932 , training accuracy :  0.815937\n",
      "i: 5500 , cost :  0.419976 , training accuracy :  0.805836\n",
      "i: 5600 , cost :  0.414836 , training accuracy :  0.817059\n",
      "i: 5700 , cost :  0.47884 , training accuracy :  0.783389\n",
      "i: 5800 , cost :  0.441831 , training accuracy :  0.813693\n",
      "i: 5900 , cost :  0.457013 , training accuracy :  0.791246\n",
      "i: 6000 , cost :  0.545746 , training accuracy :  0.775533\n",
      "i: 6100 , cost :  0.411326 , training accuracy :  0.821549\n",
      "i: 6200 , cost :  0.483925 , training accuracy :  0.783389\n",
      "i: 6300 , cost :  0.412783 , training accuracy :  0.819304\n",
      "i: 6400 , cost :  0.42231 , training accuracy :  0.805836\n",
      "i: 6500 , cost :  0.410066 , training accuracy :  0.811448\n",
      "i: 6600 , cost :  0.456182 , training accuracy :  0.813693\n",
      "i: 6700 , cost :  0.426776 , training accuracy :  0.809203\n",
      "i: 6800 , cost :  0.412345 , training accuracy :  0.828283\n",
      "i: 6900 , cost :  0.404225 , training accuracy :  0.818182\n",
      "i: 7000 , cost :  0.424335 , training accuracy :  0.809203\n",
      "i: 7100 , cost :  0.433669 , training accuracy :  0.815937\n",
      "i: 7200 , cost :  0.4235 , training accuracy :  0.811448\n",
      "i: 7300 , cost :  0.407447 , training accuracy :  0.828283\n",
      "i: 7400 , cost :  0.398674 , training accuracy :  0.828283\n",
      "i: 7500 , cost :  0.395489 , training accuracy :  0.833894\n",
      "i: 7600 , cost :  0.394006 , training accuracy :  0.824916\n",
      "i: 7700 , cost :  0.406966 , training accuracy :  0.813693\n",
      "i: 7800 , cost :  0.432075 , training accuracy :  0.81257\n",
      "i: 7900 , cost :  0.42924 , training accuracy :  0.803591\n",
      "i: 8000 , cost :  0.424291 , training accuracy :  0.817059\n",
      "i: 8100 , cost :  0.39706 , training accuracy :  0.824916\n",
      "i: 8200 , cost :  0.397469 , training accuracy :  0.821549\n",
      "i: 8300 , cost :  0.415402 , training accuracy :  0.826038\n",
      "i: 8400 , cost :  0.410688 , training accuracy :  0.814815\n",
      "i: 8500 , cost :  0.391193 , training accuracy :  0.835017\n",
      "i: 8600 , cost :  0.423787 , training accuracy :  0.818182\n",
      "i: 8700 , cost :  0.392288 , training accuracy :  0.83165\n",
      "i: 8800 , cost :  0.412961 , training accuracy :  0.809203\n",
      "i: 8900 , cost :  0.406168 , training accuracy :  0.815937\n",
      "i: 9000 , cost :  0.394304 , training accuracy :  0.823793\n",
      "i: 9100 , cost :  0.416999 , training accuracy :  0.805836\n",
      "i: 9200 , cost :  0.388963 , training accuracy :  0.83165\n",
      "i: 9300 , cost :  0.452427 , training accuracy :  0.809203\n",
      "i: 9400 , cost :  0.388427 , training accuracy :  0.835017\n",
      "i: 9500 , cost :  0.388109 , training accuracy :  0.836139\n",
      "i: 9600 , cost :  0.392989 , training accuracy :  0.823793\n",
      "i: 9700 , cost :  0.389582 , training accuracy :  0.833894\n",
      "i: 9800 , cost :  0.396489 , training accuracy :  0.824916\n",
      "i: 9900 , cost :  0.38844 , training accuracy :  0.835017\n",
      "Final training accuracy :  0.835017\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 10000 ][hidden: 15 ][file: bhavul_tr_acc_0.84_prediction.csv ] ACCURACY :  0.835017\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  124.808 , training accuracy :  0.405163\n",
      "i: 100 , cost :  13.138 , training accuracy :  0.69697\n",
      "i: 200 , cost :  5.13989 , training accuracy :  0.740741\n",
      "i: 300 , cost :  11.0476 , training accuracy :  0.693603\n",
      "i: 400 , cost :  7.28251 , training accuracy :  0.757576\n",
      "i: 500 , cost :  4.06102 , training accuracy :  0.773288\n",
      "i: 600 , cost :  4.69284 , training accuracy :  0.757576\n",
      "i: 700 , cost :  5.19457 , training accuracy :  0.765432\n",
      "i: 800 , cost :  3.63666 , training accuracy :  0.735129\n",
      "i: 900 , cost :  2.31612 , training accuracy :  0.809203\n",
      "i: 1000 , cost :  3.96875 , training accuracy :  0.775533\n",
      "i: 1100 , cost :  1.74797 , training accuracy :  0.753086\n",
      "i: 1200 , cost :  8.3709 , training accuracy :  0.735129\n",
      "i: 1300 , cost :  7.21484 , training accuracy :  0.4422\n",
      "i: 1400 , cost :  1.42846 , training accuracy :  0.833894\n",
      "i: 1500 , cost :  5.02399 , training accuracy :  0.74523\n",
      "i: 1600 , cost :  1.01647 , training accuracy :  0.845118\n",
      "i: 1700 , cost :  2.35141 , training accuracy :  0.786756\n",
      "i: 1800 , cost :  1.09346 , training accuracy :  0.851852\n",
      "i: 1900 , cost :  2.81308 , training accuracy :  0.710438\n",
      "i: 2000 , cost :  0.953366 , training accuracy :  0.849607\n",
      "i: 2100 , cost :  0.958943 , training accuracy :  0.837261\n",
      "i: 2200 , cost :  3.72268 , training accuracy :  0.750842\n",
      "i: 2300 , cost :  0.828392 , training accuracy :  0.847363\n",
      "i: 2400 , cost :  11.6625 , training accuracy :  0.402918\n",
      "i: 2500 , cost :  0.735232 , training accuracy :  0.847363\n",
      "i: 2600 , cost :  4.56732 , training accuracy :  0.647587\n",
      "i: 2700 , cost :  0.943732 , training accuracy :  0.849607\n",
      "i: 2800 , cost :  0.604001 , training accuracy :  0.838384\n",
      "i: 2900 , cost :  3.12925 , training accuracy :  0.593715\n",
      "i: 3000 , cost :  0.629221 , training accuracy :  0.847363\n",
      "i: 3100 , cost :  6.805 , training accuracy :  0.693603\n",
      "i: 3200 , cost :  0.548296 , training accuracy :  0.854097\n",
      "i: 3300 , cost :  2.16666 , training accuracy :  0.782267\n",
      "i: 3400 , cost :  0.725518 , training accuracy :  0.849607\n",
      "i: 3500 , cost :  0.456084 , training accuracy :  0.84624\n",
      "i: 3600 , cost :  1.06907 , training accuracy :  0.806958\n",
      "i: 3700 , cost :  0.453188 , training accuracy :  0.855219\n",
      "i: 3800 , cost :  2.90862 , training accuracy :  0.459035\n",
      "i: 3900 , cost :  0.416609 , training accuracy :  0.851852\n",
      "i: 4000 , cost :  0.595165 , training accuracy :  0.829405\n",
      "i: 4100 , cost :  1.73736 , training accuracy :  0.463524\n",
      "i: 4200 , cost :  0.683461 , training accuracy :  0.804714\n",
      "i: 4300 , cost :  0.453814 , training accuracy :  0.848485\n",
      "i: 4400 , cost :  0.356762 , training accuracy :  0.859708\n",
      "i: 4500 , cost :  1.66052 , training accuracy :  0.718294\n",
      "i: 4600 , cost :  0.365179 , training accuracy :  0.861953\n",
      "i: 4700 , cost :  0.334778 , training accuracy :  0.861953\n",
      "i: 4800 , cost :  0.409568 , training accuracy :  0.830527\n",
      "i: 4900 , cost :  0.415143 , training accuracy :  0.83165\n",
      "i: 5000 , cost :  0.381393 , training accuracy :  0.839506\n",
      "i: 5100 , cost :  0.442841 , training accuracy :  0.815937\n",
      "i: 5200 , cost :  0.545526 , training accuracy :  0.836139\n",
      "i: 5300 , cost :  0.359278 , training accuracy :  0.863075\n",
      "i: 5400 , cost :  0.329624 , training accuracy :  0.866442\n",
      "i: 5500 , cost :  0.350216 , training accuracy :  0.858586\n",
      "i: 5600 , cost :  0.351758 , training accuracy :  0.856341\n",
      "i: 5700 , cost :  0.35573 , training accuracy :  0.858586\n",
      "i: 5800 , cost :  0.367693 , training accuracy :  0.849607\n",
      "i: 5900 , cost :  1.47475 , training accuracy :  0.523008\n",
      "i: 6000 , cost :  0.334559 , training accuracy :  0.868687\n",
      "i: 6100 , cost :  0.319584 , training accuracy :  0.872054\n",
      "i: 6200 , cost :  0.315577 , training accuracy :  0.873176\n",
      "i: 6300 , cost :  0.313166 , training accuracy :  0.872054\n",
      "i: 6400 , cost :  0.31113 , training accuracy :  0.873176\n",
      "i: 6500 , cost :  0.310331 , training accuracy :  0.873176\n",
      "i: 6600 , cost :  0.313283 , training accuracy :  0.875421\n",
      "i: 6700 , cost :  0.331099 , training accuracy :  0.859708\n",
      "i: 6800 , cost :  0.357038 , training accuracy :  0.838384\n",
      "i: 6900 , cost :  0.398006 , training accuracy :  0.835017\n",
      "i: 7000 , cost :  0.326787 , training accuracy :  0.858586\n",
      "i: 7100 , cost :  0.654839 , training accuracy :  0.79798\n",
      "i: 7200 , cost :  0.317352 , training accuracy :  0.874299\n",
      "i: 7300 , cost :  0.306987 , training accuracy :  0.877666\n",
      "i: 7400 , cost :  0.30326 , training accuracy :  0.885522\n",
      "i: 7500 , cost :  0.300394 , training accuracy :  0.881033\n",
      "i: 7600 , cost :  0.328136 , training accuracy :  0.858586\n",
      "i: 7700 , cost :  0.357375 , training accuracy :  0.841751\n",
      "i: 7800 , cost :  0.299343 , training accuracy :  0.885522\n",
      "i: 7900 , cost :  0.335939 , training accuracy :  0.863075\n",
      "i: 8000 , cost :  0.373722 , training accuracy :  0.84624\n",
      "i: 8100 , cost :  0.323066 , training accuracy :  0.872054\n",
      "i: 8200 , cost :  0.308809 , training accuracy :  0.876543\n",
      "i: 8300 , cost :  0.302662 , training accuracy :  0.878788\n",
      "i: 8400 , cost :  0.298356 , training accuracy :  0.882155\n",
      "i: 8500 , cost :  0.295228 , training accuracy :  0.878788\n",
      "i: 8600 , cost :  0.292735 , training accuracy :  0.87991\n",
      "i: 8700 , cost :  0.290673 , training accuracy :  0.882155\n",
      "i: 8800 , cost :  0.287965 , training accuracy :  0.883277\n",
      "i: 8900 , cost :  0.286755 , training accuracy :  0.883277\n",
      "i: 9000 , cost :  0.349927 , training accuracy :  0.852974\n",
      "i: 9100 , cost :  0.282787 , training accuracy :  0.887767\n",
      "i: 9200 , cost :  0.329727 , training accuracy :  0.858586\n",
      "i: 9300 , cost :  0.544342 , training accuracy :  0.822671\n",
      "i: 9400 , cost :  0.282585 , training accuracy :  0.887767\n",
      "i: 9500 , cost :  0.280247 , training accuracy :  0.892256\n",
      "i: 9600 , cost :  0.27839 , training accuracy :  0.888889\n",
      "i: 9700 , cost :  0.319887 , training accuracy :  0.873176\n",
      "i: 9800 , cost :  0.303366 , training accuracy :  0.878788\n",
      "i: 9900 , cost :  0.588197 , training accuracy :  0.734007\n",
      "Final training accuracy :  0.8844\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 10000 ][hidden: 50 ][file: bhavul_tr_acc_0.88_prediction.csv ] ACCURACY :  0.8844\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  1826.32 , training accuracy :  0.616162\n",
      "i: 100 , cost :  81.6246 , training accuracy :  0.391695\n",
      "i: 200 , cost :  9.59429 , training accuracy :  0.647587\n",
      "i: 300 , cost :  7.41155 , training accuracy :  0.781145\n",
      "i: 400 , cost :  3.46202 , training accuracy :  0.821549\n",
      "i: 500 , cost :  4.77682 , training accuracy :  0.805836\n",
      "i: 600 , cost :  3.62779 , training accuracy :  0.821549\n",
      "i: 700 , cost :  16.7597 , training accuracy :  0.526375\n",
      "i: 800 , cost :  3.14953 , training accuracy :  0.828283\n",
      "i: 900 , cost :  18.512 , training accuracy :  0.565657\n",
      "i: 1000 , cost :  6.44496 , training accuracy :  0.787879\n",
      "i: 1100 , cost :  12.7191 , training accuracy :  0.709315\n",
      "i: 1200 , cost :  17.4886 , training accuracy :  0.746352\n",
      "i: 1300 , cost :  2.57395 , training accuracy :  0.839506\n",
      "i: 1400 , cost :  2.56437 , training accuracy :  0.83165\n",
      "i: 1500 , cost :  12.1696 , training accuracy :  0.493827\n",
      "i: 1600 , cost :  5.4704 , training accuracy :  0.810326\n",
      "i: 1700 , cost :  3.86858 , training accuracy :  0.800224\n",
      "i: 1800 , cost :  7.77889 , training accuracy :  0.74523\n",
      "i: 1900 , cost :  3.68626 , training accuracy :  0.805836\n",
      "i: 2000 , cost :  9.07109 , training accuracy :  0.737374\n",
      "i: 2100 , cost :  2.27152 , training accuracy :  0.847363\n",
      "i: 2200 , cost :  7.87837 , training accuracy :  0.791246\n",
      "i: 2300 , cost :  3.33138 , training accuracy :  0.809203\n",
      "i: 2400 , cost :  1.83893 , training accuracy :  0.836139\n",
      "i: 2500 , cost :  4.17112 , training accuracy :  0.819304\n",
      "i: 2600 , cost :  2.85116 , training accuracy :  0.847363\n",
      "i: 2700 , cost :  6.6879 , training accuracy :  0.740741\n",
      "i: 2800 , cost :  3.44555 , training accuracy :  0.79349\n",
      "i: 2900 , cost :  7.40353 , training accuracy :  0.579125\n",
      "i: 3000 , cost :  1.61984 , training accuracy :  0.869809\n",
      "i: 3100 , cost :  7.98735 , training accuracy :  0.579125\n",
      "i: 3200 , cost :  1.89289 , training accuracy :  0.867565\n",
      "i: 3300 , cost :  6.09794 , training accuracy :  0.79798\n",
      "i: 3400 , cost :  18.1652 , training accuracy :  0.742985\n",
      "i: 3500 , cost :  8.96635 , training accuracy :  0.747475\n",
      "i: 3600 , cost :  9.92027 , training accuracy :  0.751964\n",
      "i: 3700 , cost :  2.45227 , training accuracy :  0.861953\n",
      "i: 3800 , cost :  1.49647 , training accuracy :  0.877666\n",
      "i: 3900 , cost :  8.9162 , training accuracy :  0.523008\n",
      "i: 4000 , cost :  1.18989 , training accuracy :  0.873176\n",
      "i: 4100 , cost :  3.81145 , training accuracy :  0.7789\n",
      "i: 4200 , cost :  8.90909 , training accuracy :  0.523008\n",
      "i: 4300 , cost :  1.21229 , training accuracy :  0.861953\n",
      "i: 4400 , cost :  1.84618 , training accuracy :  0.856341\n",
      "i: 4500 , cost :  3.03914 , training accuracy :  0.814815\n",
      "i: 4600 , cost :  6.7676 , training accuracy :  0.763187\n",
      "i: 4700 , cost :  4.3457 , training accuracy :  0.654321\n",
      "i: 4800 , cost :  0.999743 , training accuracy :  0.873176\n",
      "i: 4900 , cost :  1.42807 , training accuracy :  0.869809\n",
      "i: 5000 , cost :  2.27685 , training accuracy :  0.780022\n",
      "i: 5100 , cost :  1.4031 , training accuracy :  0.867565\n",
      "i: 5200 , cost :  2.38397 , training accuracy :  0.809203\n",
      "i: 5300 , cost :  1.96015 , training accuracy :  0.79798\n",
      "i: 5400 , cost :  0.845159 , training accuracy :  0.873176\n",
      "i: 5500 , cost :  16.9691 , training accuracy :  0.417508\n",
      "i: 5600 , cost :  0.886921 , training accuracy :  0.875421\n",
      "i: 5700 , cost :  24.8307 , training accuracy :  0.392817\n",
      "i: 5800 , cost :  0.776138 , training accuracy :  0.872054\n",
      "i: 5900 , cost :  3.30471 , training accuracy :  0.776655\n",
      "i: 6000 , cost :  8.12747 , training accuracy :  0.731762\n",
      "i: 6100 , cost :  0.609782 , training accuracy :  0.86532\n",
      "i: 6200 , cost :  0.645404 , training accuracy :  0.888889\n",
      "i: 6300 , cost :  8.59535 , training accuracy :  0.429854\n",
      "i: 6400 , cost :  5.15561 , training accuracy :  0.801347\n",
      "i: 6500 , cost :  5.40592 , training accuracy :  0.795735\n",
      "i: 6600 , cost :  0.626022 , training accuracy :  0.883277\n",
      "i: 6700 , cost :  3.07602 , training accuracy :  0.815937\n",
      "i: 6800 , cost :  0.985428 , training accuracy :  0.863075\n",
      "i: 6900 , cost :  0.572779 , training accuracy :  0.872054\n",
      "i: 7000 , cost :  0.527623 , training accuracy :  0.886644\n",
      "i: 7100 , cost :  3.39224 , training accuracy :  0.781145\n",
      "i: 7200 , cost :  1.34704 , training accuracy :  0.837261\n",
      "i: 7300 , cost :  0.526408 , training accuracy :  0.885522\n",
      "i: 7400 , cost :  0.608848 , training accuracy :  0.867565\n",
      "i: 7500 , cost :  2.15844 , training accuracy :  0.820426\n",
      "i: 7600 , cost :  0.435854 , training accuracy :  0.886644\n",
      "i: 7700 , cost :  3.06886 , training accuracy :  0.821549\n",
      "i: 7800 , cost :  0.684247 , training accuracy :  0.861953\n",
      "i: 7900 , cost :  0.389643 , training accuracy :  0.875421\n",
      "i: 8000 , cost :  2.04006 , training accuracy :  0.817059\n",
      "i: 8100 , cost :  0.591539 , training accuracy :  0.874299\n",
      "i: 8200 , cost :  0.377455 , training accuracy :  0.883277\n",
      "i: 8300 , cost :  2.56291 , training accuracy :  0.556678\n",
      "i: 8400 , cost :  0.52267 , training accuracy :  0.874299\n",
      "i: 8500 , cost :  0.54505 , training accuracy :  0.870932\n",
      "i: 8600 , cost :  0.382538 , training accuracy :  0.876543\n",
      "i: 8700 , cost :  1.56647 , training accuracy :  0.808081\n",
      "i: 8800 , cost :  2.93014 , training accuracy :  0.792368\n",
      "i: 8900 , cost :  0.31617 , training accuracy :  0.883277\n",
      "i: 9000 , cost :  0.28856 , training accuracy :  0.882155\n",
      "i: 9100 , cost :  1.91356 , training accuracy :  0.823793\n",
      "i: 9200 , cost :  0.300014 , training accuracy :  0.8844\n",
      "i: 9300 , cost :  0.274486 , training accuracy :  0.888889\n",
      "i: 9400 , cost :  0.518854 , training accuracy :  0.828283\n",
      "i: 9500 , cost :  0.296471 , training accuracy :  0.8844\n",
      "i: 9600 , cost :  0.26634 , training accuracy :  0.892256\n",
      "i: 9700 , cost :  3.06584 , training accuracy :  0.474747\n",
      "i: 9800 , cost :  0.295489 , training accuracy :  0.894501\n",
      "i: 9900 , cost :  0.264811 , training accuracy :  0.896745\n",
      "Final training accuracy :  0.887767\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 10000 ][hidden: 100 ][file: bhavul_tr_acc_0.89_prediction.csv ] ACCURACY :  0.887767\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  2.75837 , training accuracy :  0.611672\n",
      "i: 100 , cost :  0.665371 , training accuracy :  0.616162\n",
      "i: 200 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 1000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 1100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 1200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 1300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 1400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 1500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 1600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 1700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 1800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 1900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 2000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 2100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 2200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 2300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 2400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 2500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 2600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 2700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 2800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 2900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 3000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 3100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 3200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 3300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 3400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 3500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 3600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 3700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 3800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 3900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 4000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 4100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 4200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 4300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 4400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 4500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 4600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 4700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 4800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 4900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 5000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 5100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 5200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 5300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 5400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 5500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 5600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 5700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 5800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 5900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 6000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 6100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 6200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 6300 , cost :  0.66537 , training accuracy :  0.616162\n",
      "i: 6400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 6500 , cost :  0.66537 , training accuracy :  0.616162\n",
      "i: 6600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 6700 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 6800 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 6900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 7000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 7100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 7200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 7300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 7400 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 7500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 7600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 7700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 7800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 7900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 8000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 8100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 8200 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 8300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 8400 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 8500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 8600 , cost :  0.66537 , training accuracy :  0.616162\n",
      "i: 8700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 8800 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 8900 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 9000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 9100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 9200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 9300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 9400 , cost :  0.66537 , training accuracy :  0.616162\n",
      "i: 9500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 9600 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 9700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 9800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 9900 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 10000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 10100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 10200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 10300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 10400 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 10500 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 10600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 10700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 10800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 10900 , cost :  0.665372 , training accuracy :  0.616162\n",
      "i: 11000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 11100 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 11200 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 11300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 11400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 11500 , cost :  0.66537 , training accuracy :  0.616162\n",
      "i: 11600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 11700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 11800 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 11900 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 12000 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 12100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 12200 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 12300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 12400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 12500 , cost :  0.66537 , training accuracy :  0.616162\n",
      "i: 12600 , cost :  0.665375 , training accuracy :  0.616162\n",
      "i: 12700 , cost :  0.665366 , training accuracy :  0.616162\n",
      "i: 12800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 12900 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 13000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 13100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 13200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 13300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 13400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 13500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 13600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 13700 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 13800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 13900 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 14000 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 14100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 14200 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 14300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 14400 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 14500 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 14600 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 14700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 14800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 14900 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 15000 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 15100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 15200 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 15300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 15400 , cost :  0.665373 , training accuracy :  0.616162\n",
      "i: 15500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 15600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 15700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 15800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 15900 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 16000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 16100 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 16200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 16300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 16400 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 16500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 16600 , cost :  0.665373 , training accuracy :  0.616162\n",
      "i: 16700 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 16800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 16900 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 17000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 17100 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 17200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 17300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 17400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 17500 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 17600 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 17700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 17800 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 17900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 18000 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 18100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 18200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 18300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 18400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 18500 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 18600 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 18700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 18800 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 18900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 19000 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 19100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 19200 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 19300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 19400 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 19500 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 19600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 19700 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 19800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 19900 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 20000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 20100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 20200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 20300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 20400 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 20500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 20600 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 20700 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 20800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 20900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 21000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 21100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 21200 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 21300 , cost :  0.66537 , training accuracy :  0.616162\n",
      "i: 21400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 21500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 21600 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 21700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 21800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 21900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 22000 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 22100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 22200 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 22300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 22400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 22500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 22600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 22700 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 22800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 22900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 23000 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 23100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 23200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 23300 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 23400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 23500 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 23600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 23700 , cost :  0.665375 , training accuracy :  0.616162\n",
      "i: 23800 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 23900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 24000 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 24100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 24200 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 24300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 24400 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 24500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 24600 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 24700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 24800 , cost :  0.665373 , training accuracy :  0.616162\n",
      "i: 24900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 25000 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 25100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 25200 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 25300 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 25400 , cost :  0.665372 , training accuracy :  0.616162\n",
      "i: 25500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 25600 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 25700 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 25800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 25900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 26000 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 26100 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 26200 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 26300 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 26400 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 26500 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 26600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 26700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 26800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 26900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 27000 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 27100 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 27200 , cost :  0.665374 , training accuracy :  0.616162\n",
      "i: 27300 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 27400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 27500 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 27600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 27700 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 27800 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 27900 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 28000 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 28100 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 28200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 28300 , cost :  0.665371 , training accuracy :  0.616162\n",
      "i: 28400 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 28500 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 28600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 28700 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 28800 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 28900 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 29000 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 29100 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 29200 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 29300 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 29400 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 29500 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 29600 , cost :  0.665368 , training accuracy :  0.616162\n",
      "i: 29700 , cost :  0.665367 , training accuracy :  0.616162\n",
      "i: 29800 , cost :  0.665369 , training accuracy :  0.616162\n",
      "i: 29900 , cost :  0.66537 , training accuracy :  0.616162\n",
      "Final training accuracy :  0.616162\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 30000 ][hidden: 3 ][file: bhavul_tr_acc_0.62_prediction.csv ] ACCURACY :  0.616162\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  923.844 , training accuracy :  0.383838\n",
      "i: 100 , cost :  1.53417 , training accuracy :  0.71156\n",
      "i: 200 , cost :  0.662098 , training accuracy :  0.777778\n",
      "i: 300 , cost :  0.94193 , training accuracy :  0.718294\n",
      "i: 400 , cost :  0.550205 , training accuracy :  0.777778\n",
      "i: 500 , cost :  0.76968 , training accuracy :  0.751964\n",
      "i: 600 , cost :  0.468372 , training accuracy :  0.795735\n",
      "i: 700 , cost :  1.20779 , training accuracy :  0.578002\n",
      "i: 800 , cost :  0.450957 , training accuracy :  0.795735\n",
      "i: 900 , cost :  0.711309 , training accuracy :  0.707071\n",
      "i: 1000 , cost :  0.444698 , training accuracy :  0.794613\n",
      "i: 1100 , cost :  0.481849 , training accuracy :  0.79798\n",
      "i: 1200 , cost :  0.510053 , training accuracy :  0.791246\n",
      "i: 1300 , cost :  0.551003 , training accuracy :  0.776655\n",
      "i: 1400 , cost :  0.523795 , training accuracy :  0.782267\n",
      "i: 1500 , cost :  0.464995 , training accuracy :  0.804714\n",
      "i: 1600 , cost :  0.489073 , training accuracy :  0.780022\n",
      "i: 1700 , cost :  0.498742 , training accuracy :  0.777778\n",
      "i: 1800 , cost :  0.439466 , training accuracy :  0.79798\n",
      "i: 1900 , cost :  0.43259 , training accuracy :  0.79798\n",
      "i: 2000 , cost :  0.658746 , training accuracy :  0.687991\n",
      "i: 2100 , cost :  0.4337 , training accuracy :  0.801347\n",
      "i: 2200 , cost :  0.503538 , training accuracy :  0.790123\n",
      "i: 2300 , cost :  0.432708 , training accuracy :  0.799102\n",
      "i: 2400 , cost :  0.450848 , training accuracy :  0.805836\n",
      "i: 2500 , cost :  0.507771 , training accuracy :  0.791246\n",
      "i: 2600 , cost :  0.500442 , training accuracy :  0.773288\n",
      "i: 2700 , cost :  0.460637 , training accuracy :  0.795735\n",
      "i: 2800 , cost :  0.518293 , training accuracy :  0.784512\n",
      "i: 2900 , cost :  0.524255 , training accuracy :  0.783389\n",
      "i: 3000 , cost :  0.499141 , training accuracy :  0.792368\n",
      "i: 3100 , cost :  0.495802 , training accuracy :  0.790123\n",
      "i: 3200 , cost :  0.525197 , training accuracy :  0.781145\n",
      "i: 3300 , cost :  0.525906 , training accuracy :  0.781145\n",
      "i: 3400 , cost :  0.524132 , training accuracy :  0.781145\n",
      "i: 3500 , cost :  0.519516 , training accuracy :  0.781145\n",
      "i: 3600 , cost :  0.521864 , training accuracy :  0.780022\n",
      "i: 3700 , cost :  0.483729 , training accuracy :  0.79349\n",
      "i: 3800 , cost :  0.443352 , training accuracy :  0.805836\n",
      "i: 3900 , cost :  0.433888 , training accuracy :  0.79798\n",
      "i: 4000 , cost :  0.435951 , training accuracy :  0.801347\n",
      "i: 4100 , cost :  0.441214 , training accuracy :  0.808081\n",
      "i: 4200 , cost :  0.480702 , training accuracy :  0.79349\n",
      "i: 4300 , cost :  0.493872 , training accuracy :  0.790123\n",
      "i: 4400 , cost :  0.451308 , training accuracy :  0.802469\n",
      "i: 4500 , cost :  0.486855 , training accuracy :  0.790123\n",
      "i: 4600 , cost :  0.500899 , training accuracy :  0.786756\n",
      "i: 4700 , cost :  0.449664 , training accuracy :  0.806958\n",
      "i: 4800 , cost :  0.42711 , training accuracy :  0.808081\n",
      "i: 4900 , cost :  0.465012 , training accuracy :  0.79349\n",
      "i: 5000 , cost :  0.474002 , training accuracy :  0.79349\n",
      "i: 5100 , cost :  0.442669 , training accuracy :  0.805836\n",
      "i: 5200 , cost :  0.425932 , training accuracy :  0.805836\n",
      "i: 5300 , cost :  0.437451 , training accuracy :  0.806958\n",
      "i: 5400 , cost :  0.458384 , training accuracy :  0.796857\n",
      "i: 5500 , cost :  0.437739 , training accuracy :  0.809203\n",
      "i: 5600 , cost :  0.435285 , training accuracy :  0.809203\n",
      "i: 5700 , cost :  0.419904 , training accuracy :  0.811448\n",
      "i: 5800 , cost :  0.426214 , training accuracy :  0.809203\n",
      "i: 5900 , cost :  0.436306 , training accuracy :  0.811448\n",
      "i: 6000 , cost :  0.419978 , training accuracy :  0.821549\n",
      "i: 6100 , cost :  0.45571 , training accuracy :  0.805836\n",
      "i: 6200 , cost :  0.50437 , training accuracy :  0.76431\n",
      "i: 6300 , cost :  0.423691 , training accuracy :  0.821549\n",
      "i: 6400 , cost :  0.418698 , training accuracy :  0.81257\n",
      "i: 6500 , cost :  0.430592 , training accuracy :  0.801347\n",
      "i: 6600 , cost :  0.466959 , training accuracy :  0.783389\n",
      "i: 6700 , cost :  0.407672 , training accuracy :  0.824916\n",
      "i: 6800 , cost :  0.449315 , training accuracy :  0.791246\n",
      "i: 6900 , cost :  0.408839 , training accuracy :  0.828283\n",
      "i: 7000 , cost :  0.410666 , training accuracy :  0.824916\n",
      "i: 7100 , cost :  0.441871 , training accuracy :  0.79349\n",
      "i: 7200 , cost :  0.42799 , training accuracy :  0.795735\n",
      "i: 7300 , cost :  0.457555 , training accuracy :  0.789001\n",
      "i: 7400 , cost :  0.43327 , training accuracy :  0.802469\n",
      "i: 7500 , cost :  0.427228 , training accuracy :  0.801347\n",
      "i: 7600 , cost :  0.45956 , training accuracy :  0.810326\n",
      "i: 7700 , cost :  0.404329 , training accuracy :  0.828283\n",
      "i: 7800 , cost :  0.421462 , training accuracy :  0.820426\n",
      "i: 7900 , cost :  0.406291 , training accuracy :  0.828283\n",
      "i: 8000 , cost :  0.45239 , training accuracy :  0.814815\n",
      "i: 8100 , cost :  0.413937 , training accuracy :  0.82716\n",
      "i: 8200 , cost :  0.417575 , training accuracy :  0.824916\n",
      "i: 8300 , cost :  0.413473 , training accuracy :  0.82716\n",
      "i: 8400 , cost :  0.399048 , training accuracy :  0.83165\n",
      "i: 8500 , cost :  0.397027 , training accuracy :  0.839506\n",
      "i: 8600 , cost :  0.401674 , training accuracy :  0.830527\n",
      "i: 8700 , cost :  0.396543 , training accuracy :  0.841751\n",
      "i: 8800 , cost :  0.40298 , training accuracy :  0.83165\n",
      "i: 8900 , cost :  0.397776 , training accuracy :  0.840629\n",
      "i: 9000 , cost :  0.393791 , training accuracy :  0.838384\n",
      "i: 9100 , cost :  0.423336 , training accuracy :  0.81257\n",
      "i: 9200 , cost :  0.420439 , training accuracy :  0.826038\n",
      "i: 9300 , cost :  0.40677 , training accuracy :  0.836139\n",
      "i: 9400 , cost :  0.391129 , training accuracy :  0.840629\n",
      "i: 9500 , cost :  0.402748 , training accuracy :  0.836139\n",
      "i: 9600 , cost :  0.410858 , training accuracy :  0.824916\n",
      "i: 9700 , cost :  0.412756 , training accuracy :  0.83165\n",
      "i: 9800 , cost :  0.439793 , training accuracy :  0.809203\n",
      "i: 9900 , cost :  0.392594 , training accuracy :  0.841751\n",
      "i: 10000 , cost :  0.40804 , training accuracy :  0.822671\n",
      "i: 10100 , cost :  0.395372 , training accuracy :  0.843996\n",
      "i: 10200 , cost :  0.400929 , training accuracy :  0.82716\n",
      "i: 10300 , cost :  0.403342 , training accuracy :  0.840629\n",
      "i: 10400 , cost :  0.392794 , training accuracy :  0.838384\n",
      "i: 10500 , cost :  0.400506 , training accuracy :  0.840629\n",
      "i: 10600 , cost :  0.389379 , training accuracy :  0.840629\n",
      "i: 10700 , cost :  0.390952 , training accuracy :  0.845118\n",
      "i: 10800 , cost :  0.390655 , training accuracy :  0.836139\n",
      "i: 10900 , cost :  0.392694 , training accuracy :  0.85073\n",
      "i: 11000 , cost :  0.406927 , training accuracy :  0.82716\n",
      "i: 11100 , cost :  0.390105 , training accuracy :  0.84624\n",
      "i: 11200 , cost :  0.386361 , training accuracy :  0.845118\n",
      "i: 11300 , cost :  0.424595 , training accuracy :  0.824916\n",
      "i: 11400 , cost :  0.383857 , training accuracy :  0.84624\n",
      "i: 11500 , cost :  0.381496 , training accuracy :  0.84624\n",
      "i: 11600 , cost :  0.383015 , training accuracy :  0.84624\n",
      "i: 11700 , cost :  0.381509 , training accuracy :  0.849607\n",
      "i: 11800 , cost :  0.381667 , training accuracy :  0.847363\n",
      "i: 11900 , cost :  0.387218 , training accuracy :  0.840629\n",
      "i: 12000 , cost :  0.383491 , training accuracy :  0.841751\n",
      "i: 12100 , cost :  0.395954 , training accuracy :  0.837261\n",
      "i: 12200 , cost :  0.385853 , training accuracy :  0.838384\n",
      "i: 12300 , cost :  0.403714 , training accuracy :  0.840629\n",
      "i: 12400 , cost :  0.380121 , training accuracy :  0.848485\n",
      "i: 12500 , cost :  0.397852 , training accuracy :  0.837261\n",
      "i: 12600 , cost :  0.389654 , training accuracy :  0.829405\n",
      "i: 12700 , cost :  0.380033 , training accuracy :  0.848485\n",
      "i: 12800 , cost :  0.376727 , training accuracy :  0.842873\n",
      "i: 12900 , cost :  0.376933 , training accuracy :  0.842873\n",
      "i: 13000 , cost :  0.39751 , training accuracy :  0.835017\n",
      "i: 13100 , cost :  0.423758 , training accuracy :  0.82716\n",
      "i: 13200 , cost :  0.37579 , training accuracy :  0.84624\n",
      "i: 13300 , cost :  0.399627 , training accuracy :  0.828283\n",
      "i: 13400 , cost :  0.383734 , training accuracy :  0.849607\n",
      "i: 13500 , cost :  0.389039 , training accuracy :  0.838384\n",
      "i: 13600 , cost :  0.374992 , training accuracy :  0.848485\n",
      "i: 13700 , cost :  0.407706 , training accuracy :  0.818182\n",
      "i: 13800 , cost :  0.390877 , training accuracy :  0.82716\n",
      "i: 13900 , cost :  0.374153 , training accuracy :  0.856341\n",
      "i: 14000 , cost :  0.373757 , training accuracy :  0.848485\n",
      "i: 14100 , cost :  0.421263 , training accuracy :  0.821549\n",
      "i: 14200 , cost :  0.375414 , training accuracy :  0.85073\n",
      "i: 14300 , cost :  0.37297 , training accuracy :  0.854097\n",
      "i: 14400 , cost :  0.379784 , training accuracy :  0.851852\n",
      "i: 14500 , cost :  0.373159 , training accuracy :  0.851852\n",
      "i: 14600 , cost :  0.39649 , training accuracy :  0.829405\n",
      "i: 14700 , cost :  0.377881 , training accuracy :  0.85073\n",
      "i: 14800 , cost :  0.373582 , training accuracy :  0.845118\n",
      "i: 14900 , cost :  0.401368 , training accuracy :  0.835017\n",
      "i: 15000 , cost :  0.401814 , training accuracy :  0.819304\n",
      "i: 15100 , cost :  0.376236 , training accuracy :  0.835017\n",
      "i: 15200 , cost :  0.393803 , training accuracy :  0.828283\n",
      "i: 15300 , cost :  0.380602 , training accuracy :  0.84624\n",
      "i: 15400 , cost :  0.374399 , training accuracy :  0.838384\n",
      "i: 15500 , cost :  0.383824 , training accuracy :  0.838384\n",
      "i: 15600 , cost :  0.401242 , training accuracy :  0.823793\n",
      "i: 15700 , cost :  0.381043 , training accuracy :  0.837261\n",
      "i: 15800 , cost :  0.403137 , training accuracy :  0.82716\n",
      "i: 15900 , cost :  0.398116 , training accuracy :  0.836139\n",
      "i: 16000 , cost :  0.376209 , training accuracy :  0.840629\n",
      "i: 16100 , cost :  0.375346 , training accuracy :  0.841751\n",
      "i: 16200 , cost :  0.37229 , training accuracy :  0.848485\n",
      "i: 16300 , cost :  0.371861 , training accuracy :  0.851852\n",
      "i: 16400 , cost :  0.409901 , training accuracy :  0.824916\n",
      "i: 16500 , cost :  0.393075 , training accuracy :  0.839506\n",
      "i: 16600 , cost :  0.37146 , training accuracy :  0.854097\n",
      "i: 16700 , cost :  0.372268 , training accuracy :  0.847363\n",
      "i: 16800 , cost :  0.380138 , training accuracy :  0.837261\n",
      "i: 16900 , cost :  0.396375 , training accuracy :  0.82716\n",
      "i: 17000 , cost :  0.408024 , training accuracy :  0.826038\n",
      "i: 17100 , cost :  0.381205 , training accuracy :  0.85073\n",
      "i: 17200 , cost :  0.372015 , training accuracy :  0.849607\n",
      "i: 17300 , cost :  0.382281 , training accuracy :  0.845118\n",
      "i: 17400 , cost :  0.400698 , training accuracy :  0.83165\n",
      "i: 17500 , cost :  0.387027 , training accuracy :  0.82716\n",
      "i: 17600 , cost :  0.400662 , training accuracy :  0.833894\n",
      "i: 17700 , cost :  0.387437 , training accuracy :  0.842873\n",
      "i: 17800 , cost :  0.393426 , training accuracy :  0.830527\n",
      "i: 17900 , cost :  0.375619 , training accuracy :  0.849607\n",
      "i: 18000 , cost :  0.383984 , training accuracy :  0.848485\n",
      "i: 18100 , cost :  0.373058 , training accuracy :  0.845118\n",
      "i: 18200 , cost :  0.37687 , training accuracy :  0.838384\n",
      "i: 18300 , cost :  0.37022 , training accuracy :  0.847363\n",
      "i: 18400 , cost :  0.370387 , training accuracy :  0.847363\n",
      "i: 18500 , cost :  0.435178 , training accuracy :  0.817059\n",
      "i: 18600 , cost :  0.386807 , training accuracy :  0.828283\n",
      "i: 18700 , cost :  0.371007 , training accuracy :  0.847363\n",
      "i: 18800 , cost :  0.435942 , training accuracy :  0.813693\n",
      "i: 18900 , cost :  0.381159 , training accuracy :  0.848485\n",
      "i: 19000 , cost :  0.394261 , training accuracy :  0.835017\n",
      "i: 19100 , cost :  0.372628 , training accuracy :  0.847363\n",
      "i: 19200 , cost :  0.376845 , training accuracy :  0.851852\n",
      "i: 19300 , cost :  0.37584 , training accuracy :  0.851852\n",
      "i: 19400 , cost :  0.369907 , training accuracy :  0.85073\n",
      "i: 19500 , cost :  0.37105 , training accuracy :  0.854097\n",
      "i: 19600 , cost :  0.375821 , training accuracy :  0.849607\n",
      "i: 19700 , cost :  0.382157 , training accuracy :  0.848485\n",
      "i: 19800 , cost :  0.377559 , training accuracy :  0.837261\n",
      "i: 19900 , cost :  0.415915 , training accuracy :  0.820426\n",
      "i: 20000 , cost :  0.384385 , training accuracy :  0.845118\n",
      "i: 20100 , cost :  0.390069 , training accuracy :  0.842873\n",
      "i: 20200 , cost :  0.376122 , training accuracy :  0.851852\n",
      "i: 20300 , cost :  0.37124 , training accuracy :  0.852974\n",
      "i: 20400 , cost :  0.381544 , training accuracy :  0.83165\n",
      "i: 20500 , cost :  0.397627 , training accuracy :  0.83165\n",
      "i: 20600 , cost :  0.373864 , training accuracy :  0.842873\n",
      "i: 20700 , cost :  0.386696 , training accuracy :  0.845118\n",
      "i: 20800 , cost :  0.39376 , training accuracy :  0.828283\n",
      "i: 20900 , cost :  0.379524 , training accuracy :  0.835017\n",
      "i: 21000 , cost :  0.370162 , training accuracy :  0.848485\n",
      "i: 21100 , cost :  0.370148 , training accuracy :  0.848485\n",
      "i: 21200 , cost :  0.381501 , training accuracy :  0.849607\n",
      "i: 21300 , cost :  0.408141 , training accuracy :  0.82716\n",
      "i: 21400 , cost :  0.41369 , training accuracy :  0.824916\n",
      "i: 21500 , cost :  0.391375 , training accuracy :  0.833894\n",
      "i: 21600 , cost :  0.376114 , training accuracy :  0.839506\n",
      "i: 21700 , cost :  0.394218 , training accuracy :  0.838384\n",
      "i: 21800 , cost :  0.377281 , training accuracy :  0.849607\n",
      "i: 21900 , cost :  0.37869 , training accuracy :  0.833894\n",
      "i: 22000 , cost :  0.394433 , training accuracy :  0.82716\n",
      "i: 22100 , cost :  0.369709 , training accuracy :  0.847363\n",
      "i: 22200 , cost :  0.369623 , training accuracy :  0.848485\n",
      "i: 22300 , cost :  0.380306 , training accuracy :  0.854097\n",
      "i: 22400 , cost :  0.370118 , training accuracy :  0.85073\n",
      "i: 22500 , cost :  0.38477 , training accuracy :  0.84624\n",
      "i: 22600 , cost :  0.383135 , training accuracy :  0.835017\n",
      "i: 22700 , cost :  0.393185 , training accuracy :  0.839506\n",
      "i: 22800 , cost :  0.375076 , training accuracy :  0.852974\n",
      "i: 22900 , cost :  0.387096 , training accuracy :  0.847363\n",
      "i: 23000 , cost :  0.395662 , training accuracy :  0.837261\n",
      "i: 23100 , cost :  0.384866 , training accuracy :  0.83165\n",
      "i: 23200 , cost :  0.373458 , training accuracy :  0.841751\n",
      "i: 23300 , cost :  0.381763 , training accuracy :  0.84624\n",
      "i: 23400 , cost :  0.391379 , training accuracy :  0.824916\n",
      "i: 23500 , cost :  0.379786 , training accuracy :  0.835017\n",
      "i: 23600 , cost :  0.37486 , training accuracy :  0.85073\n",
      "i: 23700 , cost :  0.380961 , training accuracy :  0.833894\n",
      "i: 23800 , cost :  0.420602 , training accuracy :  0.820426\n",
      "i: 23900 , cost :  0.372215 , training accuracy :  0.85073\n",
      "i: 24000 , cost :  0.409746 , training accuracy :  0.826038\n",
      "i: 24100 , cost :  0.397381 , training accuracy :  0.826038\n",
      "i: 24200 , cost :  0.374514 , training accuracy :  0.85073\n",
      "i: 24300 , cost :  0.383386 , training accuracy :  0.829405\n",
      "i: 24400 , cost :  0.416231 , training accuracy :  0.824916\n",
      "i: 24500 , cost :  0.398458 , training accuracy :  0.832772\n",
      "i: 24600 , cost :  0.387253 , training accuracy :  0.842873\n",
      "i: 24700 , cost :  0.37398 , training accuracy :  0.85073\n",
      "i: 24800 , cost :  0.368817 , training accuracy :  0.849607\n",
      "i: 24900 , cost :  0.385157 , training accuracy :  0.830527\n",
      "i: 25000 , cost :  0.396973 , training accuracy :  0.829405\n",
      "i: 25100 , cost :  0.382947 , training accuracy :  0.839506\n",
      "i: 25200 , cost :  0.378894 , training accuracy :  0.835017\n",
      "i: 25300 , cost :  0.369367 , training accuracy :  0.848485\n",
      "i: 25400 , cost :  0.374317 , training accuracy :  0.841751\n",
      "i: 25500 , cost :  0.373192 , training accuracy :  0.852974\n",
      "i: 25600 , cost :  0.388409 , training accuracy :  0.82716\n",
      "i: 25700 , cost :  0.368409 , training accuracy :  0.847363\n",
      "i: 25800 , cost :  0.427507 , training accuracy :  0.820426\n",
      "i: 25900 , cost :  0.376527 , training accuracy :  0.839506\n",
      "i: 26000 , cost :  0.369326 , training accuracy :  0.85073\n",
      "i: 26100 , cost :  0.40088 , training accuracy :  0.826038\n",
      "i: 26200 , cost :  0.387821 , training accuracy :  0.843996\n",
      "i: 26300 , cost :  0.371282 , training accuracy :  0.848485\n",
      "i: 26400 , cost :  0.372654 , training accuracy :  0.843996\n",
      "i: 26500 , cost :  0.374179 , training accuracy :  0.842873\n",
      "i: 26600 , cost :  0.4054 , training accuracy :  0.830527\n",
      "i: 26700 , cost :  0.374861 , training accuracy :  0.85073\n",
      "i: 26800 , cost :  0.370237 , training accuracy :  0.849607\n",
      "i: 26900 , cost :  0.377619 , training accuracy :  0.840629\n",
      "i: 27000 , cost :  0.400824 , training accuracy :  0.832772\n",
      "i: 27100 , cost :  0.372847 , training accuracy :  0.842873\n",
      "i: 27200 , cost :  0.403623 , training accuracy :  0.82716\n",
      "i: 27300 , cost :  0.37063 , training accuracy :  0.847363\n",
      "i: 27400 , cost :  0.399911 , training accuracy :  0.835017\n",
      "i: 27500 , cost :  0.372468 , training accuracy :  0.849607\n",
      "i: 27600 , cost :  0.411948 , training accuracy :  0.828283\n",
      "i: 27700 , cost :  0.371362 , training accuracy :  0.849607\n",
      "i: 27800 , cost :  0.397046 , training accuracy :  0.837261\n",
      "i: 27900 , cost :  0.419117 , training accuracy :  0.822671\n",
      "i: 28000 , cost :  0.421347 , training accuracy :  0.822671\n",
      "i: 28100 , cost :  0.388113 , training accuracy :  0.835017\n",
      "i: 28200 , cost :  0.369899 , training accuracy :  0.848485\n",
      "i: 28300 , cost :  0.398123 , training accuracy :  0.835017\n",
      "i: 28400 , cost :  0.385138 , training accuracy :  0.84624\n",
      "i: 28500 , cost :  0.373615 , training accuracy :  0.842873\n",
      "i: 28600 , cost :  0.421375 , training accuracy :  0.821549\n",
      "i: 28700 , cost :  0.380812 , training accuracy :  0.84624\n",
      "i: 28800 , cost :  0.38734 , training accuracy :  0.847363\n",
      "i: 28900 , cost :  0.371117 , training accuracy :  0.852974\n",
      "i: 29000 , cost :  0.368964 , training accuracy :  0.854097\n",
      "i: 29100 , cost :  0.376549 , training accuracy :  0.854097\n",
      "i: 29200 , cost :  0.383172 , training accuracy :  0.849607\n",
      "i: 29300 , cost :  0.430758 , training accuracy :  0.815937\n",
      "i: 29400 , cost :  0.370822 , training accuracy :  0.841751\n",
      "i: 29500 , cost :  0.381055 , training accuracy :  0.848485\n",
      "i: 29600 , cost :  0.411929 , training accuracy :  0.824916\n",
      "i: 29700 , cost :  0.382188 , training accuracy :  0.845118\n",
      "i: 29800 , cost :  0.373588 , training accuracy :  0.849607\n",
      "i: 29900 , cost :  0.383641 , training accuracy :  0.847363\n",
      "Final training accuracy :  0.852974\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 30000 ][hidden: 10 ][file: bhavul_tr_acc_0.85_prediction.csv ] ACCURACY :  0.852974\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  673.976 , training accuracy :  0.383838\n",
      "i: 100 , cost :  2.40623 , training accuracy :  0.749719\n",
      "i: 200 , cost :  3.5718 , training accuracy :  0.713805\n",
      "i: 300 , cost :  7.32347 , training accuracy :  0.414141\n",
      "i: 400 , cost :  3.82191 , training accuracy :  0.751964\n",
      "i: 500 , cost :  3.91137 , training accuracy :  0.609428\n",
      "i: 600 , cost :  2.9346 , training accuracy :  0.731762\n",
      "i: 700 , cost :  0.937021 , training accuracy :  0.801347\n",
      "i: 800 , cost :  1.42537 , training accuracy :  0.794613\n",
      "i: 900 , cost :  21.1675 , training accuracy :  0.38945\n",
      "i: 1000 , cost :  1.94974 , training accuracy :  0.79349\n",
      "i: 1100 , cost :  0.924311 , training accuracy :  0.803591\n",
      "i: 1200 , cost :  10.0867 , training accuracy :  0.684624\n",
      "i: 1300 , cost :  1.22764 , training accuracy :  0.791246\n",
      "i: 1400 , cost :  0.6506 , training accuracy :  0.808081\n",
      "i: 1500 , cost :  6.48083 , training accuracy :  0.672278\n",
      "i: 1600 , cost :  1.00839 , training accuracy :  0.799102\n",
      "i: 1700 , cost :  0.711428 , training accuracy :  0.81257\n",
      "i: 1800 , cost :  1.10212 , training accuracy :  0.71156\n",
      "i: 1900 , cost :  1.15638 , training accuracy :  0.783389\n",
      "i: 2000 , cost :  0.615993 , training accuracy :  0.805836\n",
      "i: 2100 , cost :  1.11465 , training accuracy :  0.774411\n",
      "i: 2200 , cost :  4.16188 , training accuracy :  0.689113\n",
      "i: 2300 , cost :  0.69085 , training accuracy :  0.802469\n",
      "i: 2400 , cost :  0.898295 , training accuracy :  0.763187\n",
      "i: 2500 , cost :  1.6938 , training accuracy :  0.650954\n",
      "i: 2600 , cost :  4.7945 , training accuracy :  0.674523\n",
      "i: 2700 , cost :  1.01623 , training accuracy :  0.787879\n",
      "i: 2800 , cost :  0.513888 , training accuracy :  0.802469\n",
      "i: 2900 , cost :  0.687947 , training accuracy :  0.806958\n",
      "i: 3000 , cost :  0.972494 , training accuracy :  0.765432\n",
      "i: 3100 , cost :  0.707475 , training accuracy :  0.7789\n",
      "i: 3200 , cost :  0.67446 , training accuracy :  0.794613\n",
      "i: 3300 , cost :  0.608758 , training accuracy :  0.804714\n",
      "i: 3400 , cost :  0.664523 , training accuracy :  0.784512\n",
      "i: 3500 , cost :  4.83147 , training accuracy :  0.391695\n",
      "i: 3600 , cost :  0.493355 , training accuracy :  0.808081\n",
      "i: 3700 , cost :  1.54951 , training accuracy :  0.74523\n",
      "i: 3800 , cost :  0.517285 , training accuracy :  0.828283\n",
      "i: 3900 , cost :  2.28492 , training accuracy :  0.676768\n",
      "i: 4000 , cost :  0.513615 , training accuracy :  0.819304\n",
      "i: 4100 , cost :  0.604618 , training accuracy :  0.751964\n",
      "i: 4200 , cost :  1.07099 , training accuracy :  0.768799\n",
      "i: 4300 , cost :  0.449511 , training accuracy :  0.822671\n",
      "i: 4400 , cost :  0.518232 , training accuracy :  0.784512\n",
      "i: 4500 , cost :  0.4756 , training accuracy :  0.817059\n",
      "i: 4600 , cost :  0.621253 , training accuracy :  0.782267\n",
      "i: 4700 , cost :  0.599145 , training accuracy :  0.76431\n",
      "i: 4800 , cost :  0.495874 , training accuracy :  0.822671\n",
      "i: 4900 , cost :  0.42469 , training accuracy :  0.823793\n",
      "i: 5000 , cost :  0.459898 , training accuracy :  0.799102\n",
      "i: 5100 , cost :  0.425646 , training accuracy :  0.821549\n",
      "i: 5200 , cost :  0.492377 , training accuracy :  0.784512\n",
      "i: 5300 , cost :  0.645465 , training accuracy :  0.772166\n",
      "i: 5400 , cost :  0.425247 , training accuracy :  0.819304\n",
      "i: 5500 , cost :  0.602928 , training accuracy :  0.75982\n",
      "i: 5600 , cost :  0.444171 , training accuracy :  0.805836\n",
      "i: 5700 , cost :  0.819369 , training accuracy :  0.766554\n",
      "i: 5800 , cost :  0.426043 , training accuracy :  0.823793\n",
      "i: 5900 , cost :  0.407518 , training accuracy :  0.82716\n",
      "i: 6000 , cost :  0.481278 , training accuracy :  0.806958\n",
      "i: 6100 , cost :  0.409824 , training accuracy :  0.828283\n",
      "i: 6200 , cost :  0.402604 , training accuracy :  0.826038\n",
      "i: 6300 , cost :  0.439618 , training accuracy :  0.792368\n",
      "i: 6400 , cost :  0.421501 , training accuracy :  0.802469\n",
      "i: 6500 , cost :  0.491861 , training accuracy :  0.795735\n",
      "i: 6600 , cost :  0.456943 , training accuracy :  0.79349\n",
      "i: 6700 , cost :  0.422483 , training accuracy :  0.829405\n",
      "i: 6800 , cost :  0.435709 , training accuracy :  0.818182\n",
      "i: 6900 , cost :  0.467802 , training accuracy :  0.790123\n",
      "i: 7000 , cost :  0.425757 , training accuracy :  0.828283\n",
      "i: 7100 , cost :  0.452178 , training accuracy :  0.815937\n",
      "i: 7200 , cost :  0.451577 , training accuracy :  0.799102\n",
      "i: 7300 , cost :  0.396007 , training accuracy :  0.824916\n",
      "i: 7400 , cost :  0.472571 , training accuracy :  0.800224\n",
      "i: 7500 , cost :  0.42331 , training accuracy :  0.813693\n",
      "i: 7600 , cost :  0.42863 , training accuracy :  0.801347\n",
      "i: 7700 , cost :  0.461741 , training accuracy :  0.808081\n",
      "i: 7800 , cost :  1.067 , training accuracy :  0.474747\n",
      "i: 7900 , cost :  0.425103 , training accuracy :  0.805836\n",
      "i: 8000 , cost :  0.42033 , training accuracy :  0.809203\n",
      "i: 8100 , cost :  0.418022 , training accuracy :  0.811448\n",
      "i: 8200 , cost :  0.4145 , training accuracy :  0.818182\n",
      "i: 8300 , cost :  0.411159 , training accuracy :  0.818182\n",
      "i: 8400 , cost :  0.408771 , training accuracy :  0.821549\n",
      "i: 8500 , cost :  0.420989 , training accuracy :  0.818182\n",
      "i: 8600 , cost :  0.427124 , training accuracy :  0.814815\n",
      "i: 8700 , cost :  0.419751 , training accuracy :  0.811448\n",
      "i: 8800 , cost :  0.426313 , training accuracy :  0.81257\n",
      "i: 8900 , cost :  0.405713 , training accuracy :  0.824916\n",
      "i: 9000 , cost :  0.427492 , training accuracy :  0.813693\n",
      "i: 9100 , cost :  0.399861 , training accuracy :  0.822671\n",
      "i: 9200 , cost :  0.430628 , training accuracy :  0.808081\n",
      "i: 9300 , cost :  0.406422 , training accuracy :  0.823793\n",
      "i: 9400 , cost :  0.394979 , training accuracy :  0.826038\n",
      "i: 9500 , cost :  0.422105 , training accuracy :  0.815937\n",
      "i: 9600 , cost :  0.395365 , training accuracy :  0.830527\n",
      "i: 9700 , cost :  0.400122 , training accuracy :  0.829405\n",
      "i: 9800 , cost :  0.398709 , training accuracy :  0.830527\n",
      "i: 9900 , cost :  0.400586 , training accuracy :  0.824916\n",
      "i: 10000 , cost :  0.402877 , training accuracy :  0.828283\n",
      "i: 10100 , cost :  0.40266 , training accuracy :  0.829405\n",
      "i: 10200 , cost :  0.400496 , training accuracy :  0.83165\n",
      "i: 10300 , cost :  0.394758 , training accuracy :  0.83165\n",
      "i: 10400 , cost :  0.392665 , training accuracy :  0.833894\n",
      "i: 10500 , cost :  0.388964 , training accuracy :  0.836139\n",
      "i: 10600 , cost :  0.406525 , training accuracy :  0.82716\n",
      "i: 10700 , cost :  0.389623 , training accuracy :  0.837261\n",
      "i: 10800 , cost :  0.387865 , training accuracy :  0.836139\n",
      "i: 10900 , cost :  0.417768 , training accuracy :  0.811448\n",
      "i: 11000 , cost :  0.388188 , training accuracy :  0.836139\n",
      "i: 11100 , cost :  0.393084 , training accuracy :  0.828283\n",
      "i: 11200 , cost :  0.386365 , training accuracy :  0.833894\n",
      "i: 11300 , cost :  0.386871 , training accuracy :  0.836139\n",
      "i: 11400 , cost :  0.387948 , training accuracy :  0.838384\n",
      "i: 11500 , cost :  0.386167 , training accuracy :  0.839506\n",
      "i: 11600 , cost :  0.401483 , training accuracy :  0.829405\n",
      "i: 11700 , cost :  0.385794 , training accuracy :  0.835017\n",
      "i: 11800 , cost :  0.398588 , training accuracy :  0.832772\n",
      "i: 11900 , cost :  0.395413 , training accuracy :  0.829405\n",
      "i: 12000 , cost :  0.385829 , training accuracy :  0.838384\n",
      "i: 12100 , cost :  0.389537 , training accuracy :  0.837261\n",
      "i: 12200 , cost :  0.40749 , training accuracy :  0.822671\n",
      "i: 12300 , cost :  0.389515 , training accuracy :  0.838384\n",
      "i: 12400 , cost :  0.385275 , training accuracy :  0.836139\n",
      "i: 12500 , cost :  0.385293 , training accuracy :  0.832772\n",
      "i: 12600 , cost :  0.385983 , training accuracy :  0.839506\n",
      "i: 12700 , cost :  0.389699 , training accuracy :  0.837261\n",
      "i: 12800 , cost :  0.390065 , training accuracy :  0.838384\n",
      "i: 12900 , cost :  0.384942 , training accuracy :  0.833894\n",
      "i: 13000 , cost :  0.391593 , training accuracy :  0.836139\n",
      "i: 13100 , cost :  0.388312 , training accuracy :  0.836139\n",
      "i: 13200 , cost :  0.386431 , training accuracy :  0.83165\n",
      "i: 13300 , cost :  0.389575 , training accuracy :  0.828283\n",
      "i: 13400 , cost :  0.389352 , training accuracy :  0.824916\n",
      "i: 13500 , cost :  0.384106 , training accuracy :  0.832772\n",
      "i: 13600 , cost :  0.390637 , training accuracy :  0.836139\n",
      "i: 13700 , cost :  0.387645 , training accuracy :  0.835017\n",
      "i: 13800 , cost :  0.387719 , training accuracy :  0.837261\n",
      "i: 13900 , cost :  0.388425 , training accuracy :  0.838384\n",
      "i: 14000 , cost :  0.380747 , training accuracy :  0.838384\n",
      "i: 14100 , cost :  0.379483 , training accuracy :  0.840629\n",
      "i: 14200 , cost :  0.381581 , training accuracy :  0.836139\n",
      "i: 14300 , cost :  0.38369 , training accuracy :  0.838384\n",
      "i: 14400 , cost :  0.376716 , training accuracy :  0.84624\n",
      "i: 14500 , cost :  0.38544 , training accuracy :  0.84624\n",
      "i: 14600 , cost :  0.397115 , training accuracy :  0.837261\n",
      "i: 14700 , cost :  0.375474 , training accuracy :  0.849607\n",
      "i: 14800 , cost :  0.375132 , training accuracy :  0.85073\n",
      "i: 14900 , cost :  0.374574 , training accuracy :  0.848485\n",
      "i: 15000 , cost :  0.381502 , training accuracy :  0.837261\n",
      "i: 15100 , cost :  0.374632 , training accuracy :  0.848485\n",
      "i: 15200 , cost :  0.385152 , training accuracy :  0.836139\n",
      "i: 15300 , cost :  0.373151 , training accuracy :  0.848485\n",
      "i: 15400 , cost :  0.374244 , training accuracy :  0.847363\n",
      "i: 15500 , cost :  0.374124 , training accuracy :  0.851852\n",
      "i: 15600 , cost :  0.372358 , training accuracy :  0.851852\n",
      "i: 15700 , cost :  0.385382 , training accuracy :  0.830527\n",
      "i: 15800 , cost :  0.373217 , training accuracy :  0.851852\n",
      "i: 15900 , cost :  0.373766 , training accuracy :  0.85073\n",
      "i: 16000 , cost :  0.377925 , training accuracy :  0.84624\n",
      "i: 16100 , cost :  0.377525 , training accuracy :  0.841751\n",
      "i: 16200 , cost :  0.373103 , training accuracy :  0.85073\n",
      "i: 16300 , cost :  0.37373 , training accuracy :  0.847363\n",
      "i: 16400 , cost :  0.38282 , training accuracy :  0.832772\n",
      "i: 16500 , cost :  0.37861 , training accuracy :  0.845118\n",
      "i: 16600 , cost :  0.39322 , training accuracy :  0.833894\n",
      "i: 16700 , cost :  0.373595 , training accuracy :  0.849607\n",
      "i: 16800 , cost :  0.398607 , training accuracy :  0.833894\n",
      "i: 16900 , cost :  0.382028 , training accuracy :  0.833894\n",
      "i: 17000 , cost :  0.378573 , training accuracy :  0.841751\n",
      "i: 17100 , cost :  0.394846 , training accuracy :  0.826038\n",
      "i: 17200 , cost :  0.371126 , training accuracy :  0.85073\n",
      "i: 17300 , cost :  0.387401 , training accuracy :  0.824916\n",
      "i: 17400 , cost :  0.379401 , training accuracy :  0.836139\n",
      "i: 17500 , cost :  0.383057 , training accuracy :  0.826038\n",
      "i: 17600 , cost :  0.375075 , training accuracy :  0.843996\n",
      "i: 17700 , cost :  0.38148 , training accuracy :  0.840629\n",
      "i: 17800 , cost :  0.37299 , training accuracy :  0.848485\n",
      "i: 17900 , cost :  0.376663 , training accuracy :  0.84624\n",
      "i: 18000 , cost :  0.371035 , training accuracy :  0.854097\n",
      "i: 18100 , cost :  0.371807 , training accuracy :  0.851852\n",
      "i: 18200 , cost :  0.371261 , training accuracy :  0.851852\n",
      "i: 18300 , cost :  0.380231 , training accuracy :  0.841751\n",
      "i: 18400 , cost :  0.385187 , training accuracy :  0.838384\n",
      "i: 18500 , cost :  0.382514 , training accuracy :  0.838384\n",
      "i: 18600 , cost :  0.371746 , training accuracy :  0.85073\n",
      "i: 18700 , cost :  0.387633 , training accuracy :  0.828283\n",
      "i: 18800 , cost :  0.371627 , training accuracy :  0.849607\n",
      "i: 18900 , cost :  0.37314 , training accuracy :  0.849607\n",
      "i: 19000 , cost :  0.372873 , training accuracy :  0.851852\n",
      "i: 19100 , cost :  0.376336 , training accuracy :  0.847363\n",
      "i: 19200 , cost :  0.376177 , training accuracy :  0.845118\n",
      "i: 19300 , cost :  0.38477 , training accuracy :  0.835017\n",
      "i: 19400 , cost :  0.379615 , training accuracy :  0.835017\n",
      "i: 19500 , cost :  0.372281 , training accuracy :  0.85073\n",
      "i: 19600 , cost :  0.374279 , training accuracy :  0.847363\n",
      "i: 19700 , cost :  0.379071 , training accuracy :  0.835017\n",
      "i: 19800 , cost :  0.37671 , training accuracy :  0.842873\n",
      "i: 19900 , cost :  0.375987 , training accuracy :  0.845118\n",
      "i: 20000 , cost :  0.374272 , training accuracy :  0.849607\n",
      "i: 20100 , cost :  0.382545 , training accuracy :  0.829405\n",
      "i: 20200 , cost :  0.383503 , training accuracy :  0.837261\n",
      "i: 20300 , cost :  0.370146 , training accuracy :  0.85073\n",
      "i: 20400 , cost :  0.384929 , training accuracy :  0.840629\n",
      "i: 20500 , cost :  0.370035 , training accuracy :  0.855219\n",
      "i: 20600 , cost :  0.378901 , training accuracy :  0.845118\n",
      "i: 20700 , cost :  0.383922 , training accuracy :  0.839506\n",
      "i: 20800 , cost :  0.380641 , training accuracy :  0.839506\n",
      "i: 20900 , cost :  0.378442 , training accuracy :  0.845118\n",
      "i: 21000 , cost :  0.371814 , training accuracy :  0.848485\n",
      "i: 21100 , cost :  0.385742 , training accuracy :  0.838384\n",
      "i: 21200 , cost :  0.387983 , training accuracy :  0.829405\n",
      "i: 21300 , cost :  0.377885 , training accuracy :  0.837261\n",
      "i: 21400 , cost :  0.372453 , training accuracy :  0.849607\n",
      "i: 21500 , cost :  0.37653 , training accuracy :  0.840629\n",
      "i: 21600 , cost :  0.379688 , training accuracy :  0.835017\n",
      "i: 21700 , cost :  0.371917 , training accuracy :  0.849607\n",
      "i: 21800 , cost :  0.376401 , training accuracy :  0.845118\n",
      "i: 21900 , cost :  0.381486 , training accuracy :  0.830527\n",
      "i: 22000 , cost :  0.38344 , training accuracy :  0.841751\n",
      "i: 22100 , cost :  0.38492 , training accuracy :  0.837261\n",
      "i: 22200 , cost :  0.381183 , training accuracy :  0.842873\n",
      "i: 22300 , cost :  0.383823 , training accuracy :  0.829405\n",
      "i: 22400 , cost :  0.381078 , training accuracy :  0.835017\n",
      "i: 22500 , cost :  0.371414 , training accuracy :  0.84624\n",
      "i: 22600 , cost :  0.383249 , training accuracy :  0.841751\n",
      "i: 22700 , cost :  0.377214 , training accuracy :  0.843996\n",
      "i: 22800 , cost :  0.369867 , training accuracy :  0.849607\n",
      "i: 22900 , cost :  0.382826 , training accuracy :  0.841751\n",
      "i: 23000 , cost :  0.374775 , training accuracy :  0.847363\n",
      "i: 23100 , cost :  0.369879 , training accuracy :  0.854097\n",
      "i: 23200 , cost :  0.381782 , training accuracy :  0.842873\n",
      "i: 23300 , cost :  0.377851 , training accuracy :  0.841751\n",
      "i: 23400 , cost :  0.380015 , training accuracy :  0.836139\n",
      "i: 23500 , cost :  0.370383 , training accuracy :  0.849607\n",
      "i: 23600 , cost :  0.372273 , training accuracy :  0.851852\n",
      "i: 23700 , cost :  0.382455 , training accuracy :  0.829405\n",
      "i: 23800 , cost :  0.378986 , training accuracy :  0.837261\n",
      "i: 23900 , cost :  0.371045 , training accuracy :  0.85073\n",
      "i: 24000 , cost :  0.382557 , training accuracy :  0.841751\n",
      "i: 24100 , cost :  0.373143 , training accuracy :  0.84624\n",
      "i: 24200 , cost :  0.374243 , training accuracy :  0.840629\n",
      "i: 24300 , cost :  0.380647 , training accuracy :  0.837261\n",
      "i: 24400 , cost :  0.370776 , training accuracy :  0.851852\n",
      "i: 24500 , cost :  0.380377 , training accuracy :  0.833894\n",
      "i: 24600 , cost :  0.37628 , training accuracy :  0.843996\n",
      "i: 24700 , cost :  0.373705 , training accuracy :  0.849607\n",
      "i: 24800 , cost :  0.380796 , training accuracy :  0.83165\n",
      "i: 24900 , cost :  0.380768 , training accuracy :  0.840629\n",
      "i: 25000 , cost :  0.378131 , training accuracy :  0.842873\n",
      "i: 25100 , cost :  0.379074 , training accuracy :  0.833894\n",
      "i: 25200 , cost :  0.370045 , training accuracy :  0.852974\n",
      "i: 25300 , cost :  0.379868 , training accuracy :  0.843996\n",
      "i: 25400 , cost :  0.388151 , training accuracy :  0.826038\n",
      "i: 25500 , cost :  0.369844 , training accuracy :  0.85073\n",
      "i: 25600 , cost :  0.374198 , training accuracy :  0.84624\n",
      "i: 25700 , cost :  0.377684 , training accuracy :  0.837261\n",
      "i: 25800 , cost :  0.370771 , training accuracy :  0.847363\n",
      "i: 25900 , cost :  0.374224 , training accuracy :  0.845118\n",
      "i: 26000 , cost :  0.38473 , training accuracy :  0.828283\n",
      "i: 26100 , cost :  0.369709 , training accuracy :  0.852974\n",
      "i: 26200 , cost :  0.370528 , training accuracy :  0.852974\n",
      "i: 26300 , cost :  0.385295 , training accuracy :  0.841751\n",
      "i: 26400 , cost :  0.371845 , training accuracy :  0.848485\n",
      "i: 26500 , cost :  0.37005 , training accuracy :  0.848485\n",
      "i: 26600 , cost :  0.378521 , training accuracy :  0.841751\n",
      "i: 26700 , cost :  0.371497 , training accuracy :  0.85073\n",
      "i: 26800 , cost :  0.375332 , training accuracy :  0.839506\n",
      "i: 26900 , cost :  0.381104 , training accuracy :  0.843996\n",
      "i: 27000 , cost :  0.375079 , training accuracy :  0.842873\n",
      "i: 27100 , cost :  0.376332 , training accuracy :  0.836139\n",
      "i: 27200 , cost :  0.384002 , training accuracy :  0.839506\n",
      "i: 27300 , cost :  0.369766 , training accuracy :  0.855219\n",
      "i: 27400 , cost :  0.383272 , training accuracy :  0.829405\n",
      "i: 27500 , cost :  0.370081 , training accuracy :  0.854097\n",
      "i: 27600 , cost :  0.384947 , training accuracy :  0.839506\n",
      "i: 27700 , cost :  0.372557 , training accuracy :  0.84624\n",
      "i: 27800 , cost :  0.380851 , training accuracy :  0.841751\n",
      "i: 27900 , cost :  0.37087 , training accuracy :  0.849607\n",
      "i: 28000 , cost :  0.376323 , training accuracy :  0.837261\n",
      "i: 28100 , cost :  0.382369 , training accuracy :  0.838384\n",
      "i: 28200 , cost :  0.370536 , training accuracy :  0.85073\n",
      "i: 28300 , cost :  0.383813 , training accuracy :  0.841751\n",
      "i: 28400 , cost :  0.369776 , training accuracy :  0.852974\n",
      "i: 28500 , cost :  0.377085 , training accuracy :  0.836139\n",
      "i: 28600 , cost :  0.377069 , training accuracy :  0.843996\n",
      "i: 28700 , cost :  0.371499 , training accuracy :  0.84624\n",
      "i: 28800 , cost :  0.376626 , training accuracy :  0.835017\n",
      "i: 28900 , cost :  0.380108 , training accuracy :  0.839506\n",
      "i: 29000 , cost :  0.372299 , training accuracy :  0.849607\n",
      "i: 29100 , cost :  0.385938 , training accuracy :  0.824916\n",
      "i: 29200 , cost :  0.370802 , training accuracy :  0.848485\n",
      "i: 29300 , cost :  0.382034 , training accuracy :  0.841751\n",
      "i: 29400 , cost :  0.371246 , training accuracy :  0.85073\n",
      "i: 29500 , cost :  0.388633 , training accuracy :  0.838384\n",
      "i: 29600 , cost :  0.381234 , training accuracy :  0.833894\n",
      "i: 29700 , cost :  0.376361 , training accuracy :  0.837261\n",
      "i: 29800 , cost :  0.371139 , training accuracy :  0.85073\n",
      "i: 29900 , cost :  0.381731 , training accuracy :  0.841751\n",
      "Final training accuracy :  0.848485\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 30000 ][hidden: 15 ][file: bhavul_tr_acc_0.85_prediction.csv ] ACCURACY :  0.848485\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  214.722 , training accuracy :  0.320988\n",
      "i: 100 , cost :  16.7332 , training accuracy :  0.561167\n",
      "i: 200 , cost :  30.9326 , training accuracy :  0.401796\n",
      "i: 300 , cost :  7.94079 , training accuracy :  0.754209\n",
      "i: 400 , cost :  11.8826 , training accuracy :  0.737374\n",
      "i: 500 , cost :  3.52527 , training accuracy :  0.813693\n",
      "i: 600 , cost :  6.24669 , training accuracy :  0.705948\n",
      "i: 700 , cost :  5.64298 , training accuracy :  0.701459\n",
      "i: 800 , cost :  12.4214 , training accuracy :  0.538721\n",
      "i: 900 , cost :  4.01089 , training accuracy :  0.815937\n",
      "i: 1000 , cost :  5.38663 , training accuracy :  0.753086\n",
      "i: 1100 , cost :  4.66748 , training accuracy :  0.758698\n",
      "i: 1200 , cost :  3.58413 , training accuracy :  0.808081\n",
      "i: 1300 , cost :  29.2092 , training accuracy :  0.6633\n",
      "i: 1400 , cost :  8.33661 , training accuracy :  0.612795\n",
      "i: 1500 , cost :  23.7833 , training accuracy :  0.685746\n",
      "i: 1600 , cost :  1.63665 , training accuracy :  0.839506\n",
      "i: 1700 , cost :  3.43376 , training accuracy :  0.796857\n",
      "i: 1800 , cost :  4.19067 , training accuracy :  0.525253\n",
      "i: 1900 , cost :  4.31726 , training accuracy :  0.796857\n",
      "i: 2000 , cost :  2.73471 , training accuracy :  0.830527\n",
      "i: 2100 , cost :  16.1941 , training accuracy :  0.689113\n",
      "i: 2200 , cost :  1.62969 , training accuracy :  0.85073\n",
      "i: 2300 , cost :  13.3299 , training accuracy :  0.448934\n",
      "i: 2400 , cost :  4.39321 , training accuracy :  0.780022\n",
      "i: 2500 , cost :  2.48029 , training accuracy :  0.823793\n",
      "i: 2600 , cost :  1.39774 , training accuracy :  0.837261\n",
      "i: 2700 , cost :  5.75212 , training accuracy :  0.786756\n",
      "i: 2800 , cost :  9.77034 , training accuracy :  0.50505\n",
      "i: 2900 , cost :  4.30914 , training accuracy :  0.804714\n",
      "i: 3000 , cost :  2.47662 , training accuracy :  0.814815\n",
      "i: 3100 , cost :  16.6031 , training accuracy :  0.686869\n",
      "i: 3200 , cost :  1.08034 , training accuracy :  0.857464\n",
      "i: 3300 , cost :  6.53105 , training accuracy :  0.782267\n",
      "i: 3400 , cost :  5.68523 , training accuracy :  0.649832\n",
      "i: 3500 , cost :  1.20255 , training accuracy :  0.852974\n",
      "i: 3600 , cost :  2.32049 , training accuracy :  0.809203\n",
      "i: 3700 , cost :  8.92418 , training accuracy :  0.709315\n",
      "i: 3800 , cost :  6.28083 , training accuracy :  0.751964\n",
      "i: 3900 , cost :  4.79854 , training accuracy :  0.670034\n",
      "i: 4000 , cost :  3.17352 , training accuracy :  0.814815\n",
      "i: 4100 , cost :  0.976912 , training accuracy :  0.864198\n",
      "i: 4200 , cost :  3.28759 , training accuracy :  0.802469\n",
      "i: 4300 , cost :  12.6968 , training accuracy :  0.415264\n",
      "i: 4400 , cost :  1.25961 , training accuracy :  0.858586\n",
      "i: 4500 , cost :  0.722949 , training accuracy :  0.858586\n",
      "i: 4600 , cost :  1.32542 , training accuracy :  0.828283\n",
      "i: 4700 , cost :  2.26604 , training accuracy :  0.655443\n",
      "i: 4800 , cost :  0.803874 , training accuracy :  0.839506\n",
      "i: 4900 , cost :  0.784057 , training accuracy :  0.839506\n",
      "i: 5000 , cost :  0.956916 , training accuracy :  0.833894\n",
      "i: 5100 , cost :  1.84144 , training accuracy :  0.815937\n",
      "i: 5200 , cost :  0.579672 , training accuracy :  0.855219\n",
      "i: 5300 , cost :  0.774021 , training accuracy :  0.843996\n",
      "i: 5400 , cost :  2.97089 , training accuracy :  0.781145\n",
      "i: 5500 , cost :  1.38089 , training accuracy :  0.809203\n",
      "i: 5600 , cost :  0.971703 , training accuracy :  0.83165\n",
      "i: 5700 , cost :  0.442687 , training accuracy :  0.859708\n",
      "i: 5800 , cost :  0.679603 , training accuracy :  0.863075\n",
      "i: 5900 , cost :  1.16124 , training accuracy :  0.837261\n",
      "i: 6000 , cost :  0.488735 , training accuracy :  0.854097\n",
      "i: 6100 , cost :  1.54649 , training accuracy :  0.624018\n",
      "i: 6200 , cost :  0.428951 , training accuracy :  0.86532\n",
      "i: 6300 , cost :  0.488745 , training accuracy :  0.841751\n",
      "i: 6400 , cost :  4.10132 , training accuracy :  0.445567\n",
      "i: 6500 , cost :  0.605799 , training accuracy :  0.822671\n",
      "i: 6600 , cost :  0.474215 , training accuracy :  0.813693\n",
      "i: 6700 , cost :  1.0296 , training accuracy :  0.817059\n",
      "i: 6800 , cost :  0.434131 , training accuracy :  0.847363\n",
      "i: 6900 , cost :  1.43301 , training accuracy :  0.790123\n",
      "i: 7000 , cost :  0.529517 , training accuracy :  0.832772\n",
      "i: 7100 , cost :  0.318633 , training accuracy :  0.867565\n",
      "i: 7200 , cost :  0.328115 , training accuracy :  0.859708\n",
      "i: 7300 , cost :  0.343526 , training accuracy :  0.855219\n",
      "i: 7400 , cost :  1.36593 , training accuracy :  0.682379\n",
      "i: 7500 , cost :  0.369164 , training accuracy :  0.84624\n",
      "i: 7600 , cost :  0.318752 , training accuracy :  0.869809\n",
      "i: 7700 , cost :  0.348946 , training accuracy :  0.861953\n",
      "i: 7800 , cost :  1.5664 , training accuracy :  0.564534\n",
      "i: 7900 , cost :  0.348958 , training accuracy :  0.860831\n",
      "i: 8000 , cost :  0.409191 , training accuracy :  0.841751\n",
      "i: 8100 , cost :  0.311295 , training accuracy :  0.869809\n",
      "i: 8200 , cost :  0.304601 , training accuracy :  0.870932\n",
      "i: 8300 , cost :  0.318847 , training accuracy :  0.870932\n",
      "i: 8400 , cost :  0.328361 , training accuracy :  0.867565\n",
      "i: 8500 , cost :  0.339186 , training accuracy :  0.866442\n",
      "i: 8600 , cost :  0.314192 , training accuracy :  0.867565\n",
      "i: 8700 , cost :  0.336299 , training accuracy :  0.866442\n",
      "i: 8800 , cost :  0.34318 , training accuracy :  0.860831\n",
      "i: 8900 , cost :  0.384296 , training accuracy :  0.830527\n",
      "i: 9000 , cost :  0.56565 , training accuracy :  0.830527\n",
      "i: 9100 , cost :  0.460852 , training accuracy :  0.794613\n",
      "i: 9200 , cost :  0.337154 , training accuracy :  0.863075\n",
      "i: 9300 , cost :  0.375647 , training accuracy :  0.856341\n",
      "i: 9400 , cost :  0.297564 , training accuracy :  0.875421\n",
      "i: 9500 , cost :  0.293978 , training accuracy :  0.877666\n",
      "i: 9600 , cost :  0.292045 , training accuracy :  0.87991\n",
      "i: 9700 , cost :  0.29407 , training accuracy :  0.878788\n",
      "i: 9800 , cost :  0.300936 , training accuracy :  0.876543\n",
      "i: 9900 , cost :  0.306248 , training accuracy :  0.874299\n",
      "i: 10000 , cost :  0.545395 , training accuracy :  0.82716\n",
      "i: 10100 , cost :  0.348126 , training accuracy :  0.845118\n",
      "i: 10200 , cost :  0.292032 , training accuracy :  0.885522\n",
      "i: 10300 , cost :  0.289926 , training accuracy :  0.883277\n",
      "i: 10400 , cost :  0.28954 , training accuracy :  0.8844\n",
      "i: 10500 , cost :  0.289597 , training accuracy :  0.877666\n",
      "i: 10600 , cost :  0.288244 , training accuracy :  0.8844\n",
      "i: 10700 , cost :  0.299261 , training accuracy :  0.869809\n",
      "i: 10800 , cost :  0.329208 , training accuracy :  0.867565\n",
      "i: 10900 , cost :  0.308522 , training accuracy :  0.869809\n",
      "i: 11000 , cost :  0.34215 , training accuracy :  0.866442\n",
      "i: 11100 , cost :  0.291801 , training accuracy :  0.878788\n",
      "i: 11200 , cost :  0.288392 , training accuracy :  0.876543\n",
      "i: 11300 , cost :  0.286436 , training accuracy :  0.881033\n",
      "i: 11400 , cost :  0.284618 , training accuracy :  0.883277\n",
      "i: 11500 , cost :  0.283492 , training accuracy :  0.883277\n",
      "i: 11600 , cost :  0.289171 , training accuracy :  0.883277\n",
      "i: 11700 , cost :  0.587337 , training accuracy :  0.791246\n",
      "i: 11800 , cost :  0.298956 , training accuracy :  0.875421\n",
      "i: 11900 , cost :  0.292009 , training accuracy :  0.878788\n",
      "i: 12000 , cost :  0.289671 , training accuracy :  0.878788\n",
      "i: 12100 , cost :  0.300221 , training accuracy :  0.870932\n",
      "i: 12200 , cost :  0.28693 , training accuracy :  0.872054\n",
      "i: 12300 , cost :  0.386701 , training accuracy :  0.833894\n",
      "i: 12400 , cost :  0.298698 , training accuracy :  0.867565\n",
      "i: 12500 , cost :  0.303768 , training accuracy :  0.857464\n",
      "i: 12600 , cost :  0.297084 , training accuracy :  0.870932\n",
      "i: 12700 , cost :  0.287118 , training accuracy :  0.875421\n",
      "i: 12800 , cost :  0.289226 , training accuracy :  0.86532\n",
      "i: 12900 , cost :  0.709621 , training accuracy :  0.682379\n",
      "i: 13000 , cost :  0.301308 , training accuracy :  0.869809\n",
      "i: 13100 , cost :  0.296568 , training accuracy :  0.875421\n",
      "i: 13200 , cost :  0.302096 , training accuracy :  0.869809\n",
      "i: 13300 , cost :  0.32998 , training accuracy :  0.855219\n",
      "i: 13400 , cost :  0.328492 , training accuracy :  0.859708\n",
      "i: 13500 , cost :  0.291788 , training accuracy :  0.874299\n",
      "i: 13600 , cost :  0.283831 , training accuracy :  0.876543\n",
      "i: 13700 , cost :  0.2912 , training accuracy :  0.877666\n",
      "i: 13800 , cost :  0.289684 , training accuracy :  0.87991\n",
      "i: 13900 , cost :  0.280584 , training accuracy :  0.881033\n",
      "i: 14000 , cost :  0.278139 , training accuracy :  0.882155\n",
      "i: 14100 , cost :  0.275162 , training accuracy :  0.882155\n",
      "i: 14200 , cost :  0.280558 , training accuracy :  0.887767\n",
      "i: 14300 , cost :  0.277199 , training accuracy :  0.885522\n",
      "i: 14400 , cost :  0.311895 , training accuracy :  0.863075\n",
      "i: 14500 , cost :  0.268783 , training accuracy :  0.885522\n",
      "i: 14600 , cost :  0.270596 , training accuracy :  0.891134\n",
      "i: 14700 , cost :  0.308211 , training accuracy :  0.867565\n",
      "i: 14800 , cost :  0.26849 , training accuracy :  0.890011\n",
      "i: 14900 , cost :  0.320028 , training accuracy :  0.859708\n",
      "i: 15000 , cost :  0.28065 , training accuracy :  0.885522\n",
      "i: 15100 , cost :  0.275622 , training accuracy :  0.886644\n",
      "i: 15200 , cost :  0.278259 , training accuracy :  0.890011\n",
      "i: 15300 , cost :  0.29944 , training accuracy :  0.876543\n",
      "i: 15400 , cost :  0.290296 , training accuracy :  0.874299\n",
      "i: 15500 , cost :  0.302908 , training accuracy :  0.875421\n",
      "i: 15600 , cost :  0.277748 , training accuracy :  0.886644\n",
      "i: 15700 , cost :  0.32129 , training accuracy :  0.864198\n",
      "i: 15800 , cost :  0.302834 , training accuracy :  0.875421\n",
      "i: 15900 , cost :  0.296683 , training accuracy :  0.877666\n",
      "i: 16000 , cost :  0.300045 , training accuracy :  0.869809\n",
      "i: 16100 , cost :  0.294784 , training accuracy :  0.875421\n",
      "i: 16200 , cost :  0.309349 , training accuracy :  0.873176\n",
      "i: 16300 , cost :  0.304052 , training accuracy :  0.869809\n",
      "i: 16400 , cost :  0.297526 , training accuracy :  0.869809\n",
      "i: 16500 , cost :  0.319571 , training accuracy :  0.866442\n",
      "i: 16600 , cost :  0.287916 , training accuracy :  0.8844\n",
      "i: 16700 , cost :  0.31573 , training accuracy :  0.867565\n",
      "i: 16800 , cost :  0.299664 , training accuracy :  0.8844\n",
      "i: 16900 , cost :  0.297764 , training accuracy :  0.87991\n",
      "i: 17000 , cost :  0.302505 , training accuracy :  0.877666\n",
      "i: 17100 , cost :  0.300483 , training accuracy :  0.883277\n",
      "i: 17200 , cost :  0.309541 , training accuracy :  0.869809\n",
      "i: 17300 , cost :  0.29961 , training accuracy :  0.873176\n",
      "i: 17400 , cost :  0.304857 , training accuracy :  0.875421\n",
      "i: 17500 , cost :  0.322705 , training accuracy :  0.864198\n",
      "i: 17600 , cost :  0.301273 , training accuracy :  0.874299\n",
      "i: 17700 , cost :  0.308963 , training accuracy :  0.873176\n",
      "i: 17800 , cost :  0.292793 , training accuracy :  0.877666\n",
      "i: 17900 , cost :  0.29578 , training accuracy :  0.878788\n",
      "i: 18000 , cost :  0.291124 , training accuracy :  0.87991\n",
      "i: 18100 , cost :  0.294362 , training accuracy :  0.881033\n",
      "i: 18200 , cost :  0.32807 , training accuracy :  0.861953\n",
      "i: 18300 , cost :  0.294085 , training accuracy :  0.882155\n",
      "i: 18400 , cost :  0.298742 , training accuracy :  0.875421\n",
      "i: 18500 , cost :  0.303182 , training accuracy :  0.874299\n",
      "i: 18600 , cost :  0.29557 , training accuracy :  0.877666\n",
      "i: 18700 , cost :  0.301482 , training accuracy :  0.878788\n",
      "i: 18800 , cost :  0.288116 , training accuracy :  0.886644\n",
      "i: 18900 , cost :  0.288055 , training accuracy :  0.881033\n",
      "i: 19000 , cost :  0.322418 , training accuracy :  0.873176\n",
      "i: 19100 , cost :  0.317516 , training accuracy :  0.861953\n",
      "i: 19200 , cost :  0.313618 , training accuracy :  0.872054\n",
      "i: 19300 , cost :  0.319724 , training accuracy :  0.864198\n",
      "i: 19400 , cost :  0.334624 , training accuracy :  0.859708\n",
      "i: 19500 , cost :  0.349196 , training accuracy :  0.851852\n",
      "i: 19600 , cost :  0.313006 , training accuracy :  0.864198\n",
      "i: 19700 , cost :  0.338089 , training accuracy :  0.856341\n",
      "i: 19800 , cost :  0.324929 , training accuracy :  0.86532\n",
      "i: 19900 , cost :  0.308576 , training accuracy :  0.873176\n",
      "i: 20000 , cost :  0.351937 , training accuracy :  0.848485\n",
      "i: 20100 , cost :  0.314646 , training accuracy :  0.868687\n",
      "i: 20200 , cost :  0.328936 , training accuracy :  0.859708\n",
      "i: 20300 , cost :  0.32726 , training accuracy :  0.867565\n",
      "i: 20400 , cost :  0.313918 , training accuracy :  0.867565\n",
      "i: 20500 , cost :  0.308084 , training accuracy :  0.875421\n",
      "i: 20600 , cost :  0.312327 , training accuracy :  0.866442\n",
      "i: 20700 , cost :  0.305268 , training accuracy :  0.870932\n",
      "i: 20800 , cost :  0.331485 , training accuracy :  0.867565\n",
      "i: 20900 , cost :  0.309385 , training accuracy :  0.869809\n",
      "i: 21000 , cost :  0.309994 , training accuracy :  0.872054\n",
      "i: 21100 , cost :  0.328049 , training accuracy :  0.869809\n",
      "i: 21200 , cost :  0.321481 , training accuracy :  0.872054\n",
      "i: 21300 , cost :  0.310721 , training accuracy :  0.872054\n",
      "i: 21400 , cost :  0.298455 , training accuracy :  0.875421\n",
      "i: 21500 , cost :  0.299092 , training accuracy :  0.874299\n",
      "i: 21600 , cost :  0.322664 , training accuracy :  0.86532\n",
      "i: 21700 , cost :  0.335144 , training accuracy :  0.86532\n",
      "i: 21800 , cost :  0.318743 , training accuracy :  0.870932\n",
      "i: 21900 , cost :  0.299914 , training accuracy :  0.869809\n",
      "i: 22000 , cost :  0.338282 , training accuracy :  0.857464\n",
      "i: 22100 , cost :  0.297576 , training accuracy :  0.881033\n",
      "i: 22200 , cost :  0.300676 , training accuracy :  0.873176\n",
      "i: 22300 , cost :  0.317566 , training accuracy :  0.863075\n",
      "i: 22400 , cost :  0.348842 , training accuracy :  0.859708\n",
      "i: 22500 , cost :  0.298948 , training accuracy :  0.878788\n",
      "i: 22600 , cost :  0.318203 , training accuracy :  0.863075\n",
      "i: 22700 , cost :  0.316825 , training accuracy :  0.867565\n",
      "i: 22800 , cost :  0.329864 , training accuracy :  0.854097\n",
      "i: 22900 , cost :  0.326872 , training accuracy :  0.863075\n",
      "i: 23000 , cost :  0.3262 , training accuracy :  0.861953\n",
      "i: 23100 , cost :  0.340522 , training accuracy :  0.861953\n",
      "i: 23200 , cost :  0.358759 , training accuracy :  0.851852\n",
      "i: 23300 , cost :  0.327589 , training accuracy :  0.866442\n",
      "i: 23400 , cost :  0.349933 , training accuracy :  0.847363\n",
      "i: 23500 , cost :  0.339382 , training accuracy :  0.857464\n",
      "i: 23600 , cost :  0.320192 , training accuracy :  0.864198\n",
      "i: 23700 , cost :  0.345123 , training accuracy :  0.855219\n",
      "i: 23800 , cost :  0.327198 , training accuracy :  0.863075\n",
      "i: 23900 , cost :  0.339799 , training accuracy :  0.851852\n",
      "i: 24000 , cost :  0.319717 , training accuracy :  0.863075\n",
      "i: 24100 , cost :  0.323171 , training accuracy :  0.864198\n",
      "i: 24200 , cost :  0.318804 , training accuracy :  0.861953\n",
      "i: 24300 , cost :  0.322024 , training accuracy :  0.868687\n",
      "i: 24400 , cost :  0.319804 , training accuracy :  0.860831\n",
      "i: 24500 , cost :  0.316654 , training accuracy :  0.869809\n",
      "i: 24600 , cost :  0.320334 , training accuracy :  0.868687\n",
      "i: 24700 , cost :  0.317481 , training accuracy :  0.867565\n",
      "i: 24800 , cost :  0.32114 , training accuracy :  0.868687\n",
      "i: 24900 , cost :  0.354545 , training accuracy :  0.858586\n",
      "i: 25000 , cost :  0.314377 , training accuracy :  0.866442\n",
      "i: 25100 , cost :  0.402726 , training accuracy :  0.833894\n",
      "i: 25200 , cost :  0.322084 , training accuracy :  0.859708\n",
      "i: 25300 , cost :  0.326608 , training accuracy :  0.859708\n",
      "i: 25400 , cost :  0.320905 , training accuracy :  0.863075\n",
      "i: 25500 , cost :  0.319793 , training accuracy :  0.860831\n",
      "i: 25600 , cost :  0.31915 , training accuracy :  0.866442\n",
      "i: 25700 , cost :  0.31923 , training accuracy :  0.860831\n",
      "i: 25800 , cost :  0.365301 , training accuracy :  0.849607\n",
      "i: 25900 , cost :  0.329865 , training accuracy :  0.857464\n",
      "i: 26000 , cost :  0.326062 , training accuracy :  0.859708\n",
      "i: 26100 , cost :  0.3177 , training accuracy :  0.86532\n",
      "i: 26200 , cost :  0.320771 , training accuracy :  0.860831\n",
      "i: 26300 , cost :  0.357098 , training accuracy :  0.848485\n",
      "i: 26400 , cost :  0.323986 , training accuracy :  0.863075\n",
      "i: 26500 , cost :  0.31912 , training accuracy :  0.864198\n",
      "i: 26600 , cost :  0.335708 , training accuracy :  0.859708\n",
      "i: 26700 , cost :  0.317473 , training accuracy :  0.863075\n",
      "i: 26800 , cost :  0.32505 , training accuracy :  0.859708\n",
      "i: 26900 , cost :  0.321905 , training accuracy :  0.864198\n",
      "i: 27000 , cost :  0.34911 , training accuracy :  0.847363\n",
      "i: 27100 , cost :  0.316584 , training accuracy :  0.863075\n",
      "i: 27200 , cost :  0.364863 , training accuracy :  0.840629\n",
      "i: 27300 , cost :  0.342155 , training accuracy :  0.843996\n",
      "i: 27400 , cost :  0.333962 , training accuracy :  0.855219\n",
      "i: 27500 , cost :  0.456296 , training accuracy :  0.801347\n",
      "i: 27600 , cost :  0.328974 , training accuracy :  0.86532\n",
      "i: 27700 , cost :  0.357152 , training accuracy :  0.854097\n",
      "i: 27800 , cost :  0.324573 , training accuracy :  0.860831\n",
      "i: 27900 , cost :  0.347379 , training accuracy :  0.85073\n",
      "i: 28000 , cost :  0.356932 , training accuracy :  0.849607\n",
      "i: 28100 , cost :  0.322335 , training accuracy :  0.863075\n",
      "i: 28200 , cost :  0.326443 , training accuracy :  0.861953\n",
      "i: 28300 , cost :  0.328147 , training accuracy :  0.858586\n",
      "i: 28400 , cost :  0.340382 , training accuracy :  0.855219\n",
      "i: 28500 , cost :  0.400467 , training accuracy :  0.835017\n",
      "i: 28600 , cost :  0.311962 , training accuracy :  0.859708\n",
      "i: 28700 , cost :  0.313403 , training accuracy :  0.863075\n",
      "i: 28800 , cost :  0.337554 , training accuracy :  0.851852\n",
      "i: 28900 , cost :  0.318447 , training accuracy :  0.855219\n",
      "i: 29000 , cost :  0.312765 , training accuracy :  0.860831\n",
      "i: 29100 , cost :  0.345598 , training accuracy :  0.852974\n",
      "i: 29200 , cost :  0.314316 , training accuracy :  0.863075\n",
      "i: 29300 , cost :  0.34716 , training accuracy :  0.857464\n",
      "i: 29400 , cost :  0.333241 , training accuracy :  0.852974\n",
      "i: 29500 , cost :  0.327571 , training accuracy :  0.858586\n",
      "i: 29600 , cost :  0.372526 , training accuracy :  0.854097\n",
      "i: 29700 , cost :  0.325065 , training accuracy :  0.861953\n",
      "i: 29800 , cost :  0.35111 , training accuracy :  0.855219\n",
      "i: 29900 , cost :  0.405338 , training accuracy :  0.840629\n",
      "Final training accuracy :  0.851852\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 30000 ][hidden: 50 ][file: bhavul_tr_acc_0.85_prediction.csv ] ACCURACY :  0.851852\n",
      "================================================== \n",
      "\n",
      "Filling NaN values for column: Age\n",
      "Filling NaN values for column: Age  in test data\n",
      "Filling NaN values for column: Fare  in test data\n",
      "Filling NaN values for column: Cabin\n",
      "Filling NaN values for column: Cabin  in test data\n",
      "Filling NaN values for column: Embarked\n",
      "Fixed NaN values in training and testing data.\n",
      "i: 0 , cost :  2741.51 , training accuracy :  0.378227\n",
      "i: 100 , cost :  19.7439 , training accuracy :  0.723906\n",
      "i: 200 , cost :  6.52125 , training accuracy :  0.780022\n",
      "i: 300 , cost :  3.79852 , training accuracy :  0.790123\n",
      "i: 400 , cost :  13.9845 , training accuracy :  0.735129\n",
      "i: 500 , cost :  11.3402 , training accuracy :  0.741863\n",
      "i: 600 , cost :  3.09789 , training accuracy :  0.829405\n",
      "i: 700 , cost :  10.6051 , training accuracy :  0.749719\n",
      "i: 800 , cost :  2.58467 , training accuracy :  0.842873\n",
      "i: 900 , cost :  3.30342 , training accuracy :  0.815937\n",
      "i: 1000 , cost :  3.38254 , training accuracy :  0.806958\n",
      "i: 1100 , cost :  16.778 , training accuracy :  0.626263\n",
      "i: 1200 , cost :  2.29005 , training accuracy :  0.826038\n",
      "i: 1300 , cost :  6.34098 , training accuracy :  0.777778\n",
      "i: 1400 , cost :  5.2438 , training accuracy :  0.767677\n",
      "i: 1500 , cost :  2.64869 , training accuracy :  0.836139\n",
      "i: 1600 , cost :  12.2357 , training accuracy :  0.555556\n",
      "i: 1700 , cost :  7.8229 , training accuracy :  0.672278\n",
      "i: 1800 , cost :  12.4795 , training accuracy :  0.69697\n",
      "i: 1900 , cost :  3.1396 , training accuracy :  0.709315\n",
      "i: 2000 , cost :  3.35357 , training accuracy :  0.810326\n",
      "i: 2100 , cost :  1.80379 , training accuracy :  0.824916\n",
      "i: 2200 , cost :  1.30443 , training accuracy :  0.840629\n",
      "i: 2300 , cost :  2.0394 , training accuracy :  0.829405\n",
      "i: 2400 , cost :  4.63892 , training accuracy :  0.791246\n",
      "i: 2500 , cost :  4.14765 , training accuracy :  0.75982\n",
      "i: 2600 , cost :  1.83474 , training accuracy :  0.842873\n",
      "i: 2700 , cost :  6.34328 , training accuracy :  0.760943\n",
      "i: 2800 , cost :  1.63234 , training accuracy :  0.85073\n",
      "i: 2900 , cost :  1.46153 , training accuracy :  0.787879\n",
      "i: 3000 , cost :  3.14863 , training accuracy :  0.800224\n",
      "i: 3100 , cost :  1.37444 , training accuracy :  0.7789\n",
      "i: 3200 , cost :  10.5472 , training accuracy :  0.427609\n",
      "i: 3300 , cost :  0.905353 , training accuracy :  0.860831\n",
      "i: 3400 , cost :  2.14443 , training accuracy :  0.801347\n",
      "i: 3500 , cost :  1.12759 , training accuracy :  0.864198\n",
      "i: 3600 , cost :  4.10295 , training accuracy :  0.602694\n",
      "i: 3700 , cost :  1.25441 , training accuracy :  0.849607\n",
      "i: 3800 , cost :  1.85886 , training accuracy :  0.787879\n",
      "i: 3900 , cost :  3.30606 , training accuracy :  0.655443\n",
      "i: 4000 , cost :  0.89144 , training accuracy :  0.821549\n",
      "i: 4100 , cost :  1.05908 , training accuracy :  0.843996\n",
      "i: 4200 , cost :  1.87545 , training accuracy :  0.806958\n",
      "i: 4300 , cost :  13.1941 , training accuracy :  0.409652\n",
      "i: 4400 , cost :  0.763426 , training accuracy :  0.882155\n",
      "i: 4500 , cost :  3.11473 , training accuracy :  0.790123\n",
      "i: 4600 , cost :  0.681388 , training accuracy :  0.872054\n",
      "i: 4700 , cost :  9.34713 , training accuracy :  0.421998\n",
      "i: 4800 , cost :  0.618162 , training accuracy :  0.859708\n",
      "i: 4900 , cost :  1.01018 , training accuracy :  0.83165\n",
      "i: 5000 , cost :  0.559561 , training accuracy :  0.852974\n",
      "i: 5100 , cost :  4.42477 , training accuracy :  0.537598\n",
      "i: 5200 , cost :  7.42671 , training accuracy :  0.700337\n",
      "i: 5300 , cost :  0.603404 , training accuracy :  0.851852\n",
      "i: 5400 , cost :  2.96504 , training accuracy :  0.757576\n",
      "i: 5500 , cost :  0.796608 , training accuracy :  0.854097\n",
      "i: 5600 , cost :  0.516522 , training accuracy :  0.854097\n",
      "i: 5700 , cost :  0.509207 , training accuracy :  0.848485\n",
      "i: 5800 , cost :  0.876356 , training accuracy :  0.849607\n",
      "i: 5900 , cost :  0.595583 , training accuracy :  0.838384\n",
      "i: 6000 , cost :  0.945265 , training accuracy :  0.79798\n",
      "i: 6100 , cost :  0.599692 , training accuracy :  0.849607\n",
      "i: 6200 , cost :  0.654976 , training accuracy :  0.845118\n",
      "i: 6300 , cost :  0.559702 , training accuracy :  0.863075\n",
      "i: 6400 , cost :  2.73326 , training accuracy :  0.800224\n",
      "i: 6500 , cost :  0.361346 , training accuracy :  0.882155\n",
      "i: 6600 , cost :  2.60001 , training accuracy :  0.751964\n",
      "i: 6700 , cost :  0.453663 , training accuracy :  0.875421\n",
      "i: 6800 , cost :  1.36165 , training accuracy :  0.830527\n",
      "i: 6900 , cost :  0.731685 , training accuracy :  0.859708\n",
      "i: 7000 , cost :  0.397875 , training accuracy :  0.854097\n",
      "i: 7100 , cost :  0.431659 , training accuracy :  0.873176\n",
      "i: 7200 , cost :  0.406154 , training accuracy :  0.843996\n",
      "i: 7300 , cost :  1.89234 , training accuracy :  0.563412\n",
      "i: 7400 , cost :  0.33419 , training accuracy :  0.890011\n",
      "i: 7500 , cost :  3.39594 , training accuracy :  0.487093\n",
      "i: 7600 , cost :  0.671088 , training accuracy :  0.821549\n",
      "i: 7700 , cost :  1.00013 , training accuracy :  0.709315\n",
      "i: 7800 , cost :  0.287494 , training accuracy :  0.895623\n",
      "i: 7900 , cost :  0.479428 , training accuracy :  0.848485\n",
      "i: 8000 , cost :  0.822475 , training accuracy :  0.756453\n",
      "i: 8100 , cost :  0.376263 , training accuracy :  0.878788\n",
      "i: 8200 , cost :  1.54585 , training accuracy :  0.826038\n",
      "i: 8300 , cost :  0.361445 , training accuracy :  0.882155\n",
      "i: 8400 , cost :  0.524719 , training accuracy :  0.839506\n",
      "i: 8500 , cost :  2.15537 , training accuracy :  0.776655\n",
      "i: 8600 , cost :  1.06006 , training accuracy :  0.773288\n",
      "i: 8700 , cost :  1.1398 , training accuracy :  0.836139\n",
      "i: 8800 , cost :  1.45916 , training accuracy :  0.717172\n",
      "i: 8900 , cost :  1.70931 , training accuracy :  0.785634\n",
      "i: 9000 , cost :  0.413502 , training accuracy :  0.8844\n",
      "i: 9100 , cost :  0.295743 , training accuracy :  0.893378\n",
      "i: 9200 , cost :  0.271558 , training accuracy :  0.886644\n",
      "i: 9300 , cost :  0.269772 , training accuracy :  0.891134\n",
      "i: 9400 , cost :  0.267887 , training accuracy :  0.888889\n",
      "i: 9500 , cost :  0.273774 , training accuracy :  0.8844\n",
      "i: 9600 , cost :  0.408346 , training accuracy :  0.870932\n",
      "i: 9700 , cost :  2.10432 , training accuracy :  0.503928\n",
      "i: 9800 , cost :  0.969823 , training accuracy :  0.822671\n",
      "i: 9900 , cost :  0.278256 , training accuracy :  0.895623\n",
      "i: 10000 , cost :  0.258066 , training accuracy :  0.900112\n",
      "i: 10100 , cost :  0.35454 , training accuracy :  0.857464\n",
      "i: 10200 , cost :  0.471617 , training accuracy :  0.843996\n",
      "i: 10300 , cost :  0.85053 , training accuracy :  0.822671\n",
      "i: 10400 , cost :  0.267619 , training accuracy :  0.900112\n",
      "i: 10500 , cost :  0.256982 , training accuracy :  0.893378\n",
      "i: 10600 , cost :  0.251401 , training accuracy :  0.895623\n",
      "i: 10700 , cost :  0.252904 , training accuracy :  0.89899\n",
      "i: 10800 , cost :  0.247247 , training accuracy :  0.901235\n",
      "i: 10900 , cost :  2.06064 , training accuracy :  0.773288\n",
      "i: 11000 , cost :  0.727192 , training accuracy :  0.83165\n",
      "i: 11100 , cost :  0.271311 , training accuracy :  0.888889\n",
      "i: 11200 , cost :  0.267866 , training accuracy :  0.891134\n",
      "i: 11300 , cost :  0.265167 , training accuracy :  0.893378\n",
      "i: 11400 , cost :  0.279238 , training accuracy :  0.883277\n",
      "i: 11500 , cost :  0.274993 , training accuracy :  0.888889\n",
      "i: 11600 , cost :  0.357936 , training accuracy :  0.856341\n",
      "i: 11700 , cost :  0.268956 , training accuracy :  0.893378\n",
      "i: 11800 , cost :  0.31591 , training accuracy :  0.87991\n",
      "i: 11900 , cost :  0.325494 , training accuracy :  0.876543\n",
      "i: 12000 , cost :  0.258399 , training accuracy :  0.896745\n",
      "i: 12100 , cost :  0.266055 , training accuracy :  0.885522\n",
      "i: 12200 , cost :  0.352551 , training accuracy :  0.855219\n",
      "i: 12300 , cost :  0.257586 , training accuracy :  0.897868\n",
      "i: 12400 , cost :  0.32613 , training accuracy :  0.875421\n",
      "i: 12500 , cost :  0.326365 , training accuracy :  0.873176\n",
      "i: 12600 , cost :  0.33903 , training accuracy :  0.857464\n",
      "i: 12700 , cost :  0.271137 , training accuracy :  0.893378\n",
      "i: 12800 , cost :  1.42028 , training accuracy :  0.765432\n",
      "i: 12900 , cost :  0.766294 , training accuracy :  0.704826\n",
      "i: 13000 , cost :  0.29461 , training accuracy :  0.874299\n",
      "i: 13100 , cost :  0.286082 , training accuracy :  0.8844\n",
      "i: 13200 , cost :  0.279121 , training accuracy :  0.882155\n",
      "i: 13300 , cost :  0.275399 , training accuracy :  0.888889\n",
      "i: 13400 , cost :  0.272337 , training accuracy :  0.890011\n",
      "i: 13500 , cost :  0.269718 , training accuracy :  0.890011\n",
      "i: 13600 , cost :  0.267285 , training accuracy :  0.891134\n",
      "i: 13700 , cost :  0.260704 , training accuracy :  0.890011\n",
      "i: 13800 , cost :  0.264703 , training accuracy :  0.885522\n",
      "i: 13900 , cost :  0.290461 , training accuracy :  0.867565\n",
      "i: 14000 , cost :  0.257825 , training accuracy :  0.8844\n",
      "i: 14100 , cost :  0.257762 , training accuracy :  0.891134\n",
      "i: 14200 , cost :  0.299641 , training accuracy :  0.872054\n",
      "i: 14300 , cost :  0.292133 , training accuracy :  0.885522\n",
      "i: 14400 , cost :  0.275267 , training accuracy :  0.875421\n",
      "i: 14500 , cost :  0.258854 , training accuracy :  0.887767\n",
      "i: 14600 , cost :  0.382181 , training accuracy :  0.828283\n",
      "i: 14700 , cost :  0.336694 , training accuracy :  0.849607\n",
      "i: 14800 , cost :  0.332114 , training accuracy :  0.852974\n",
      "i: 14900 , cost :  0.32747 , training accuracy :  0.858586\n",
      "i: 15000 , cost :  0.325474 , training accuracy :  0.859708\n",
      "i: 15100 , cost :  0.323741 , training accuracy :  0.860831\n",
      "i: 15200 , cost :  0.322488 , training accuracy :  0.863075\n",
      "i: 15300 , cost :  0.322268 , training accuracy :  0.857464\n",
      "i: 15400 , cost :  0.329152 , training accuracy :  0.852974\n",
      "i: 15500 , cost :  0.360678 , training accuracy :  0.84624\n",
      "i: 15600 , cost :  0.319539 , training accuracy :  0.866442\n",
      "i: 15700 , cost :  0.319406 , training accuracy :  0.857464\n",
      "i: 15800 , cost :  0.328601 , training accuracy :  0.858586\n",
      "i: 15900 , cost :  0.317137 , training accuracy :  0.859708\n",
      "i: 16000 , cost :  0.360361 , training accuracy :  0.85073\n",
      "i: 16100 , cost :  0.34475 , training accuracy :  0.84624\n",
      "i: 16200 , cost :  0.336392 , training accuracy :  0.847363\n",
      "i: 16300 , cost :  0.326308 , training accuracy :  0.860831\n",
      "i: 16400 , cost :  0.333496 , training accuracy :  0.847363\n",
      "i: 16500 , cost :  0.326284 , training accuracy :  0.848485\n",
      "i: 16600 , cost :  0.31866 , training accuracy :  0.867565\n",
      "i: 16700 , cost :  0.317686 , training accuracy :  0.854097\n",
      "i: 16800 , cost :  0.316132 , training accuracy :  0.866442\n",
      "i: 16900 , cost :  0.32866 , training accuracy :  0.84624\n",
      "i: 17000 , cost :  0.346761 , training accuracy :  0.840629\n",
      "i: 17100 , cost :  0.319626 , training accuracy :  0.858586\n",
      "i: 17200 , cost :  0.325754 , training accuracy :  0.85073\n",
      "i: 17300 , cost :  0.317165 , training accuracy :  0.860831\n",
      "i: 17400 , cost :  0.332393 , training accuracy :  0.855219\n",
      "i: 17500 , cost :  0.315299 , training accuracy :  0.867565\n",
      "i: 17600 , cost :  0.376654 , training accuracy :  0.833894\n",
      "i: 17700 , cost :  0.320618 , training accuracy :  0.849607\n",
      "i: 17800 , cost :  0.317349 , training accuracy :  0.86532\n",
      "i: 17900 , cost :  0.349176 , training accuracy :  0.849607\n",
      "i: 18000 , cost :  0.346875 , training accuracy :  0.854097\n",
      "i: 18100 , cost :  0.332062 , training accuracy :  0.851852\n",
      "i: 18200 , cost :  0.312644 , training accuracy :  0.866442\n",
      "i: 18300 , cost :  0.31717 , training accuracy :  0.855219\n",
      "i: 18400 , cost :  0.315773 , training accuracy :  0.866442\n",
      "i: 18500 , cost :  0.310838 , training accuracy :  0.873176\n",
      "i: 18600 , cost :  0.327629 , training accuracy :  0.861953\n",
      "i: 18700 , cost :  0.327779 , training accuracy :  0.864198\n",
      "i: 18800 , cost :  0.33249 , training accuracy :  0.856341\n",
      "i: 18900 , cost :  0.307332 , training accuracy :  0.873176\n",
      "i: 19000 , cost :  0.309591 , training accuracy :  0.863075\n",
      "i: 19100 , cost :  0.316842 , training accuracy :  0.861953\n",
      "i: 19200 , cost :  0.318511 , training accuracy :  0.854097\n",
      "i: 19300 , cost :  0.310499 , training accuracy :  0.860831\n",
      "i: 19400 , cost :  0.320272 , training accuracy :  0.858586\n",
      "i: 19500 , cost :  0.312991 , training accuracy :  0.863075\n",
      "i: 19600 , cost :  0.323891 , training accuracy :  0.861953\n",
      "i: 19700 , cost :  0.331565 , training accuracy :  0.86532\n",
      "i: 19800 , cost :  0.315628 , training accuracy :  0.861953\n",
      "i: 19900 , cost :  0.388151 , training accuracy :  0.837261\n",
      "i: 20000 , cost :  0.363473 , training accuracy :  0.837261\n",
      "i: 20100 , cost :  0.327126 , training accuracy :  0.860831\n",
      "i: 20200 , cost :  0.319463 , training accuracy :  0.870932\n",
      "i: 20300 , cost :  0.318687 , training accuracy :  0.857464\n",
      "i: 20400 , cost :  0.325783 , training accuracy :  0.86532\n",
      "i: 20500 , cost :  0.360187 , training accuracy :  0.837261\n",
      "i: 20600 , cost :  0.327156 , training accuracy :  0.86532\n",
      "i: 20700 , cost :  0.341203 , training accuracy :  0.864198\n",
      "i: 20800 , cost :  0.325066 , training accuracy :  0.86532\n",
      "i: 20900 , cost :  0.338246 , training accuracy :  0.86532\n",
      "i: 21000 , cost :  0.31716 , training accuracy :  0.860831\n",
      "i: 21100 , cost :  0.316056 , training accuracy :  0.861953\n",
      "i: 21200 , cost :  0.320854 , training accuracy :  0.863075\n",
      "i: 21300 , cost :  0.324961 , training accuracy :  0.857464\n",
      "i: 21400 , cost :  0.359406 , training accuracy :  0.843996\n",
      "i: 21500 , cost :  0.346791 , training accuracy :  0.851852\n",
      "i: 21600 , cost :  0.323528 , training accuracy :  0.864198\n",
      "i: 21700 , cost :  0.323111 , training accuracy :  0.863075\n",
      "i: 21800 , cost :  0.330581 , training accuracy :  0.864198\n",
      "i: 21900 , cost :  0.328804 , training accuracy :  0.858586\n",
      "i: 22000 , cost :  0.375008 , training accuracy :  0.837261\n",
      "i: 22100 , cost :  0.33398 , training accuracy :  0.868687\n",
      "i: 22200 , cost :  0.336942 , training accuracy :  0.863075\n",
      "i: 22300 , cost :  0.376491 , training accuracy :  0.833894\n",
      "i: 22400 , cost :  0.375849 , training accuracy :  0.829405\n",
      "i: 22500 , cost :  0.378052 , training accuracy :  0.842873\n",
      "i: 22600 , cost :  0.370672 , training accuracy :  0.843996\n",
      "i: 22700 , cost :  0.366233 , training accuracy :  0.836139\n",
      "i: 22800 , cost :  0.366979 , training accuracy :  0.835017\n",
      "i: 22900 , cost :  0.358377 , training accuracy :  0.847363\n",
      "i: 23000 , cost :  0.356579 , training accuracy :  0.842873\n",
      "i: 23100 , cost :  0.391467 , training accuracy :  0.840629\n",
      "i: 23200 , cost :  0.391057 , training accuracy :  0.845118\n",
      "i: 23300 , cost :  0.375787 , training accuracy :  0.848485\n",
      "i: 23400 , cost :  0.387746 , training accuracy :  0.829405\n",
      "i: 23500 , cost :  0.384973 , training accuracy :  0.828283\n",
      "i: 23600 , cost :  0.380817 , training accuracy :  0.845118\n",
      "i: 23700 , cost :  0.37385 , training accuracy :  0.838384\n",
      "i: 23800 , cost :  0.375038 , training accuracy :  0.842873\n",
      "i: 23900 , cost :  0.379711 , training accuracy :  0.830527\n",
      "i: 24000 , cost :  0.37421 , training accuracy :  0.837261\n",
      "i: 24100 , cost :  0.375831 , training accuracy :  0.833894\n",
      "i: 24200 , cost :  0.37458 , training accuracy :  0.848485\n",
      "i: 24300 , cost :  0.37783 , training accuracy :  0.832772\n",
      "i: 24400 , cost :  0.393528 , training accuracy :  0.829405\n",
      "i: 24500 , cost :  0.379597 , training accuracy :  0.847363\n",
      "i: 24600 , cost :  0.374115 , training accuracy :  0.842873\n",
      "i: 24700 , cost :  0.373808 , training accuracy :  0.838384\n",
      "i: 24800 , cost :  0.463064 , training accuracy :  0.803591\n",
      "i: 24900 , cost :  0.404123 , training accuracy :  0.822671\n",
      "i: 25000 , cost :  0.394657 , training accuracy :  0.83165\n",
      "i: 25100 , cost :  0.387063 , training accuracy :  0.839506\n",
      "i: 25200 , cost :  0.393203 , training accuracy :  0.833894\n",
      "i: 25300 , cost :  0.386874 , training accuracy :  0.832772\n",
      "i: 25400 , cost :  0.39339 , training accuracy :  0.829405\n",
      "i: 25500 , cost :  0.39234 , training accuracy :  0.83165\n",
      "i: 25600 , cost :  0.391643 , training accuracy :  0.832772\n",
      "i: 25700 , cost :  0.389327 , training accuracy :  0.830527\n",
      "i: 25800 , cost :  0.389335 , training accuracy :  0.840629\n",
      "i: 25900 , cost :  0.394022 , training accuracy :  0.838384\n",
      "i: 26000 , cost :  0.385799 , training accuracy :  0.840629\n",
      "i: 26100 , cost :  0.400536 , training accuracy :  0.832772\n",
      "i: 26200 , cost :  0.392914 , training accuracy :  0.824916\n",
      "i: 26300 , cost :  0.393005 , training accuracy :  0.826038\n",
      "i: 26400 , cost :  0.391851 , training accuracy :  0.82716\n",
      "i: 26500 , cost :  0.39472 , training accuracy :  0.824916\n",
      "i: 26600 , cost :  0.390025 , training accuracy :  0.830527\n",
      "i: 26700 , cost :  0.383651 , training accuracy :  0.838384\n",
      "i: 26800 , cost :  0.383273 , training accuracy :  0.838384\n",
      "i: 26900 , cost :  0.38609 , training accuracy :  0.833894\n",
      "i: 27000 , cost :  0.392729 , training accuracy :  0.836139\n",
      "i: 27100 , cost :  0.40695 , training accuracy :  0.838384\n",
      "i: 27200 , cost :  0.414611 , training accuracy :  0.835017\n",
      "i: 27300 , cost :  0.402775 , training accuracy :  0.839506\n",
      "i: 27400 , cost :  0.391607 , training accuracy :  0.840629\n",
      "i: 27500 , cost :  0.393253 , training accuracy :  0.836139\n",
      "i: 27600 , cost :  0.391485 , training accuracy :  0.845118\n",
      "i: 27700 , cost :  0.399764 , training accuracy :  0.832772\n",
      "i: 27800 , cost :  0.406093 , training accuracy :  0.826038\n",
      "i: 27900 , cost :  0.409367 , training accuracy :  0.829405\n",
      "i: 28000 , cost :  0.389154 , training accuracy :  0.840629\n",
      "i: 28100 , cost :  0.392015 , training accuracy :  0.833894\n",
      "i: 28200 , cost :  0.390154 , training accuracy :  0.835017\n",
      "i: 28300 , cost :  0.396312 , training accuracy :  0.843996\n",
      "i: 28400 , cost :  0.389897 , training accuracy :  0.836139\n",
      "i: 28500 , cost :  0.397338 , training accuracy :  0.838384\n",
      "i: 28600 , cost :  0.392509 , training accuracy :  0.835017\n",
      "i: 28700 , cost :  0.388536 , training accuracy :  0.838384\n",
      "i: 28800 , cost :  0.388782 , training accuracy :  0.837261\n",
      "i: 28900 , cost :  0.388275 , training accuracy :  0.839506\n",
      "i: 29000 , cost :  0.393665 , training accuracy :  0.836139\n",
      "i: 29100 , cost :  0.391456 , training accuracy :  0.840629\n",
      "i: 29200 , cost :  0.398957 , training accuracy :  0.836139\n",
      "i: 29300 , cost :  0.388337 , training accuracy :  0.842873\n",
      "i: 29400 , cost :  0.392633 , training accuracy :  0.835017\n",
      "i: 29500 , cost :  0.421773 , training accuracy :  0.830527\n",
      "i: 29600 , cost :  0.396776 , training accuracy :  0.832772\n",
      "i: 29700 , cost :  0.421576 , training accuracy :  0.804714\n",
      "i: 29800 , cost :  0.387369 , training accuracy :  0.837261\n",
      "i: 29900 , cost :  0.388047 , training accuracy :  0.842873\n",
      "Final training accuracy :  0.842873\n",
      "Done.\n",
      "\n",
      " ==================================================\n",
      "[lr: 0.05 ][epoch: 30000 ][hidden: 100 ][file: bhavul_tr_acc_0.84_prediction.csv ] ACCURACY :  0.842873\n",
      "================================================== \n",
      "\n",
      "This run of \"Titanic all variations\" ran for 0:54:13 and logs are available locally at: /Users/bhavul.g/.hyperdash/logs/titanic-all-variations/titanic-all-variations_2018-04-16t22-16-11-187482.log\n"
     ]
    }
   ],
   "source": [
    "%%monitor_cell \"Titanic all variations\"\n",
    "\n",
    "columns_to_use = ['Pclass','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked']\n",
    "learning_rates = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05]\n",
    "num_of_epochs_arr = [1000, 5000, 10000, 30000]\n",
    "hidden_units_arr = [3, 10, 15, 50, 100]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for num_of_epochs in num_of_epochs_arr:\n",
    "        for hidden_units in hidden_units_arr:\n",
    "            filename, accuracy_val = execute_steps_for_titanic(columns_to_use, \"bhavul\", learning_rate=learning_rate, num_of_epochs=num_of_epochs, hidden_units=hidden_units, threshold_for_output=0.5)\n",
    "            print(\"\\n\",\"=\"*50)\n",
    "            print(\"[lr:\",learning_rate,\"][epoch:\",num_of_epochs,\"][hidden:\",hidden_units,\"][file:\",filename,\"] ACCURACY : \",accuracy_val)\n",
    "            print(\"=\"*50,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
